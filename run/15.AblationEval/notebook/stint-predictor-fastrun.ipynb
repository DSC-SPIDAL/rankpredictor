{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stint-Predictor-Fastrun\n",
    "\n",
    "based on: LongTerm-Predictor\n",
    "\n",
    "long term predictor by continuously regressive forecasting\n",
    "\n",
    "\n",
    "support:\n",
    "+ train/test split by ratio or event\n",
    "+ incremental training evaluation(adjust ratio)\n",
    "+ go beyond curtrack and zerotrack by modeling the track status\n",
    "+ halfwin mode(0:no, 1:halfwin, 2:continous)\n",
    "+ split by stage, support all events (todo)\n",
    "\n",
    "+ disturbance analysis by adding disturbance to oracle trackstatus and lapstatus\n",
    "\n",
    "+ rank prediction directly\n",
    "+ rank prediction by laptime2rank,timediff2rank\n",
    "+ laptime,lapstatus prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import inspect\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pathlib import Path\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.deep_factor import DeepFactorEstimator\n",
    "from gluonts.model.deepstate import DeepStateEstimator\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "from gluonts.distribution.multivariate_gaussian import MultivariateGaussianOutput\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.model.prophet import ProphetPredictor\n",
    "from gluonts.model.r_forecast import RForecastPredictor\n",
    "from indycar.model.NaivePredictor import NaivePredictor\n",
    "from indycar.model.ZeroPredictor import ZeroPredictor\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch_hdd/hpda/indycar/notebook/15.AblationEval'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "random.seed()\n",
    "os.getcwd()\n",
    "#GPUID = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# remove NaN at the tail\n",
    "# there should be no nans in the middle of the ts\n",
    "COL_LAPTIME=0\n",
    "COL_RANK=1\n",
    "COL_TRACKSTATUS=2\n",
    "COL_LAPSTATUS=3\n",
    "COL_TIMEDIFF=4\n",
    "COL_CAUTION_LAPS_INSTINT=5\n",
    "COL_LAPS_INSTINT= 6\n",
    "\n",
    "FEATURE_STATUS = 2\n",
    "FEATURE_PITAGE = 4\n",
    "\n",
    "# oracle mode\n",
    "MODE_ORACLE = 1024  # oracle = track + lap\n",
    "MODE_ORACLE_TRACKONLY = 1\n",
    "MODE_ORACLE_LAPONLY = 2   \n",
    "   \n",
    "\n",
    "# oracle mode for training\n",
    "MODE_NOLAP = 1   \n",
    "MODE_NOTRACK = 2\n",
    "\n",
    "# predicting mode\n",
    "MODE_TESTZERO = 4\n",
    "MODE_TESTCURTRACK = 8\n",
    "\n",
    "MODE_PREDTRACK = 16\n",
    "MODE_PREDPIT = 32\n",
    "\n",
    "# disturbe analysis\n",
    "MODE_DISTURB_CLEARTRACK = 64\n",
    "MODE_DISTURB_ADJUSTTRACK = 128\n",
    "MODE_DISTURB_ADJUSTPIT = 256\n",
    "\n",
    "\n",
    "_mode_map = {MODE_ORACLE:'MODE_ORACLE',MODE_ORACLE_TRACKONLY:'MODE_ORACLE_TRACKONLY',\n",
    "            MODE_ORACLE_LAPONLY:'MODE_ORACLE_LAPONLY',\n",
    "             MODE_TESTZERO:'MODE_TESTZERO',MODE_TESTCURTRACK:'MODE_TESTCURTRACK',\n",
    "             MODE_PREDTRACK:'MODE_PREDTRACK',MODE_PREDPIT:'MODE_PREDPIT',\n",
    "            MODE_DISTURB_CLEARTRACK:'MODE_DISTURB_CLEARTRACK',MODE_DISTURB_ADJUSTTRACK:'MODE_DISTURB_ADJUSTTRACK',\n",
    "            MODE_DISTURB_ADJUSTPIT:'MODE_DISTURB_ADJUSTPIT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(event, year=0):\n",
    "    #inputfile = '../data/final/C_'+ event +'-' + year + '-final.csv'\n",
    "    if year>0:\n",
    "        inputfile = '../data/final/C_'+ event +'-' + year + '.csv'\n",
    "    else:\n",
    "        inputfile = '../data/final/C_'+ event +'.csv'\n",
    "    #outputprefix = year +'-' + event + '-'\n",
    "    dataset = pd.read_csv(inputfile)\n",
    "    #dataset.info(verbose=True)    \n",
    "    \n",
    "    final_lap = max(dataset.completed_laps)\n",
    "    total_laps = final_lap + 1\n",
    "\n",
    "    # get records for the cars that finish the race\n",
    "    completed_car_numbers= dataset[dataset.completed_laps == final_lap].car_number.values\n",
    "    completed_car_count = len(completed_car_numbers)\n",
    "\n",
    "    #print('count of completed cars:', completed_car_count)\n",
    "    #print('completed cars:', completed_car_numbers)\n",
    "\n",
    "    #make a copy\n",
    "    alldata = dataset.copy()\n",
    "    dataset = dataset[dataset['car_number'].isin(completed_car_numbers)]\n",
    "    rankdata = alldata.rename_axis('MyIdx').sort_values(by=['elapsed_time','MyIdx'], ascending=True)\n",
    "    rankdata = rankdata.drop_duplicates(subset=['car_number', 'completed_laps'], keep='first')\n",
    "    \n",
    "    cldata = make_cl_data(dataset)\n",
    "    acldata = make_cl_data(alldata)\n",
    "\n",
    "    return alldata, rankdata, acldata\n",
    "\n",
    "# make indy car completed_laps dataset\n",
    "# car_number, completed_laps, rank, elapsed_time, rank_diff, elapsed_time_diff \n",
    "def make_cl_data(dataset):\n",
    "\n",
    "    # pick up data with valid rank\n",
    "    rankdata = dataset.rename_axis('MyIdx').sort_values(by=['elapsed_time','MyIdx'], ascending=True)\n",
    "    rankdata = rankdata.drop_duplicates(subset=['car_number', 'completed_laps'], keep='first')\n",
    "\n",
    "    # resort by car_number, lap\n",
    "    uni_ds = rankdata.sort_values(by=['car_number', 'completed_laps', 'elapsed_time'], ascending=True)    \n",
    "    #uni_ds = uni_ds.drop([\"unique_id\", \"best_lap\", \"current_status\", \"track_status\", \"lap_status\",\n",
    "    #                  \"laps_behind_leade\",\"laps_behind_prec\",\"overall_rank\",\"pit_stop_count\",\n",
    "    #                  \"last_pitted_lap\",\"start_position\",\"laps_led\"], axis=1)\n",
    "    \n",
    "    uni_ds = uni_ds.drop([\"unique_id\", \"best_lap\", \n",
    "                      \"laps_behind_leade\",\"laps_behind_prec\",\"overall_rank\",\"pit_stop_count\",\n",
    "                      \"last_pitted_lap\",\"start_position\",\"laps_led\"], axis=1)\n",
    "        \n",
    "    carnumber = set(uni_ds['car_number'])\n",
    "    #print('cars:', carnumber)\n",
    "    #print('#cars=', len(carnumber))\n",
    "   \n",
    "    # faster solution , uni_ds already sorted by car_number and lap\n",
    "    uni_ds['rank_diff'] = uni_ds['rank'].diff()\n",
    "    mask = uni_ds.car_number != uni_ds.car_number.shift(1)\n",
    "    uni_ds['rank_diff'][mask] = 0\n",
    "    \n",
    "    uni_ds['time_diff'] = uni_ds['elapsed_time'].diff()\n",
    "    mask = uni_ds.car_number != uni_ds.car_number.shift(1)\n",
    "    uni_ds['time_diff'][mask] = 0\n",
    "    \n",
    "    #df = uni_ds[['car_number','completed_laps','rank','elapsed_time','rank_diff','time_diff']]\n",
    "    df = uni_ds[['car_number','completed_laps','rank','elapsed_time',\n",
    "                 'rank_diff','time_diff',\"current_status\", \"track_status\", \"lap_status\"]]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def test_flag(a, bitflag):\n",
    "    return (a & bitflag) ==  bitflag\n",
    "\n",
    "#\n",
    "# remove NaN at the tail\n",
    "# there should be no nans in the middle of the ts\n",
    "def get_modestr(a):\n",
    "    modestr = ''\n",
    "    for key in _mode_map:\n",
    "        if test_flag(a, key):\n",
    "            modestr += '%s,'%(_mode_map[key])\n",
    "            \n",
    "    return modestr\n",
    "\n",
    "# endpos -> vector of prediction_length\n",
    "_track_pred  = {}\n",
    "_track_true  = {}\n",
    "def init_track_model():\n",
    "    global _track_pred,_track_true\n",
    "    _track_pred = {}\n",
    "    _track_true  = {}\n",
    "    \n",
    "def get_track_model(track_rec, endpos, prediction_length, context_len=10):\n",
    "    \"\"\"\n",
    "    return the predicted track status\n",
    "    \"\"\"\n",
    "    global _track_pred,_track_true\n",
    "    # this is the perfect track model for Indy500 2018\n",
    "    track_model = [6,4,4,5,6,6,4]\n",
    "    if endpos in _track_pred:\n",
    "        return _track_pred[endpos]\n",
    "    else:\n",
    "        #get yflag lap count from the start pred point\n",
    "        yflaplen = 0\n",
    "        for i in range(1, context_len):\n",
    "            if track_rec[- prediction_length - i] == 1:\n",
    "                yflaplen += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        #laps remain, fill into the future\n",
    "        trackpred = np.array([0 for x in range(prediction_length)])\n",
    "        \n",
    "        yflap_pred = random.choice(track_model)\n",
    "        if yflaplen > 0 and yflap_pred > yflaplen:\n",
    "            trackpred[:(yflap_pred - yflaplen)] = 1\n",
    "        _track_pred[endpos] = trackpred\n",
    "        \n",
    "        _track_true[endpos]  = track_rec[- prediction_length:].copy()\n",
    "        \n",
    "        return trackpred\n",
    "\n",
    "    \n",
    "# endpos -> vector of prediction_length\n",
    "_track_adjust  = {}\n",
    "def init_adjust_track_model():\n",
    "    global _track_adjust\n",
    "    _track_adjust = {}\n",
    "    \n",
    "def adjust_track_model(track_rec, endpos, prediction_length, tailpos):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        tailpos ; <0 end pos of 1\n",
    "    return the predicted track status\n",
    "    \"\"\"\n",
    "    global _track_adjust\n",
    "    # this is the perfect track model for Indy500 2018\n",
    "    track_model = [-1,0,1]\n",
    "    if endpos in _track_adjust:\n",
    "        return _track_adjust[endpos]\n",
    "    else:\n",
    "        yflap_adjust = random.choice(track_model)\n",
    "        \n",
    "        #laps remain, fill into the future\n",
    "        trackadjust = track_rec[-prediction_length:].copy()\n",
    "        if yflap_adjust == -1:\n",
    "            trackadjust[tailpos] = 0\n",
    "        elif yflap_adjust == 1:\n",
    "            trackadjust[tailpos] = 0\n",
    "            if (tailpos + 1) <= -1:\n",
    "                trackadjust[tailpos+1] = 1\n",
    "        \n",
    "        _track_adjust[endpos] = trackadjust\n",
    "        \n",
    "        return trackadjust\n",
    "\n",
    "# carno -> lap_status\n",
    "_lap_adjust = {}    \n",
    "_empirical_model = {}\n",
    "def init_adjust_pitmodel():\n",
    "    global _lap_adjust\n",
    "    _lap_adjust = {}    \n",
    "    _empirical_model = {}\n",
    "\n",
    "def get_adjust_lapstatus(carno, lapstatus, force = True):\n",
    "    \"\"\"\n",
    "    init the lapstatus for each car, save it for future reference\n",
    "    \n",
    "    input:\n",
    "        carno;\n",
    "        lapstatus  ; the trueth\n",
    "    \n",
    "    \"\"\"\n",
    "    if carno not in _lap_adjust:\n",
    "        #adjust it\n",
    "        lapadjust = lapstatus.copy()\n",
    "        for pos in range(0, len(lapstatus)):\n",
    "            if lapadjust[pos] == 1:\n",
    "\n",
    "                success = False\n",
    "\n",
    "                while(not success):\n",
    "                    # adjust this pit lap position\n",
    "                    pos_adjust = get_random_choice(_adjust_model)\n",
    "\n",
    "                    new_pos = pos + pos_adjust\n",
    "\n",
    "                    if new_pos >= 0 and new_pos < len(lapstatus):\n",
    "                        #valid\n",
    "                        lapadjust[pos] = 0\n",
    "                        lapadjust[new_pos] = 1\n",
    "                        success = True\n",
    "                        \n",
    "                        #add statistics\n",
    "                        if pos_adjust not in _empirical_model:\n",
    "                            _empirical_model[pos_adjust] = 1\n",
    "                        else:\n",
    "                            _empirical_model[pos_adjust] += 1\n",
    "\n",
    "                    if force==False:\n",
    "                        break\n",
    "\n",
    "        _lap_adjust[carno] = lapadjust\n",
    "\n",
    "    return _lap_adjust[carno]\n",
    "        \n",
    "        \n",
    "def build_random_model(modeldict):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        modeldict ; {val: probability}\n",
    "    return:\n",
    "        model  ;  [val, cdf]\n",
    "    \"\"\"\n",
    "    # val, cdf\n",
    "    cdf = 0\n",
    "    model = np.zeros((len(modeldict), 2))\n",
    "    for idx, val in enumerate(sorted(modeldict.keys())):\n",
    "        model[idx, 0] = val\n",
    "        model[idx, 1] = cdf + modeldict[val]\n",
    "        cdf = model[idx, 1]\n",
    "        \n",
    "    #normalize\n",
    "    model[:, 1] = model[:, 1]/cdf\n",
    "    return model\n",
    "    \n",
    "def print_model(model, iscdf=True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model  ;  [val, cdf]\n",
    "    \"\"\"\n",
    "    sorted_model = model[np.argsort(model[:, 0])]\n",
    "    cdf = 0\n",
    "    \n",
    "    sumval = 1.\n",
    "    if not iscdf:\n",
    "        sumval = np.sum(sorted_model[:,1])\n",
    "    \n",
    "    ret = []\n",
    "    for row in sorted_model:\n",
    "        ret.append((row[0], (row[1]-cdf)/sumval))\n",
    "        if iscdf:\n",
    "            cdf = row[1]\n",
    "    #output\n",
    "    print(['%d:%.3f'%(x[0],x[1]) for x in ret])\n",
    "    \n",
    "    \n",
    "def get_random_choice(model):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model  ;  [val, cdf]\n",
    "    return:\n",
    "        val according to its probability\n",
    "    \"\"\"\n",
    "    \n",
    "    target = np.random.rand()\n",
    "    idx = np.sum(model[:,1] < target)\n",
    "    return int(model[idx,0])\n",
    "    \n",
    "#_modeldict={-2:0.1,-1:0.2,0:0.4, 1:0.2, 2:0.1 }\n",
    "_modeldict={-2:0.1,-1:0.2,0:0.05, 1:0.2, 2:0.1 }\n",
    "_adjust_model = build_random_model(_modeldict)\n",
    "\n",
    "def adjust_pit_model(lap_rec, prediction_length, force=True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        tailpos ; <0 end pos of 1\n",
    "    return the predicted lap status\n",
    "    \"\"\"\n",
    "    #laps remain, fill into the future\n",
    "    lapadjust = lap_rec[-prediction_length:].copy()\n",
    "    for pos in range(0, prediction_length):\n",
    "        if lapadjust[pos] == 1:\n",
    "            \n",
    "            success = False\n",
    "            \n",
    "            while(not success):\n",
    "                # adjust this pit lap position\n",
    "                pos_adjust = get_random_choice(_adjust_model)\n",
    "\n",
    "                new_pos = pos + pos_adjust\n",
    "\n",
    "                if new_pos >= 0 and new_pos < prediction_length:\n",
    "                    #valid\n",
    "                    lapadjust[pos] = 0\n",
    "                    lapadjust[new_pos] = 1\n",
    "                    success = True\n",
    "                    \n",
    "                if force==False:\n",
    "                    break\n",
    "                    \n",
    "    return lapadjust\n",
    "\n",
    "def adjust_pit_model_fix(lap_rec, endpos, prediction_length):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        tailpos ; <0 end pos of 1\n",
    "    return the predicted lap status\n",
    "    \"\"\"\n",
    "    adjust_model = [-1,0,1]\n",
    "    lap_adjust = random.choice(adjust_model)\n",
    "        \n",
    "    #laps remain, fill into the future\n",
    "    lapadjust = lap_rec[-prediction_length:].copy()\n",
    "    for pos in range(0, prediction_length):\n",
    "        if lapadjust[pos] == 1:\n",
    "            # adjust this pit lap position\n",
    "            pos_adjust = random.choice(adjust_model)\n",
    "\n",
    "            if pos_adjust == -1:\n",
    "                if (pos - 1 >= 0):\n",
    "                    lapadjust[pos] = 0\n",
    "                    lapadjust[pos - 1] = 1\n",
    "            elif pos_adjust == 1:\n",
    "                if (pos + 1 < prediction_length):\n",
    "                    lapadjust[pos] = 0\n",
    "                    lapadjust[pos + 1] = 1\n",
    "\n",
    "    return lapadjust\n",
    "    \n",
    "# pit model is separate for each car\n",
    "def get_pit_model(cuation_laps_instint, laps_instint, prediction_length):\n",
    "    \"\"\"\n",
    "    return the predicted pit status\n",
    "    \"\"\"\n",
    "    # this is the perfect empirical pit model for Indy500 2018\n",
    "    pit_model_all = [[33, 32, 35, 32, 35, 34, 35, 34, 37, 32, 37, 30, 33, 36, 35, 33, 36, 30, 31, 33, 36, 37, 35, 34, 34, 33, 37, 35, 39, 32, 36, 35, 34, 32, 36, 32, 31, 36, 33, 33, 35, 37, 40, 32, 32, 34, 35, 36, 33, 37, 35, 37, 34, 35, 39, 32, 31, 37, 32, 35, 36, 39, 35, 36, 34, 35, 33, 33, 34, 32, 33, 34],\n",
    "                [45, 44, 46, 44, 43, 46, 45, 43, 41, 48, 46, 43, 47, 45, 49, 44, 48, 42, 44, 46, 45, 45, 43, 44, 44, 43, 46]]\n",
    "    pit_model_top8 = [[33, 32, 35, 33, 36, 33, 36, 33, 37, 35, 36, 33, 37, 34],\n",
    "                 [46, 45, 43, 48, 46, 45, 45, 43]]\n",
    "    \n",
    "    pit_model = pit_model_all\n",
    "    \n",
    "    if cuation_laps_instint>10:\n",
    "        #use low model\n",
    "        pred_pit_laps = random.choice(pit_model[0])\n",
    "    else:\n",
    "        pred_pit_laps = random.choice(pit_model[1])\n",
    "                \n",
    "    #laps remain, fill into the future\n",
    "    pitpred = np.array([0 for x in range(prediction_length)])\n",
    "    \n",
    "    if (pred_pit_laps > laps_instint) and (pred_pit_laps <= laps_instint + prediction_length):\n",
    "        pitpred[pred_pit_laps - laps_instint - 1] = 1\n",
    "         \n",
    "    return pitpred    \n",
    "    \n",
    "def make_dataset_byevent(runs, prediction_length, freq, \n",
    "                       useeid = False,\n",
    "                       run_ts= COL_LAPTIME, \n",
    "                       test_event = 'Indy500-2018',\n",
    "                       test_cars = [],  \n",
    "                       use_global_dict = True,\n",
    "                       oracle_mode = MODE_ORACLE,\n",
    "                       half_moving_win = 0,\n",
    "                       train_ratio=0.8,\n",
    "                       log_transform = False,\n",
    "                       context_ratio = 0.,\n",
    "                       verbose = False\n",
    "                ):\n",
    "    \"\"\"\n",
    "    split the ts to train and test part by the ratio\n",
    "    \n",
    "    input:\n",
    "        oracle_mode: false to simulate prediction in real by \n",
    "                set the covariates of track and lap status as nan in the testset\n",
    "        half_moving_win  ; extend to 0:-1 ,1:-1/2plen, 2:-plen\n",
    "    \n",
    "    \"\"\"    \n",
    "    run_ts= _run_ts \n",
    "    test_event = _test_event\n",
    "    feature_mode = _feature_mode\n",
    "    \n",
    "    init_track_model()\n",
    "    init_adjust_track_model()\n",
    "    \n",
    "    start = pd.Timestamp(\"01-01-2019\", freq=freq)  # can be different for each time series\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    #select run\n",
    "    if runs>=0:\n",
    "        _laptime_data = [laptime_data[runs].copy()]\n",
    "    else:\n",
    "        _laptime_data = laptime_data.copy()\n",
    "    \n",
    "   \n",
    "    #add statistics for adjust test\n",
    "    # trackstatus, lapstatus\n",
    "    mae = [0,0]\n",
    "    \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    for _data in _laptime_data:\n",
    "        _train = []\n",
    "        _test = []\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        \n",
    "        else:\n",
    "            test_mode = False\n",
    "            #jump out\n",
    "            continue            \n",
    "            \n",
    "        #statistics on the ts length\n",
    "        ts_len = [ _entry.shape[1] for _entry in _data[2]]\n",
    "        max_len = int(np.max(ts_len))\n",
    "        train_len = int(np.max(ts_len) * train_ratio)\n",
    "        \n",
    "        if context_ratio != 0.:\n",
    "            # add this part to train set\n",
    "            context_len = int(np.max(ts_len) * context_ratio)\n",
    "        else:    \n",
    "            context_len = prediction_length*2\n",
    "        if context_len < 10:\n",
    "            context_len = 10\n",
    "            \n",
    "        if verbose:\n",
    "            #print(f'====event:{events[_data[0]]}, train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)}')\n",
    "            print(f'====event:{events[_data[0]]}, prediction_len={prediction_length},train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)},context_len={context_len}')\n",
    "                \n",
    "        # process for each ts\n",
    "        for rowid in range(_data[2].shape[0]):\n",
    "            # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "            rec = _data[2][rowid].copy()\n",
    "            rec_raw = _data[2][rowid].copy()\n",
    "            \n",
    "            #remove nan(only tails)\n",
    "            nans, x= nan_helper(rec[run_ts,:])\n",
    "            nan_count = np.sum(nans)             \n",
    "            rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "            \n",
    "            # remove short ts\n",
    "            totallen = rec.shape[1]\n",
    "            if ( totallen < train_len + prediction_length):\n",
    "                if verbose:\n",
    "                    print(f'a short ts: carid={_data[1][rowid]}ï¼Œlen={totallen}')\n",
    "                continue                \n",
    "            \n",
    "            if use_global_dict:\n",
    "                carno = _data[1][rowid]\n",
    "                carid = global_carids[_data[1][rowid]]\n",
    "            else:\n",
    "                #simulation dataset, todo, fix the carids as decoder\n",
    "                carno = rowid\n",
    "                carid = rowid\n",
    "                \n",
    "            \n",
    "            if useeid:\n",
    "                static_cat = [carid, _data[0]]    \n",
    "            else:\n",
    "                static_cat = [carid]    \n",
    "                \n",
    "            #first, get target a copy    \n",
    "            # target can be COL_XXSTATUS\n",
    "            target_val = rec[run_ts,:].copy().astype(np.float32)\n",
    "            if log_transform:\n",
    "                target_val = np.log(target_val + 1.0)                \n",
    "            \n",
    "            # adjust for disturbance analysis\n",
    "            if test_mode and test_flag(oracle_mode, MODE_DISTURB_ADJUSTPIT):\n",
    "                lap_status = rec[COL_LAPSTATUS, :].copy()\n",
    "                rec[COL_LAPSTATUS, :] = get_adjust_lapstatus(carno, lap_status)\n",
    "                \n",
    "            # selection of features\n",
    "            if test_flag(oracle_mode, MODE_NOTRACK) or test_flag(oracle_mode, MODE_ORACLE_LAPONLY):                \n",
    "                rec[COL_TRACKSTATUS, :] = 0\n",
    "            if test_flag(oracle_mode, MODE_NOLAP) or test_flag(oracle_mode, MODE_ORACLE_TRACKONLY):                \n",
    "                rec[COL_LAPSTATUS, :] = 0\n",
    "\n",
    "            test_rec_cnt = 0\n",
    "            if not test_mode:\n",
    "                \n",
    "                # all go to train set\n",
    "                _train.append({'target': target_val, \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [rec[COL_TRACKSTATUS,:],\n",
    "                                       rec[COL_LAPSTATUS,:]]\n",
    "                              }\n",
    "                              )\n",
    "            else:\n",
    "                # reset train_len\n",
    "                #context_len = prediction_length*2\n",
    "                #if context_len < 10:\n",
    "                #    context_len = 10\n",
    "                \n",
    "                #context_len = train_len\n",
    "                \n",
    "                # multiple test ts(rolling window as half of the prediction_length)\n",
    "\n",
    "                #step = -int(prediction_length/2) if half_moving_win else -prediction_length\n",
    "                if half_moving_win == 1:\n",
    "                    step = -int(prediction_length/2)\n",
    "                elif half_moving_win == 2:\n",
    "                    step = -prediction_length\n",
    "                else:\n",
    "                    step = -1\n",
    "                \n",
    "                #bug fix, fixed the split point for all cars/ts\n",
    "                for endpos in range(max_len, context_len+prediction_length, \n",
    "                                    step):\n",
    "\n",
    "                    #check if enough for this ts\n",
    "                    if endpos > totallen:\n",
    "                        continue\n",
    "                    \n",
    "                    track_rec = rec[COL_TRACKSTATUS, :endpos].copy()\n",
    "                    lap_rec = rec[COL_LAPSTATUS, :endpos].copy()\n",
    "                    pitage_rec = rec[COL_LAPS_INSTINT, :endpos].copy()\n",
    "                    \n",
    "                    caution_laps_instint = int(rec[COL_CAUTION_LAPS_INSTINT, endpos -prediction_length - 1])\n",
    "                    laps_instint = int(rec[COL_LAPS_INSTINT, endpos -prediction_length - 1])\n",
    "                    \n",
    "\n",
    "                    # test mode\n",
    "                    if test_flag(oracle_mode, MODE_TESTCURTRACK):\n",
    "                        # since nan does not work, use cur-val instead\n",
    "                        track_rec[-prediction_length:] = track_rec[-prediction_length - 1]\n",
    "                        #track_rec[-prediction_length:] = random.randint(0,1)\n",
    "                        #lap_rec[-prediction_length:] = lap_rec[-prediction_length - 1]\n",
    "                        lap_rec[-prediction_length:] = 0\n",
    "                        \n",
    "                        #for pitage, just assume there is no pit\n",
    "                        start_pitage = pitage_rec[-prediction_length - 1]\n",
    "                        pitage_rec[-prediction_length:] = np.array([x+start_pitage+1 for x in range(prediction_length)])\n",
    "                        \n",
    "                    elif test_flag(oracle_mode, MODE_TESTZERO):\n",
    "                        #set prediction part as nan\n",
    "                        #track_rec[-prediction_length:] = np.nan\n",
    "                        #lap_rec[-prediction_length:] = np.nan\n",
    "                        track_rec[-prediction_length:] = 0\n",
    "                        lap_rec[-prediction_length:] = 0        \n",
    "                        #for pitage, just assume there is no pit\n",
    "                        start_pitage = pitage_rec[-prediction_length - 1]\n",
    "                        pitage_rec[-prediction_length:] = np.array([x+start_pitage+1 for x in range(prediction_length)])\n",
    " \n",
    "                    # predicting with status model\n",
    "                    if test_flag(oracle_mode, MODE_PREDTRACK):\n",
    "                        predrec = get_track_model(track_rec, endpos, prediction_length)\n",
    "                        track_rec[-prediction_length:] = predrec\n",
    "                        #lap_rec[-prediction_length:] = 0\n",
    "                        \n",
    "                    if test_flag(oracle_mode, MODE_PREDPIT):\n",
    "                        #predrec = get_track_model(track_rec, endpos, prediction_length)\n",
    "                        #track_rec[-prediction_length:] = predrec\n",
    "                        lap_rec[-prediction_length:] = get_pit_model(caution_laps_instint,\n",
    "                                                                    laps_instint,prediction_length)\n",
    "                        \n",
    "                        #for pitage, use the predicted lap info to update pitage\n",
    "                        start_pitage = pitage_rec[-prediction_length - 1]\n",
    "                        for pos in range(prediction_length):\n",
    "                            if lap_rec[-prediction_length + pos]==0:\n",
    "                                pitage_rec[-prediction_length + pos] = start_pitage+1\n",
    "                            else:\n",
    "                                #new pit\n",
    "                                start_pitage = 0\n",
    "                                pitage_rec[-prediction_length + pos] = start_pitage\n",
    "                        \n",
    "                    # disturbe analysis\n",
    "                    if test_flag(oracle_mode, MODE_DISTURB_CLEARTRACK):\n",
    "                        # clear the oracle track status\n",
    "                        # future 1s in trackstatus\n",
    "                        # pattern like 0 1 xx\n",
    "                        for _pos in range(-prediction_length + 1, -1):\n",
    "                            if track_rec[_pos - 1] == 0:\n",
    "                                track_rec[_pos] = 0\n",
    "                                \n",
    "                    if test_flag(oracle_mode, MODE_DISTURB_ADJUSTTRACK):\n",
    "                        # adjust the end position of track, or caution lap length\n",
    "                        # find the end of caution laps\n",
    "                        _tail = 0\n",
    "                        for _pos in range(-1,-prediction_length + 1,-1):\n",
    "                            if track_rec[_pos] == 1:\n",
    "                                #find the tail\n",
    "                                _tail = _pos\n",
    "                                break\n",
    "                        if _tail != 0:\n",
    "                            #found\n",
    "                            adjustrec = adjust_track_model(track_rec, endpos, prediction_length, _tail)\n",
    "                            track_rec[-prediction_length:] = adjustrec\n",
    "                        \n",
    "                    #if test_flag(oracle_mode, MODE_DISTURB_ADJUSTPIT):\n",
    "                    #    # adjust the position of pit\n",
    "                    #    if np.sum(lap_rec[-prediction_length:]) > 0:\n",
    "                    #        adjustrec = adjust_pit_model(lap_rec, endpos, prediction_length)\n",
    "                    #        lap_rec[-prediction_length:] = adjustrec\n",
    "                        \n",
    "                    #okay, end of adjustments, test difference here\n",
    "                    # rec_raw .vs. track_rec, lap_rec\n",
    "                    track_rec_raw = rec_raw[COL_TRACKSTATUS, :endpos]\n",
    "                    lap_rec_raw = rec_raw[COL_LAPSTATUS, :endpos]\n",
    "                    \n",
    "                    mae[0] = mae[0] + np.nansum(np.abs(track_rec[-prediction_length:] - track_rec_raw[-prediction_length:]))\n",
    "                    mae[1] = mae[1] + np.nansum(np.abs(lap_rec[-prediction_length:] - lap_rec_raw[-prediction_length:]))\n",
    "\n",
    "                    if feature_mode == FEATURE_STATUS:\n",
    "                        _test.append({'target': target_val[:endpos].astype(np.float32), \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [track_rec,lap_rec]\n",
    "                                 }\n",
    "                              )   \n",
    "                    elif feature_mode == FEATURE_PITAGE:\n",
    "                        _test.append({'target': target_val[:endpos].astype(np.float32), \n",
    "                                    'start': start, \n",
    "                                    'feat_static_cat': static_cat,\n",
    "                                    'feat_dynamic_real': [track_rec,lap_rec,pitage_rec]\n",
    "                                     }\n",
    "                                  )                       \n",
    "                    test_rec_cnt += 1\n",
    "            \n",
    "            #add one ts\n",
    "            if verbose:\n",
    "                print(f'carno:{carno}, totallen:{totallen}, nancount:{nan_count}, test_reccnt:{test_rec_cnt}')\n",
    "\n",
    "        train_set.extend(_train)\n",
    "        test_set.extend(_test)\n",
    "\n",
    "    print(f'train len:{len(train_set)}, test len:{len(test_set)}, mae_track:{mae[0]},mae_lap:{mae[1]},')\n",
    "    \n",
    "    train_ds = ListDataset(train_set, freq=freq)\n",
    "    test_ds = ListDataset(test_set, freq=freq)    \n",
    "    \n",
    "    return train_ds, test_ds, train_set, test_set\n",
    "\n",
    "def save_dataset(datafile,freq, prediction_length, cardinality, train_ds, test_ds):\n",
    "    with open(datafile, 'wb') as f:\n",
    "        #pack [global_carids, laptime_data]\n",
    "        savedata = [freq, prediction_length, cardinality, train_ds, test_ds]\n",
    "        #savedata = [freq, train_set, test_set]\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test for Indy500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_ds,predictor):\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    print(f'tss len={len(tss)}, forecasts len={len(forecasts)}')\n",
    "    \n",
    "    return tss, forecasts\n",
    "\n",
    "   \n",
    "def run_prediction_ex(test_ds, prediction_length, model_name,trainid):\n",
    "    with mx.Context(mx.gpu(7)):    \n",
    "        pred_ret = []\n",
    "\n",
    "        rootdir = f'../models/remote/{_dataset_id}/{_task_id}-{trainid}/'\n",
    "        # deepAR-Oracle\n",
    "        if model_name == 'curtrack':\n",
    "            model=f'deepAR-Oracle-{_task_id}-curtrack-indy-f1min-t{prediction_length}-e1000-r1_curtrack_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "\n",
    "        elif model_name == 'zerotrack':\n",
    "            model=f'deepAR-Oracle-{_task_id}-nolap-zerotrack-indy-f1min-t{prediction_length}-e1000-r1_zerotrack_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "            \n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle-laponly':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle-laponly_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle-trackonly':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle-trackonly_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        # deepAR\n",
    "        elif model_name == 'deepAR':\n",
    "            model=f'deepAR-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_deepar_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "\n",
    "        # naive\n",
    "        elif model_name == 'naive':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  NaivePredictor(freq= freq, prediction_length = prediction_length)\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        # zero, zero keeps the rank unchange\n",
    "        elif model_name == 'zero':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  ZeroPredictor(freq= freq, prediction_length = prediction_length)\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "\n",
    "        # arima\n",
    "        elif model_name == 'arima':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  RForecastPredictor(method_name='arima',freq= freq, \n",
    "                                            prediction_length = prediction_length,trunc_length=60)\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        else:\n",
    "            print(f'error: model {model_name} not support yet!')\n",
    "\n",
    "        return pred_ret     \n",
    "    \n",
    "def load_model(prediction_length, model_name,trainid):\n",
    "    with mx.Context(mx.gpu(7)):    \n",
    "        pred_ret = []\n",
    "\n",
    "        rootdir = f'../models/remote/{_dataset_id}/{_task_id}-{trainid}/'\n",
    "        # deepAR-Oracle\n",
    "        if model_name == 'curtrack':\n",
    "            model=f'deepAR-Oracle-{_task_id}-curtrack-indy-f1min-t{prediction_length}-e1000-r1_curtrack_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "\n",
    "        elif model_name == 'zerotrack':\n",
    "            model=f'deepAR-Oracle-{_task_id}-nolap-zerotrack-indy-f1min-t{prediction_length}-e1000-r1_zerotrack_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            \n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle-laponly':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle-laponly_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle-trackonly':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle-trackonly_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "        # deepAR\n",
    "        elif model_name == 'deepAR':\n",
    "            model=f'deepAR-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_deepar_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "\n",
    "        # naive\n",
    "        elif model_name == 'naive':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  NaivePredictor(freq= freq, prediction_length = prediction_length)\n",
    "        # zero, zero keeps the rank unchange\n",
    "        elif model_name == 'zero':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  ZeroPredictor(freq= freq, prediction_length = prediction_length)\n",
    "\n",
    "        # arima\n",
    "        elif model_name == 'arima':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  RForecastPredictor(method_name='arima',freq= freq, \n",
    "                                            prediction_length = prediction_length,trunc_length=60)\n",
    "        else:\n",
    "            print(f'error: model {model_name} not support yet!')\n",
    "\n",
    "        return predictor         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc rank\n",
    "def eval_rank_bytimediff(test_ds,tss,forecasts,prediction_length):\n",
    "    \"\"\"\n",
    "    timediff models\n",
    "    \n",
    "    works for one event only\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    carlist = []\n",
    "\n",
    "    # carno-lap# -> elapsed_time[] array\n",
    "    forecasts_et = dict()\n",
    "\n",
    "    ds_iter =  iter(test_ds)\n",
    "    for idx in range(len(test_ds)):\n",
    "        test_rec = next(ds_iter)\n",
    "        #global carid\n",
    "        carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "        #print('car no:', carno)\n",
    "\n",
    "        if carno not in carlist:\n",
    "            carlist.append(carno)\n",
    "\n",
    "        # calc elapsed time\n",
    "        prediction_len = forecasts[idx].samples.shape[1]\n",
    "        if prediction_length != prediction_len:\n",
    "            print('error: prediction_len does not match, {prediction_length}:{prediction_len}')\n",
    "            return []\n",
    "        \n",
    "        #forecast_laptime_mean = np.mean(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "        forecast_laptime_mean = np.median(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "        \n",
    "        timediff_array = tss[idx].values.copy()\n",
    "\n",
    "        #save the prediction\n",
    "        completed_laps = len(tss[idx]) - prediction_len + 1\n",
    "        #print('car no:', carno, 'completed_laps:', completed_laps)\n",
    "        #key = '%s-%s'%(carno, completed_laps)\n",
    "        #forecasts_et[key] = elapsed_time[-prediction_len:].copy()\n",
    "        if completed_laps not in forecasts_et:\n",
    "            forecasts_et[completed_laps] = {}\n",
    "        forecasts_et[completed_laps][carno] = [timediff_array[-prediction_len:].copy(),\n",
    "                                                   forecast_laptime_mean.copy()]\n",
    "\n",
    "\n",
    "    # calc rank\n",
    "    rank_ret = []\n",
    "    for lap in forecasts_et.keys():\n",
    "        #get car list for this lap\n",
    "        carlist = list(forecasts_et[lap].keys())\n",
    "        #print('carlist:', carlist)\n",
    "\n",
    "        caridmap={key:idx for idx, key in enumerate(carlist)}\n",
    "\n",
    "        #fill in data\n",
    "        time_diff = np.zeros((2, len(carlist), prediction_len))\n",
    "        for carno in carlist:\n",
    "            carid = caridmap[carno]\n",
    "            time_diff[0, carid, :] = forecasts_et[lap][carno][0].reshape((prediction_len))\n",
    "            time_diff[1, carid, :] = forecasts_et[lap][carno][1].reshape((prediction_len))\n",
    "\n",
    "        #calculate rank    \n",
    "        idx = np.argsort(time_diff[0], axis=0)\n",
    "        true_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "        idx = np.argsort(time_diff[1], axis=0)\n",
    "        pred_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "        rank_ret.append([lap, time_diff, true_rank, pred_rank])\n",
    "        \n",
    "    return rank_ret,forecasts_et\n",
    "    \n",
    "#calc rank\n",
    "def eval_laptime(test_ds,tss,forecasts,prediction_length, start_offset):\n",
    "    \"\"\"\n",
    "    evaluate rank by laptime forecasting\n",
    "    \n",
    "    input:\n",
    "        test_ds       ; must be test set for a single event, because test_ds itself does not \n",
    "                        contain features to identify the eventid\n",
    "        start_offset[]; elapsed time for lap0, for one specific event\n",
    "        tss,forecasts ; forecast result\n",
    "        prediction_length ; \n",
    "    return:\n",
    "        rank_ret      ;  [lap, elapsed_time, true_rank, pred_rank] \n",
    "        forecasts_et  ;  {[completed_laps][carno]} ->(elapsed_time, elapsed_time_pred)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    carlist = []\n",
    "\n",
    "    # carno-lap# -> elapsed_time[] array\n",
    "    forecasts_et = dict()\n",
    "\n",
    "    ds_iter =  iter(test_ds)\n",
    "    for idx in range(len(test_ds)):\n",
    "        test_rec = next(ds_iter)\n",
    "        #global carid\n",
    "        carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "        #print('car no:', carno)\n",
    "\n",
    "        if carno not in carlist:\n",
    "            carlist.append(carno)\n",
    "\n",
    "        #start_offset is global var\n",
    "        #offset = start_offset[(start_offset['car_number']==carno)].elapsed_time.values[0] \n",
    "        #print('start_offset:', offset)\n",
    "\n",
    "        # calc elapsed time\n",
    "        prediction_len = forecasts[idx].samples.shape[1]\n",
    "        if prediction_length != prediction_len:\n",
    "            print('error: prediction_len does not match, {prediction_length}:{prediction_len}')\n",
    "            return []\n",
    "        \n",
    "        forecast_laptime_mean = np.mean(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "        #forecast_laptime_mean = np.median(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "\n",
    "        laptime_array = tss[idx].values.copy()\n",
    "        #elapsed_time = np.cumsum(laptime_array) + offset\n",
    "\n",
    "        laptime_array_hat = tss[idx].values.copy()\n",
    "        laptime_array_hat[-prediction_len:] = forecast_laptime_mean \n",
    "        #elapsed_time_hat = np.cumsum(laptime_array) + offset\n",
    "\n",
    "        #save the prediction\n",
    "        completed_laps = len(tss[idx]) - prediction_len + 1\n",
    "        #print('car no:', carno, 'completed_laps:', completed_laps)\n",
    "        #key = '%s-%s'%(carno, completed_laps)\n",
    "        #forecasts_et[key] = elapsed_time[-prediction_len:].copy()\n",
    "        if completed_laps not in forecasts_et:\n",
    "            forecasts_et[completed_laps] = {}\n",
    "        #forecasts_et[completed_laps][carno] = [elapsed_time[-prediction_len-1:].copy(),\n",
    "        #                                           elapsed_time_hat[-prediction_len-1:].copy()]\n",
    "        forecasts_et[completed_laps][carno] = [laptime_array[-prediction_len:].copy(),\n",
    "                                                   laptime_array_hat[-prediction_len:].copy()]\n",
    "\n",
    "\n",
    "    # calc rank\n",
    "    rank_ret = []\n",
    "    for lap in forecasts_et.keys():\n",
    "        #get car list for this lap\n",
    "        carlist = list(forecasts_et[lap].keys())\n",
    "        #print('carlist:', carlist)\n",
    "\n",
    "        caridmap={key:idx for idx, key in enumerate(carlist)}\n",
    "\n",
    "        #fill in data\n",
    "        #elapsed_time = np.zeros((2, len(carlist), prediction_len+1))\n",
    "        lap_time = np.zeros((2, len(carlist), prediction_len))\n",
    "        for carno in carlist:\n",
    "            carid = caridmap[carno]\n",
    "            lap_time[0, carid, :] = forecasts_et[lap][carno][0].reshape((prediction_len))\n",
    "            lap_time[1, carid, :] = forecasts_et[lap][carno][1].reshape((prediction_len))\n",
    "\n",
    "        #calculate rank    \n",
    "        #idx = np.argsort(elapsed_time[0], axis=0)\n",
    "        #true_rank = np.argsort(idx, axis=0)\n",
    "        true_laptime = lap_time[0]\n",
    "\n",
    "        #idx = np.argsort(elapsed_time[1], axis=0)\n",
    "        #pred_rank = np.argsort(idx, axis=0)\n",
    "        pred_laptime = lap_time[1]\n",
    "\n",
    "        rank_ret.append([lap, lap_time, true_laptime, pred_laptime])\n",
    "        \n",
    "    return rank_ret,forecasts_et    \n",
    "\n",
    "#calc rank\n",
    "def eval_rank(test_ds,tss,forecasts,prediction_length, start_offset):\n",
    "    \"\"\"\n",
    "    evaluate rank by laptime forecasting\n",
    "    \n",
    "    input:\n",
    "        test_ds       ; must be test set for a single event, because test_ds itself does not \n",
    "                        contain features to identify the eventid\n",
    "        start_offset[]; elapsed time for lap0, for one specific event\n",
    "        tss,forecasts ; forecast result\n",
    "        prediction_length ; \n",
    "    return:\n",
    "        rank_ret      ;  [lap, elapsed_time, true_rank, pred_rank] \n",
    "        forecasts_et  ;  {[completed_laps][carno]} ->(elapsed_time, elapsed_time_pred)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    carlist = []\n",
    "\n",
    "    # carno-lap# -> elapsed_time[] array\n",
    "    forecasts_et = dict()\n",
    "\n",
    "    ds_iter =  iter(test_ds)\n",
    "    for idx in range(len(test_ds)):\n",
    "        test_rec = next(ds_iter)\n",
    "        #global carid\n",
    "        carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "        #print('car no:', carno)\n",
    "\n",
    "        if carno not in carlist:\n",
    "            carlist.append(carno)\n",
    "\n",
    "        #start_offset is global var\n",
    "        if isinstance(start_offset, pd.core.frame.DataFrame):\n",
    "            offset = start_offset[(start_offset['car_number']==carno)].elapsed_time.values[0] \n",
    "        \n",
    "            \n",
    "        #print('start_offset:', offset)\n",
    "\n",
    "        # calc elapsed time\n",
    "        prediction_len = forecasts[idx].samples.shape[1]\n",
    "        if prediction_length != prediction_len:\n",
    "            print('error: prediction_len does not match, {prediction_length}:{prediction_len}')\n",
    "            return []\n",
    "        \n",
    "        forecast_laptime_mean = np.mean(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "        #forecast_laptime_mean = np.median(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "\n",
    "        if isinstance(start_offset, pd.core.frame.DataFrame):\n",
    "            #print('eval_rank:laptime2rank')\n",
    "            laptime_array = tss[idx].values.copy()\n",
    "            elapsed_time = np.cumsum(laptime_array) + offset\n",
    "\n",
    "            laptime_array = tss[idx].values.copy()\n",
    "            laptime_array[-prediction_len:] = forecast_laptime_mean \n",
    "            elapsed_time_hat = np.cumsum(laptime_array) + offset\n",
    "        else:\n",
    "            #print('eval_rank:rank-direct')\n",
    "            # rank directly\n",
    "            elapsed_time  = tss[idx].values.copy()\n",
    "\n",
    "            elapsed_time_hat = tss[idx].values.copy()\n",
    "            elapsed_time_hat[-prediction_len:] = forecast_laptime_mean             \n",
    "\n",
    "        #save the prediction\n",
    "        completed_laps = len(tss[idx]) - prediction_len + 1\n",
    "        #print('car no:', carno, 'completed_laps:', completed_laps)\n",
    "        #key = '%s-%s'%(carno, completed_laps)\n",
    "        #forecasts_et[key] = elapsed_time[-prediction_len:].copy()\n",
    "        if completed_laps not in forecasts_et:\n",
    "            forecasts_et[completed_laps] = {}\n",
    "        #forecasts_et[completed_laps][carno] = [elapsed_time[-prediction_len-1:].copy(),\n",
    "        #                                           elapsed_time_hat[-prediction_len-1:].copy()]\n",
    "        forecasts_et[completed_laps][carno] = [elapsed_time[-prediction_len:].copy(),\n",
    "                                                   elapsed_time_hat[-prediction_len:].copy()]\n",
    "\n",
    "\n",
    "    # calc rank\n",
    "    rank_ret = []\n",
    "    for lap in forecasts_et.keys():\n",
    "        #get car list for this lap\n",
    "        carlist = list(forecasts_et[lap].keys())\n",
    "        #print('carlist:', carlist)\n",
    "\n",
    "        caridmap={key:idx for idx, key in enumerate(carlist)}\n",
    "\n",
    "        #fill in data\n",
    "        #elapsed_time = np.zeros((2, len(carlist), prediction_len+1))\n",
    "        elapsed_time = np.zeros((2, len(carlist), prediction_len))\n",
    "        for carno in carlist:\n",
    "            carid = caridmap[carno]\n",
    "            elapsed_time[0, carid, :] = forecasts_et[lap][carno][0].reshape((prediction_len))\n",
    "            elapsed_time[1, carid, :] = forecasts_et[lap][carno][1].reshape((prediction_len))\n",
    "\n",
    "        #calculate rank    \n",
    "        idx = np.argsort(elapsed_time[0], axis=0)\n",
    "        true_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "        idx = np.argsort(elapsed_time[1], axis=0)\n",
    "        pred_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "        rank_ret.append([lap, elapsed_time, true_rank, pred_rank])\n",
    "        \n",
    "    return rank_ret,forecasts_et    \n",
    "    \n",
    "def get_acc(rank_ret,prediction_length, verbose = False):   \n",
    "    \"\"\"\n",
    "    input:\n",
    "        rank_ret: [lap, elapsed_time, true_rank, pred_rank], use [2][3] columns\n",
    "    return:\n",
    "        ((metrics...)\n",
    "         (record count...))\n",
    "         \n",
    "    the result can be used to calculate micro/macro metrics\n",
    "    \"\"\"\n",
    "    # evaluate\n",
    "    #top1 accuracy\n",
    "    top1acc = 0\n",
    "    top1acc_farmost = 0\n",
    "    top5acc = 0\n",
    "    top5acc_farmost = 0\n",
    "    tau = 0\n",
    "    rmse = 0.\n",
    "    mae = 0.\n",
    "    \n",
    "    for rec in rank_ret:\n",
    "        trueRank = rec[2]\n",
    "        predRank = rec[3]\n",
    "\n",
    "        #top1 , rank = 0, first col is not prediction\n",
    "        top1acc += np.sum((trueRank==0) & (predRank==0)) \n",
    "        \n",
    "        top1acc_farmost += np.sum((trueRank[:,-1]==0) & (predRank[:,-1]==0))\n",
    "        \n",
    "        #top5\n",
    "        top5acc += np.sum((trueRank<5) & (predRank<5)) \n",
    "        \n",
    "        top5acc_farmost += np.sum((trueRank[:,-1]<5) & (predRank[:,-1]<5))\n",
    "        \n",
    "        # tau\n",
    "        tao, _ = stats.kendalltau(trueRank, predRank)\n",
    "        tau += tao\n",
    "        \n",
    "        #rmse\n",
    "        rmse += mean_squared_error(predRank,trueRank)\n",
    "        \n",
    "        #mae\n",
    "        mae += np.sum(np.abs(predRank - trueRank))\n",
    "        \n",
    "    recnt = len(rank_ret)\n",
    "    if recnt > 0:\n",
    "        top1acc = top1acc *1.0/ (recnt*prediction_length)\n",
    "        top1acc_farmost = top1acc_farmost *1.0/ recnt\n",
    "        top5acc = top5acc *1.0/ (5*recnt*prediction_length)\n",
    "        top5acc_farmost = top5acc_farmost *1.0/ (5*recnt)\n",
    "        tau = tau/recnt\n",
    "        rmse = rmse/recnt\n",
    "        \n",
    "        mae = mae/recnt\n",
    "\n",
    "        #debug only\n",
    "        if _run_ts == COL_LAPSTATUS:\n",
    "            tau = mae\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'total:{len(rank_ret)}, prediction_length:{prediction_length}') \n",
    "            print('top1acc=', top1acc,\n",
    "                  'top1acc_farmost=', top1acc_farmost,\n",
    "                  'top5acc=', top5acc,\n",
    "                  'top5acc_farmost=', top5acc_farmost,\n",
    "                 )\n",
    "            print('tau = ', tau,\n",
    "                 'rmse = ', rmse,\n",
    "                 'mae = ', mae)\n",
    "    \n",
    "    return ((top1acc,top1acc_farmost,top5acc,top5acc_farmost,tau,rmse),\n",
    "            (recnt*prediction_length,recnt,5*recnt*prediction_length,5*recnt,recnt,recnt))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(runs, plens, half, trainids, train_ratio, testfunc, datamode='', models=[]):\n",
    "    \"\"\"\n",
    "    \n",
    "    input:\n",
    "        plens=[2,5,10]\n",
    "        half=[False]\n",
    "        #trainids = [\"indy500-r0.2\",\"indy500-r0.4\",\"indy500\"]\n",
    "        trainids = [\"r0.5\"]\n",
    "        #half=[True,False]\n",
    "        #plens=[2]\n",
    "        runs = 5\n",
    "        train_ratio=0.5 \n",
    "        exp_id='mean-splitbystage-predpit'\n",
    "        \n",
    "        testfunc ; run_exp_predpit, run_exp_predtrack, run_exp ...\n",
    "\n",
    "    return:\n",
    "    \n",
    "        dfret  ; average result of multiple runs\n",
    "                 dataframe['model' , 'prediction_length', 'halfmode','datamode','trainid',\n",
    "                         'top1acc','top1acc_farmost','top5acc','top5acc_farmost','tau','rmse',\n",
    "                         'top1acc_std','top1acc_farmost_std','top5acc_std','top5acc_farmost_std','tau_std','rmse_std']\n",
    "                          \n",
    "        alldata_ret  ; for debug\n",
    "            [runid][halfmode,plen,trainid] -> (pred_ret, test_ds, rank_ret)\n",
    "                pred_ret[model] ->ã€€[tss, forecasts]\n",
    "                test_ds[model] ->ã€€test_ds\n",
    "                rank_ret[model] -> ([lap, elapsed_time, true_rank, pred_rank],forecast_ret)\n",
    "                    forecast_ret[completed_laps][carno] -> (elapsed_time, elapsed_time_hat)\n",
    "    \n",
    "    \"\"\"\n",
    "    if plens == [] or half == [] or trainids == []:\n",
    "        print(\"error with empty settings\")\n",
    "        return\n",
    "    \n",
    "    #testfunc or (datamode & models)\n",
    "    if isinstance(testfunc,str) and (datamode == '' or models == []):\n",
    "        print(\"error with testfunc\")\n",
    "        return\n",
    "\n",
    "    allret = []\n",
    "    alldata_ret = []\n",
    "    for runid in range(runs):\n",
    "        exp_data = []\n",
    "        exp_result = []\n",
    "\n",
    "        for halfmode in half:\n",
    "            for plen in plens:\n",
    "                for trainid in trainids:\n",
    "                    print('='*10)\n",
    "                    if not isinstance(testfunc,str):\n",
    "                        pred_ret, test_ds, rank_ret, metric_ret = testfunc(plen, halfmode, \n",
    "                                                            train_ratio=train_ratio,\n",
    "                                                            trainid=trainid)\n",
    "                    else:\n",
    "                        pred_ret, test_ds, rank_ret, metric_ret = run_exp(plen, halfmode, \n",
    "                                                            train_ratio=train_ratio,\n",
    "                                                            trainid=trainid, \n",
    "                                                            datamode=datamode,\n",
    "                                                            models=models)\n",
    "                        \n",
    "\n",
    "                    #save \n",
    "                    exp_data.append((pred_ret, test_ds, rank_ret))\n",
    "                    exp_result.extend(metric_ret)\n",
    "\n",
    "        #save result\n",
    "        result = pd.DataFrame(exp_result, columns = ['model' , 'prediction_length', 'halfmode',\n",
    "                                           'datamode','trainid',\n",
    "                                           'top1acc','top1acc_farmost','top5acc',\n",
    "                                           'top5acc_farmost','tau','rmse'])\n",
    "\n",
    "        #result['runid'] = [runid for x in range(len(result))]\n",
    "        allret.append(result)\n",
    "        alldata_ret.append(exp_data)\n",
    "\n",
    "    #final\n",
    "    rowcnt = len(allret[0])\n",
    "    metrics = np.empty((runs, rowcnt, 6))\n",
    "    for runid, ret in enumerate(allret):\n",
    "        metrics[runid, :,:] = ret[['top1acc','top1acc_farmost','top5acc',\n",
    "                                           'top5acc_farmost','tau','rmse']].values\n",
    "\n",
    "\n",
    "    #average\n",
    "    averagemat = np.mean(metrics[:,:,:], axis=0)\n",
    "    stdmat = np.std(metrics[:,:,:], axis=0)\n",
    "    dfhead = allret[0][['model' , 'prediction_length', 'halfmode', 'datamode','trainid']]\n",
    "    \n",
    "    \n",
    "    dfaverage = pd.DataFrame(averagemat, columns = ['top1acc','top1acc_farmost','top5acc',\n",
    "                                       'top5acc_farmost','tau','rmse'])\n",
    "    dfstd = pd.DataFrame(stdmat, columns = ['top1acc_std','top1acc_farmost_std','top5acc_std',\n",
    "                                       'top5acc_farmost_std','tau_std','rmse_std'])\n",
    "    dfret = pd.concat([dfhead, dfaverage, dfstd], axis=1)\n",
    "\n",
    "    #if exp_id != '':\n",
    "    #    dfret.to_csv(f'laptime2rank-evaluate-indy500-{exp_id}-result.csv', float_format='%.3f')\n",
    "\n",
    "    return dfret, alldata_ret\n",
    "\n",
    "\n",
    "def checkret_status(dataret, runid = 0, idx = 0,model='oracle'):\n",
    "    \"\"\"\n",
    "    check the test_ds track and lap status\n",
    "    \n",
    "        alldata_ret  ; for debug\n",
    "            [runid][halfmode,plen,trainid] -> (pred_ret, test_ds, rank_ret)\n",
    "                pred_ret[model] ->ã€€[tss, forecasts]\n",
    "                test_ds[model] ->ã€€test_ds\n",
    "                rank_ret[model] -> ([lap, elapsed_time, true_rank, pred_rank],forecast_ret)\n",
    "                    forecast_ret[completed_laps][carno] -> (elapsed_time, elapsed_time_hat)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _, plen = dataret[runid][idx][0][model][1][0].samples.shape\n",
    "    test_ds = dataret[runid][idx][1][model]\n",
    "   \n",
    "    \n",
    "    ds_iter =  iter(test_ds)\n",
    "    yfcnt = 0\n",
    "    pitcnt = 0\n",
    "    for recid in range(len(test_ds)):\n",
    "        test_rec = next(ds_iter)\n",
    "        \n",
    "        carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "        \n",
    "        track_rec,lap_rec = test_rec['feat_dynamic_real']\n",
    "        yfcnt += np.sum(track_rec[-plen:])\n",
    "        pitcnt += np.sum(lap_rec[-plen:])\n",
    "        \n",
    "    print('yfcnt:', yfcnt, 'pitcnt:',pitcnt)\n",
    "\n",
    "\n",
    "def get_ref_oracle_testds(plens, halfs, train_ratio=0.8, test_cars = []):           \n",
    "    \n",
    "    testset = {}\n",
    "    for prediction_length in plens:\n",
    "        for half_moving_win in halfs:\n",
    "            train_ds, test_ds,_,_ = make_dataset_byevent(events_id[_test_event], prediction_length,freq, \n",
    "                                         oracle_mode=MODE_ORACLE,\n",
    "                                         run_ts = _run_ts,\n",
    "                                         test_cars=test_cars,\n",
    "                                         half_moving_win= half_moving_win,\n",
    "                                         train_ratio=train_ratio)    \n",
    "            \n",
    "            # get key\n",
    "            key = '%d-%d'%(prediction_length,half_moving_win)\n",
    "            testset[key] = test_ds\n",
    "            \n",
    "    return testset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def checkret_confusionmat(dataret, ref_testset, runid= 0, testid = '', model='oracle'):\n",
    "    \"\"\"\n",
    "    output the 4x4 confusion matrix split by track and lap status\n",
    "    \n",
    "    input:\n",
    "        ref_oracle_testds  ; oracle test ds\n",
    "    \n",
    "    \"\"\"\n",
    "    plen_length = len(dataret[runid])\n",
    "    \n",
    "    dflist = []\n",
    "    for idx in range(plen_length):\n",
    "        _, plen = dataret[runid][idx][0][model][1][0].samples.shape\n",
    "        test_ds = dataret[runid][idx][1][model]\n",
    "        rank_ret = dataret[runid][idx][2][model][0]\n",
    "\n",
    "        key = '%d-%d'%(plen,0)\n",
    "        if key not in ref_testset:\n",
    "            print(f'error, {key} not found in ref_testset')\n",
    "            continue\n",
    "        \n",
    "        ref_oracle_testds = ref_testset[key]\n",
    "        if len(ref_oracle_testds) != len(test_ds):\n",
    "            print('error, size of testds mismatch', len(ref_oracle_testds), len(test_ds))\n",
    "            continue\n",
    "\n",
    "        # confusion matrix for <trackstatus, lapstatus> type: 00,01,10,11\n",
    "        # lap(start lap of prediction)  -> type\n",
    "        lapmap = {}\n",
    "        ds_iter =  iter(ref_oracle_testds)\n",
    "        for recid in range(len(ref_oracle_testds)):\n",
    "            test_rec = next(ds_iter) \n",
    "\n",
    "            carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "\n",
    "            track_rec,lap_rec = test_rec['feat_dynamic_real']\n",
    "            yfcnt = np.sum(track_rec[-plen:])\n",
    "            pitcnt = np.sum(lap_rec[-plen:])    \n",
    "\n",
    "            #laptype = ('0' if yfcnt==0 else '1') + ('0' if pitcnt==0 else '1')\n",
    "\n",
    "            lap = len(track_rec) - plen + 1\n",
    "            if lap not in lapmap:\n",
    "                #lapmap[lap] = laptype\n",
    "                lapmap[lap] = (yfcnt, pitcnt)\n",
    "            else:\n",
    "                oldtype = lapmap[lap]\n",
    "                lapmap[lap] = (yfcnt + oldtype[0], pitcnt + oldtype[1])\n",
    "\n",
    "\n",
    "        #split the rank_ret by laptype\n",
    "        types=['00','10','01','11']\n",
    "        acc_ret = []\n",
    "        for laptype in types:\n",
    "            check_ret = []\n",
    "            for item in rank_ret:\n",
    "                typecnt = lapmap[item[0]]\n",
    "\n",
    "                thetype = ('0' if typecnt[0]==0 else '1') + ('0' if typecnt[1]==0 else '1')\n",
    "\n",
    "                if thetype == laptype:\n",
    "                    check_ret.append(item)\n",
    "            # get acc\n",
    "            metrics = get_acc(check_ret,plen)\n",
    "            recret = [testid, plen, laptype, len(check_ret)]\n",
    "            recret.extend(metrics[0])\n",
    "            acc_ret.append(recret)\n",
    "\n",
    "        #add all test\n",
    "        metrics = get_acc(rank_ret,plen)\n",
    "        recret = [testid, plen, 'aa', len(rank_ret)]\n",
    "        recret.extend(metrics[0])\n",
    "        acc_ret.append(recret)\n",
    "        \n",
    "        _dfacc = pd.DataFrame(acc_ret, columns = ['testid','plen',\n",
    "                                'type','reccnt','top1acc','top1acc_farmost','top5acc',\n",
    "                                'top5acc_farmost','tau','rmse'])\n",
    "        \n",
    "        dflist.append(_dfacc)\n",
    "    \n",
    "    dfacc = pd.concat(dflist, axis=0)\n",
    "    \n",
    "    return dfacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_testds(datamode, test_cars=[]):\n",
    "    \"\"\"\n",
    "    report mae, etc\n",
    "    \"\"\"\n",
    "    for prediction_length in plens:\n",
    "        for half_moving_win in half:\n",
    "            train_ds, test_ds,_,_ = make_dataset_byevent(events_id[_test_event], prediction_length,freq, \n",
    "                                         oracle_mode=datamode,\n",
    "                                         run_ts = _run_ts,\n",
    "                                         test_cars=test_cars,\n",
    "                                         half_moving_win= half_moving_win,\n",
    "                                         train_ratio=train_ratio)\n",
    "            \n",
    "def dotest(config):\n",
    "    acclist = []\n",
    "    dflist = []\n",
    "    for model in config.keys():\n",
    "        conf = config[model]\n",
    "        for teststr in conf.keys():\n",
    "            testfunc = teststr\n",
    "            datamode = conf[teststr]\n",
    "\n",
    "\n",
    "            df, dataret = run_test(runs, plens, half, trainids, \n",
    "                                   train_ratio, testfunc, datamode=datamode,models=[model])\n",
    "\n",
    "            #concat\n",
    "            acc = checkret_confusionmat(dataret, ref_testset, \n",
    "                                        testid = teststr, model=model)\n",
    "            dflist.append(df)\n",
    "            acclist.append(acc)\n",
    "\n",
    "    dfret = pd.concat(dflist, axis=0)\n",
    "    dfacc = pd.concat(acclist, axis=0)\n",
    "    return dfret, dfacc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longterm_predict(predictor, runs, prediction_length, freq, \n",
    "                       useeid = False,\n",
    "                       run_ts= COL_LAPTIME, \n",
    "                       test_event = 'Indy500-2018',\n",
    "                       test_cars = [],  \n",
    "                       use_global_dict = True,\n",
    "                       oracle_mode = MODE_ORACLE,\n",
    "                       half_moving_win = 0,\n",
    "                       train_ratio=0.8,\n",
    "                       log_transform = False,\n",
    "                       verbose = False\n",
    "                ):\n",
    "    \"\"\"\n",
    "    split the ts to train and test part by the ratio\n",
    "    \n",
    "    input:\n",
    "        oracle_mode: false to simulate prediction in real by \n",
    "                set the covariates of track and lap status as nan in the testset\n",
    "        half_moving_win  ; extend to 0:-1 ,1:-1/2plen, 2:-plen\n",
    "    \n",
    "    \"\"\"    \n",
    "    run_ts= _run_ts \n",
    "    test_event = _test_event    \n",
    "    feature_mode = _feature_mode\n",
    "    context_ratio = _context_ratio\n",
    "    \n",
    "    init_track_model()\n",
    "    init_adjust_track_model()\n",
    "    \n",
    "    start = pd.Timestamp(\"01-01-2019\", freq=freq)  # can be different for each time series\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    forecasts_et = {}\n",
    "    \n",
    "    #select run\n",
    "    if runs>=0:\n",
    "        _laptime_data = [laptime_data[runs].copy()]\n",
    "    else:\n",
    "        _laptime_data = laptime_data.copy()\n",
    "    \n",
    "   \n",
    "    #add statistics for adjust test\n",
    "    # trackstatus, lapstatus\n",
    "    mae = [0,0]\n",
    "    \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    for _data in _laptime_data:\n",
    "        _train = []\n",
    "        _test = []\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        \n",
    "        else:\n",
    "            test_mode = False\n",
    "            #jump out\n",
    "            continue              \n",
    "            \n",
    "        #statistics on the ts length\n",
    "        ts_len = [ _entry.shape[1] for _entry in _data[2]]\n",
    "        max_len = int(np.max(ts_len))\n",
    "        train_len = int(np.max(ts_len) * train_ratio)\n",
    "        \n",
    "        if context_ratio != 0.:\n",
    "            # add this part to train set\n",
    "            context_len = int(np.max(ts_len) * context_ratio)\n",
    "        else:    \n",
    "            context_len = prediction_length*2\n",
    "        if context_len < 10:\n",
    "            context_len = 10\n",
    "            \n",
    "        if verbose:\n",
    "            #print(f'====event:{events[_data[0]]}, train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)}')\n",
    "            print(f'====event:{events[_data[0]]}, prediction_len={prediction_length},train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)},context_len={context_len}')\n",
    "                   \n",
    "        # process for each ts\n",
    "        for rowid in range(_data[2].shape[0]):\n",
    "            # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "            rec = _data[2][rowid].copy()\n",
    "            rec_raw = _data[2][rowid].copy()\n",
    "            \n",
    "            #remove nan(only tails)\n",
    "            nans, x= nan_helper(rec[run_ts,:])\n",
    "            nan_count = np.sum(nans)             \n",
    "            rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "            \n",
    "            # remove short ts\n",
    "            totallen = rec.shape[1]\n",
    "            if ( totallen < train_len + prediction_length):\n",
    "                if verbose:\n",
    "                    print(f'a short ts: carid={_data[1][rowid]}ï¼Œlen={totallen}')\n",
    "                continue                \n",
    "            \n",
    "            if use_global_dict:\n",
    "                carno = _data[1][rowid]\n",
    "                carid = global_carids[_data[1][rowid]]\n",
    "            else:\n",
    "                #simulation dataset, todo, fix the carids as decoder\n",
    "                carno = rowid\n",
    "                carid = rowid\n",
    "                \n",
    "            \n",
    "            if useeid:\n",
    "                static_cat = [carid, _data[0]]    \n",
    "            else:\n",
    "                static_cat = [carid]    \n",
    "                \n",
    "            #first, get target a copy    \n",
    "            # target can be COL_XXSTATUS\n",
    "            target_val = rec[run_ts,:].copy().astype(np.float32)\n",
    "            lap_status = rec[COL_LAPSTATUS, :].copy()\n",
    "            track_status = rec[COL_TRACKSTATUS, :].copy()\n",
    "            pitage_status = rec[COL_LAPS_INSTINT,:].copy()\n",
    "            # <3, totallen> \n",
    "            forecasts_et[carno] = np.zeros((5, totallen))\n",
    "            forecasts_et[carno][:,:] = np.nan\n",
    "            forecasts_et[carno][0,:] = rec[COL_LAPSTATUS, :].copy()\n",
    "            forecasts_et[carno][1,:] = rec[run_ts,:].copy().astype(np.float32)\n",
    "            forecasts_et[carno][2,:] = rec[run_ts,:].copy().astype(np.float32)\n",
    "            \n",
    "            if log_transform:\n",
    "                target_val = np.log(target_val + 1.0)                \n",
    "            \n",
    "            # adjust for disturbance analysis\n",
    "            if test_mode and test_flag(oracle_mode, MODE_DISTURB_ADJUSTPIT):\n",
    "                lap_status = rec[COL_LAPSTATUS, :].copy()\n",
    "                rec[COL_LAPSTATUS, :] = get_adjust_lapstatus(carno, lap_status)\n",
    "                \n",
    "            # selection of features\n",
    "            if test_flag(oracle_mode, MODE_NOTRACK) or test_flag(oracle_mode, MODE_ORACLE_LAPONLY):                \n",
    "                rec[COL_TRACKSTATUS, :] = 0\n",
    "            if test_flag(oracle_mode, MODE_NOLAP) or test_flag(oracle_mode, MODE_ORACLE_TRACKONLY):                \n",
    "                rec[COL_LAPSTATUS, :] = 0\n",
    "\n",
    "            test_rec_cnt = 0\n",
    "            if not test_mode:\n",
    "                \n",
    "                # all go to train set\n",
    "                _train.append({'target': target_val, \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [rec[COL_TRACKSTATUS,:],\n",
    "                                       rec[COL_LAPSTATUS,:]]\n",
    "                              }\n",
    "                              )\n",
    "            else:\n",
    "                # reset train_len\n",
    "                #context_len = prediction_length*2\n",
    "                #if context_len < 10:\n",
    "                #    context_len = 10\n",
    "                \n",
    "                #context_len = train_len\n",
    "                \n",
    "                # multiple test ts(rolling window as half of the prediction_length)\n",
    "\n",
    "                #step = -int(prediction_length/2) if half_moving_win else -prediction_length\n",
    "                if half_moving_win == 1:\n",
    "                    step = int(prediction_length/2)\n",
    "                elif half_moving_win == 2:\n",
    "                    step = prediction_length\n",
    "                else:\n",
    "                    step = 1\n",
    "                \n",
    "                #bug fix, fixed the split point for all cars/ts\n",
    "                #for endpos in range(max_len, context_len+prediction_length,step):\n",
    "                for endpos in range(context_len+prediction_length, max_len, step):\n",
    "\n",
    "                    #check if enough for this ts\n",
    "                    if endpos > totallen:\n",
    "                        break\n",
    "                        \n",
    "                    # RUN Prediction for single record\n",
    "                    _test = []\n",
    "                    \n",
    "                    # check pitstop(stint) in the last prediction\n",
    "                    # use ground truth of target before the last pitstop\n",
    "                    if np.sum(lap_status[endpos-2*prediction_length:endpos-prediction_length]) > 0:\n",
    "                        # pit found \n",
    "                        # adjust endpos \n",
    "                        pitpos = np.where(lap_status[endpos-2*prediction_length:endpos-prediction_length] == 1)\n",
    "                        endpos = endpos-2*prediction_length + pitpos[0][0] + prediction_length + 1\n",
    "                        #print('endpos:',endpos,pitpos)\n",
    "                        \n",
    "                        #check if enough for this ts\n",
    "                        if endpos > totallen:\n",
    "                            break                        \n",
    "                            \n",
    "                        #reset target, status\n",
    "                        target_val = rec[run_ts,:].copy().astype(np.float32)\n",
    "                        rec[COL_LAPSTATUS, :] = lap_status\n",
    "                        rec[COL_TRACKSTATUS, :] = track_status   \n",
    "                        rec[COL_LAPS_INSTINT, :] = pitage_status  \n",
    "                    \n",
    "                    track_rec = rec[COL_TRACKSTATUS, :endpos].copy()\n",
    "                    lap_rec = rec[COL_LAPSTATUS, :endpos].copy()\n",
    "                    pitage_rec = rec[COL_LAPS_INSTINT, :endpos].copy()\n",
    "                    \n",
    "                    caution_laps_instint = int(rec[COL_CAUTION_LAPS_INSTINT, endpos -prediction_length - 1])\n",
    "                    laps_instint = int(rec[COL_LAPS_INSTINT, endpos -prediction_length - 1])\n",
    "                    \n",
    "\n",
    "                    # test mode\n",
    "                    if test_flag(oracle_mode, MODE_TESTCURTRACK):\n",
    "                        # since nan does not work, use cur-val instead\n",
    "                        track_rec[-prediction_length:] = track_rec[-prediction_length - 1]\n",
    "                        #track_rec[-prediction_length:] = random.randint(0,1)\n",
    "                        #lap_rec[-prediction_length:] = lap_rec[-prediction_length - 1]\n",
    "                        lap_rec[-prediction_length:] = 0\n",
    "                        #for pitage, just assume there is no pit\n",
    "                        start_pitage = pitage_rec[-prediction_length - 1]\n",
    "                        pitage_rec[-prediction_length:] = np.array([x+start_pitage+1 for x in range(prediction_length)])\n",
    "                        \n",
    "                    elif test_flag(oracle_mode, MODE_TESTZERO):\n",
    "                        #set prediction part as nan\n",
    "                        #track_rec[-prediction_length:] = np.nan\n",
    "                        #lap_rec[-prediction_length:] = np.nan\n",
    "                        track_rec[-prediction_length:] = 0\n",
    "                        lap_rec[-prediction_length:] = 0        \n",
    "                        #for pitage, just assume there is no pit\n",
    "                        start_pitage = pitage_rec[-prediction_length - 1]\n",
    "                        pitage_rec[-prediction_length:] = np.array([x+start_pitage+1 for x in range(prediction_length)])\n",
    "\n",
    "                    # predicting with status model\n",
    "                    if test_flag(oracle_mode, MODE_PREDTRACK):\n",
    "                        predrec = get_track_model(track_rec, endpos, prediction_length)\n",
    "                        track_rec[-prediction_length:] = predrec\n",
    "                        #lap_rec[-prediction_length:] = 0\n",
    "                        \n",
    "                    if test_flag(oracle_mode, MODE_PREDPIT):\n",
    "                        #predrec = get_track_model(track_rec, endpos, prediction_length)\n",
    "                        #track_rec[-prediction_length:] = predrec\n",
    "                        lap_rec[-prediction_length:] = get_pit_model(caution_laps_instint,\n",
    "                                                                    laps_instint,prediction_length)\n",
    "                        \n",
    "                        #for pitage, use the predicted lap info to update pitage\n",
    "                        start_pitage = pitage_rec[-prediction_length - 1]\n",
    "                        for pos in range(prediction_length):\n",
    "                            if lap_rec[-prediction_length + pos]==0:\n",
    "                                pitage_rec[-prediction_length + pos] = start_pitage+1\n",
    "                            else:\n",
    "                                #new pit\n",
    "                                start_pitage = 0\n",
    "                                pitage_rec[-prediction_length + pos] = start_pitage\n",
    "                        \n",
    "                    # disturbe analysis\n",
    "                    if test_flag(oracle_mode, MODE_DISTURB_CLEARTRACK):\n",
    "                        # clear the oracle track status\n",
    "                        # future 1s in trackstatus\n",
    "                        # pattern like 0 1 xx\n",
    "                        for _pos in range(-prediction_length + 1, -1):\n",
    "                            if track_rec[_pos - 1] == 0:\n",
    "                                track_rec[_pos] = 0\n",
    "                                \n",
    "                    if test_flag(oracle_mode, MODE_DISTURB_ADJUSTTRACK):\n",
    "                        # adjust the end position of track, or caution lap length\n",
    "                        # find the end of caution laps\n",
    "                        _tail = 0\n",
    "                        for _pos in range(-1,-prediction_length + 1,-1):\n",
    "                            if track_rec[_pos] == 1:\n",
    "                                #find the tail\n",
    "                                _tail = _pos\n",
    "                                break\n",
    "                        if _tail != 0:\n",
    "                            #found\n",
    "                            adjustrec = adjust_track_model(track_rec, endpos, prediction_length, _tail)\n",
    "                            track_rec[-prediction_length:] = adjustrec\n",
    "                        \n",
    "                    #if test_flag(oracle_mode, MODE_DISTURB_ADJUSTPIT):\n",
    "                    #    # adjust the position of pit\n",
    "                    #    if np.sum(lap_rec[-prediction_length:]) > 0:\n",
    "                    #        adjustrec = adjust_pit_model(lap_rec, endpos, prediction_length)\n",
    "                    #        lap_rec[-prediction_length:] = adjustrec\n",
    "                        \n",
    "                    #okay, end of adjustments, test difference here\n",
    "                    # rec_raw .vs. track_rec, lap_rec\n",
    "                    track_rec_raw = rec_raw[COL_TRACKSTATUS, :endpos]\n",
    "                    lap_rec_raw = rec_raw[COL_LAPSTATUS, :endpos]\n",
    "                    \n",
    "                    mae[0] = mae[0] + np.nansum(np.abs(track_rec[-prediction_length:] - track_rec_raw[-prediction_length:]))\n",
    "                    mae[1] = mae[1] + np.nansum(np.abs(lap_rec[-prediction_length:] - lap_rec_raw[-prediction_length:]))\n",
    "\n",
    "                    if feature_mode == FEATURE_STATUS:\n",
    "                        _test.append({'target': target_val[:endpos].astype(np.float32), \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [track_rec,lap_rec]\n",
    "                                 }\n",
    "                              )   \n",
    "                    elif feature_mode == FEATURE_PITAGE:\n",
    "                        _test.append({'target': target_val[:endpos].astype(np.float32), \n",
    "                                    'start': start, \n",
    "                                    'feat_static_cat': static_cat,\n",
    "                                    'feat_dynamic_real': [track_rec,lap_rec,pitage_rec]\n",
    "                                     }\n",
    "                                  )   \n",
    "                    \n",
    "                    \n",
    "                    # RUN Prediction here, for single record\n",
    "                    test_ds = ListDataset(_test, freq=freq)\n",
    "                    \n",
    "                    forecast_it, ts_it = make_evaluation_predictions(\n",
    "                        dataset=test_ds,  # test dataset\n",
    "                        predictor=predictor,  # predictor\n",
    "                        num_samples=100,  # number of sample paths we want for evaluation\n",
    "                    )\n",
    "\n",
    "                    forecasts = list(forecast_it)\n",
    "                    tss = list(ts_it)\n",
    "                    \n",
    "                    #get prediction result \n",
    "                    forecast_laptime_mean = np.mean(forecasts[0].samples, axis=0).reshape((prediction_length))\n",
    "                    #update target_val\n",
    "                    target_val[endpos-prediction_length:endpos] = forecast_laptime_mean \n",
    "                    rec[COL_TRACKSTATUS, endpos-prediction_length:endpos] = track_rec[-prediction_length:]\n",
    "                    rec[COL_LAPSTATUS, endpos-prediction_length:endpos] = lap_rec[-prediction_length:]\n",
    "                    rec[COL_LAPS_INSTINT, endpos-prediction_length:endpos] = pitage_rec[-prediction_length:]\n",
    "                    \n",
    "                    #save forecast\n",
    "                    #save the prediction\n",
    "                    completed_laps = len(tss[0]) - prediction_length + 1\n",
    "                    #print('car no:', carno, 'completed_laps:', completed_laps)\n",
    "                    forecasts_et[carno][2, len(tss[0]) - prediction_length:len(tss[0])] = forecast_laptime_mean.copy()\n",
    "                    \n",
    "                    \n",
    "                    test_rec_cnt += 1\n",
    "            \n",
    "            #one ts\n",
    "            if verbose:\n",
    "                print(f'carno:{carno}, totallen:{totallen}, nancount:{nan_count}, test_reccnt:{test_rec_cnt}')\n",
    "\n",
    "        #train_set.extend(_train)\n",
    "        #test_set.extend(_test)\n",
    "\n",
    "    #print(f'train len:{len(train_set)}, test len:{len(test_set)}, mae_track:{mae[0]},mae_lap:{mae[1]},')\n",
    "    \n",
    "    #train_ds = ListDataset(train_set, freq=freq)\n",
    "    #test_ds = ListDataset(test_set, freq=freq)    \n",
    "    \n",
    "    return forecasts_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc rank\n",
    "def eval_stint_rank(forecasts_et, prediction_length, start_offset):\n",
    "    \"\"\"\n",
    "    evaluate rank by laptime forecasting\n",
    "    \n",
    "    input:\n",
    "        test_ds       ; must be test set for a single event, because test_ds itself does not \n",
    "                        contain features to identify the eventid\n",
    "        start_offset[]; elapsed time for lap0, for one specific event\n",
    "        tss,forecasts ; forecast result\n",
    "        prediction_length ; \n",
    "    return:\n",
    "        rank_ret      ;  [lap, elapsed_time, true_rank, pred_rank] \n",
    "        forecasts_et  ;  {[completed_laps][carno]} ->(elapsed_time, elapsed_time_pred)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    #get car list for this lap\n",
    "    carlist = list(forecasts_et.keys())\n",
    "    #print('carlist:', carlist)\n",
    "\n",
    "    caridmap={key:idx for idx, key in enumerate(carlist)}    \n",
    "\n",
    "    #convert to elapsedtime\n",
    "    #todo, Indy500 - > 200 max laps\n",
    "    maxlap = 200\n",
    "    \n",
    "    elapsed_time = np.zeros((2, len(carlist), maxlap))\n",
    "    elapsed_time[:,:] = np.nan\n",
    "    \n",
    "    for carno in forecasts_et.keys():\n",
    "\n",
    "        #start_offset is global var\n",
    "        if isinstance(start_offset, pd.core.frame.DataFrame):\n",
    "            offset = start_offset[(start_offset['car_number']==carno)].elapsed_time.values[0] \n",
    "        \n",
    "        lapnum = len(forecasts_et[carno][1,:])\n",
    "        \n",
    "        laptime_array = forecasts_et[carno][1,:]\n",
    "        elapsed = np.cumsum(laptime_array) + offset\n",
    "        elapsed_time[0, caridmap[carno],:lapnum] = elapsed\n",
    "        \n",
    "        laptime_array = forecasts_et[carno][2,:]\n",
    "        elapsed = np.cumsum(laptime_array) + offset\n",
    "        elapsed_time[1, caridmap[carno],:lapnum] = elapsed\n",
    "        \n",
    "        #maxlap = max(maxlap, len(forecasts_et[carno][1,:]))\n",
    "\n",
    "    #calculate rank, support nan\n",
    "    idx = np.argsort(elapsed_time[0], axis=0)\n",
    "    true_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "    idx = np.argsort(elapsed_time[1], axis=0)\n",
    "    pred_rank = np.argsort(idx, axis=0)\n",
    "        \n",
    "    # save the rank back\n",
    "    for carno in forecasts_et.keys():\n",
    "        lapnum = len(forecasts_et[carno][1,:])\n",
    "        \n",
    "        forecasts_et[carno][3,:] = true_rank[caridmap[carno],:lapnum]\n",
    "        forecasts_et[carno][4,:] = pred_rank[caridmap[carno],:lapnum]\n",
    "    \n",
    "    return forecasts_et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(prediction_length, half_moving_win, train_ratio=0.8, trainid=\"r0.8\",\n",
    "                   test_event='Indy500-2018', test_cars = [], \n",
    "                   datamode = MODE_ORACLE,model = 'oracle'):\n",
    "    \"\"\"\n",
    "    dependency: test_event, test on one event only\n",
    "    \n",
    "    \"\"\"\n",
    "    retdf = []\n",
    "    pred_ret = {}\n",
    "    ds_ret = {}\n",
    "    rank_result = {}\n",
    "    predictor = {}\n",
    "\n",
    "    #for model in models:\n",
    "    print('exp:',inspect.stack()[0][3],'model:', model, \n",
    "          'datamode:', get_modestr(datamode),'eval:', _exp_id )\n",
    "    predictor[model] = load_model(prediction_length, model,\n",
    "                                       trainid=trainid)\n",
    "\n",
    "    ### create test dataset\n",
    "    forecasts = longterm_predict(predictor[model],\n",
    "                                   events_id[_test_event], prediction_length,freq, \n",
    "                                     oracle_mode=datamode,\n",
    "                                     run_ts = _run_ts,\n",
    "                                     test_cars=test_cars,\n",
    "                                     half_moving_win= half_moving_win,\n",
    "                                     train_ratio=train_ratio\n",
    "                                    )\n",
    "\n",
    "    #forecasts = eval_stint_rank(forecasts_et, prediction_length, \n",
    "    #                            global_start_offset[test_event])\n",
    "            \n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sign(diff):\n",
    "    if diff > 0:\n",
    "        sign = 1\n",
    "    elif diff < 0:\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 0\n",
    "    return sign\n",
    "                \n",
    "def get_stint_acc(forecasts, TRIM=2, currank = False):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        TRIM     ; steady lap of the rank (before pit_inlap, pit_outlap)\n",
    "        forecasts;  carno -> [5,totallen]\n",
    "                0; lap_status\n",
    "                3; true_rank\n",
    "                4; pred_rank\n",
    "    output:\n",
    "        carno, stintid, startrank, endrank, diff, sign\n",
    "        \n",
    "    \"\"\"\n",
    "    rankret = []\n",
    "    for carno in forecasts.keys():\n",
    "        lapnum = len(forecasts[carno][1,:])\n",
    "        true_rank = forecasts[carno][3,:]\n",
    "        pred_rank = forecasts[carno][4,:]\n",
    "        \n",
    "        pitpos_list = np.where(forecasts[carno][0,:] == 1)[0]\n",
    "        \n",
    "        stintid = 0\n",
    "        startrank = true_rank[0]\n",
    "        \n",
    "        for pitpos in pitpos_list:\n",
    "            endrank = true_rank[pitpos-TRIM]\n",
    "            diff = endrank - startrank\n",
    "            sign = get_sign(diff)\n",
    "                \n",
    "            if currank:\n",
    "                #force into currank model, zero doesn't work here\n",
    "                pred_endrank = startrank\n",
    "                pred_diff = pred_endrank - startrank\n",
    "                pred_sign = get_sign(pred_diff)\n",
    "                \n",
    "            else:\n",
    "                pred_endrank = pred_rank[pitpos-TRIM]\n",
    "                pred_diff = pred_endrank - startrank\n",
    "                pred_sign = get_sign(pred_diff)\n",
    "            \n",
    "            rankret.append([carno, stintid, startrank, \n",
    "                            endrank, diff, sign,\n",
    "                            pred_endrank, pred_diff, pred_sign\n",
    "                            ])\n",
    "            \n",
    "            stintid += 1\n",
    "            startrank = true_rank[pitpos-TRIM]\n",
    "            \n",
    "        #end\n",
    "        if pitpos_list[-1] < lapnum - 1:\n",
    "            endrank = true_rank[-1]\n",
    "            diff = endrank - startrank\n",
    "            sign = get_sign(diff)\n",
    "\n",
    "            if currank:\n",
    "                #force into currank model, zero doesn't work here\n",
    "                pred_endrank = startrank\n",
    "                pred_diff = pred_endrank - startrank\n",
    "                pred_sign = get_sign(pred_diff)\n",
    "            else:\n",
    "                pred_endrank = pred_rank[-1]\n",
    "                pred_diff = pred_endrank - startrank\n",
    "                pred_sign = get_sign(pred_diff)\n",
    "            \n",
    "            rankret.append([carno, stintid, startrank, \n",
    "                            endrank, diff, sign,\n",
    "                            pred_endrank, pred_diff, pred_sign\n",
    "                            ])\n",
    "            \n",
    "    #add to df\n",
    "    df = pd.DataFrame(rankret, columns =['carno', 'stintid', 'startrank', \n",
    "                                         'endrank', 'diff', 'sign',\n",
    "                                         'pred_endrank', 'pred_diff', 'pred_sign',\n",
    "                                        ])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def get_stint_acc_old(forecasts, TRIM=2):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        TRIM     ; steady lap of the rank (before pit_inlap, pit_outlap)\n",
    "        forecasts;  carno -> [5,totallen]\n",
    "                0; lap_status\n",
    "                3; true_rank\n",
    "                4; pred_rank\n",
    "    output:\n",
    "        carno, stintid, startrank, endrank, diff, sign\n",
    "        \n",
    "    \"\"\"\n",
    "    rankret = []\n",
    "    for carno in forecasts.keys():\n",
    "        lapnum = len(forecasts[carno][1,:])\n",
    "        true_rank = forecasts[carno][3,:]\n",
    "        pred_rank = forecasts[carno][4,:]\n",
    "        \n",
    "        pitpos_list = np.where(forecasts[carno][0,:] == 1)[0]\n",
    "        \n",
    "        stintid = 0\n",
    "        startrank = true_rank[0]\n",
    "        \n",
    "        for pitpos in pitpos_list:\n",
    "            endrank = true_rank[pitpos-TRIM]\n",
    "            diff = endrank - startrank\n",
    "            sign = get_sign(diff)\n",
    "                \n",
    "            pred_endrank = pred_rank[pitpos-TRIM]\n",
    "            pred_diff = pred_endrank - startrank\n",
    "            pred_sign = get_sign(pred_diff)\n",
    "            \n",
    "            rankret.append([carno, stintid, startrank, \n",
    "                            endrank, diff, sign,\n",
    "                            pred_endrank, pred_diff, pred_sign\n",
    "                            ])\n",
    "            \n",
    "            stintid += 1\n",
    "            startrank = true_rank[pitpos-TRIM]\n",
    "            \n",
    "        #end\n",
    "        if pitpos_list[-1] < lapnum - 1:\n",
    "            endrank = true_rank[-1]\n",
    "            diff = endrank - startrank\n",
    "            sign = get_sign(diff)\n",
    "                \n",
    "            pred_endrank = pred_rank[-1]\n",
    "            pred_diff = pred_endrank - startrank\n",
    "            pred_sign = get_sign(pred_diff)\n",
    "            \n",
    "            rankret.append([carno, stintid, startrank, \n",
    "                            endrank, diff, sign,\n",
    "                            pred_endrank, pred_diff, pred_sign\n",
    "                            ])\n",
    "            \n",
    "    #add to df\n",
    "    df = pd.DataFrame(rankret, columns =['carno', 'stintid', 'startrank', \n",
    "                                         'endrank', 'diff', 'sign',\n",
    "                                         'pred_endrank', 'pred_diff', 'pred_sign',\n",
    "                                        ])\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# configurataion\n",
    "#\n",
    "# model path:  <_dataset_id>/<_task_id>-<trainid>/\n",
    "#_dataset_id = 'indy2013-2018-nocarid'\n",
    "_dataset_id = 'indy2013-2018'\n",
    "_test_event = 'Indy500-2018'\n",
    "#_test_event = 'Indy500-2019'\n",
    "\n",
    "_feature_mode = FEATURE_STATUS\n",
    "_context_ratio = 0.\n",
    "\n",
    "_task_id = 'timediff'  # rank,laptime, the trained model's task\n",
    "_run_ts = COL_TIMEDIFF   #COL_LAPTIME,COL_RANK\n",
    "_exp_id='timediff2rank'  #rank, laptime, laptim2rank, timediff2rank... \n",
    "\n",
    "_task_id = 'lapstatus'  # rank,laptime, the trained model's task\n",
    "_run_ts = COL_LAPSTATUS   #COL_LAPTIME,COL_RANK\n",
    "_exp_id='lapstatus'  #rank, laptime, laptim2rank, timediff2rank... \n",
    "\n",
    "_task_id = 'laptime'  # rank,laptime, the trained model's task\n",
    "_run_ts = COL_LAPTIME   #COL_LAPTIME,COL_RANK\n",
    "_exp_id='laptime2rank'  #rank, laptime, laptim2rank, timediff2rank... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_ssd/hpda/anaconda3/envs/gluonts/lib/python3.6/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/scratch_ssd/hpda/anaconda3/envs/gluonts/lib/python3.6/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# parameters\n",
    "#\n",
    "stagedata = {}\n",
    "global_carids = {}\n",
    "traindata = None\n",
    "cur_carid = 0\n",
    "#years = ['2011','2012','2013', '2014', '2015', '2016', '2017']\n",
    "\n",
    "years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "#events = ['Indy500']\n",
    "events = [f'Indy500-{x}' for x in years]\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "dbid = f'Indy500_{years[0]}_{years[-1]}'\n",
    "\n",
    "global_start_offset = {}\n",
    "\n",
    "for event in events:\n",
    "    #dataid = f'{event}-{year}'\n",
    "    #alldata, rankdata, acldata, flagdata\n",
    "    stagedata[event] = load_data(event)\n",
    "\n",
    "    alldata, rankdata, acldata = stagedata[event]\n",
    "    \n",
    "    #offset\n",
    "    global_start_offset[event] = rankdata[rankdata['completed_laps']==0][['car_number','elapsed_time']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from here\n",
    "import pickle\n",
    "#with open('laptime_rank_timediff_fulltest-oracle-%s.pickle'%year, 'rb') as f:\n",
    "with open(f'laptime_rank_timediff_pit-oracle-{dbid}.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    global_carids, laptime_data = pickle.load(f, encoding='latin1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"1min\"\n",
    "#decode global_carids\n",
    "decode_carids={carid:carno for carno, carid in global_carids.items()}\n",
    "    \n",
    "#useeid = False\n",
    "#interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "#ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "#if useeid:\n",
    "#    cardinality = [len(global_carids), len(laptime_data)]\n",
    "#else:\n",
    "\n",
    "#    cardinality = [len(global_carids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oracle test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtest(modelname, model, datamode, naivemode, trainid= \"2018\"):\n",
    "\n",
    "    forecast = run_exp(2,2, train_ratio =0.1 , trainid = trainid, \n",
    "               datamode=datamode, model=model)\n",
    "    forecasts_et = eval_stint_rank(forecast, 2, \n",
    "                        global_start_offset[_test_event])\n",
    "    df = get_stint_acc(forecasts_et, currank = naivemode)\n",
    "    correct = df[df['sign']==df['pred_sign']]\n",
    "    acc = len(correct)/len(df)\n",
    "    mae = np.sum(np.abs(df['pred_diff'].values - df['diff'].values))/len(df)\n",
    "    print('pred:', acc, mae)\n",
    "    \n",
    "    return acc, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle datamode: MODE_ORACLE, eval: laptime2rank\n",
      "predicting model=oracle, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0.6126126126126126 4.833333333333333\n",
      "exp: run_exp model: oracle datamode: MODE_ORACLE_LAPONLY, eval: laptime2rank\n",
      "predicting model=oracle, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0.6171171171171171 4.882882882882883\n",
      "exp: run_exp model: oracle datamode: MODE_ORACLE_TRACKONLY,MODE_ORACLE_LAPONLY, eval: laptime2rank\n",
      "predicting model=oracle, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0.6081081081081081 4.846846846846847\n",
      "exp: run_exp model: oracle datamode: MODE_PREDTRACK,MODE_PREDPIT, eval: laptime2rank\n",
      "predicting model=oracle, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0.4774774774774775 7.7792792792792795\n",
      "exp: run_exp model: oracle datamode: MODE_TESTCURTRACK, eval: laptime2rank\n",
      "predicting model=oracle, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n"
     ]
    }
   ],
   "source": [
    "config = {'fulloracle':['oracle',MODE_ORACLE,False],\n",
    "          'laponly':['oracle',MODE_ORACLE_LAPONLY,False],\n",
    "          'notracklap':['oracle',MODE_NOTRACK + MODE_NOLAP,False],\n",
    "          'fullpred':['oracle',MODE_PREDTRACK + MODE_PREDPIT,False],\n",
    "          'curtrack':['oracle',MODE_TESTCURTRACK,False],\n",
    "          'zerotrack':['oracle',MODE_TESTZERO,False],\n",
    "          'predtrack':['oracle',MODE_PREDTRACK + MODE_ORACLE_TRACKONLY,False],\n",
    "          'predpit':['oracle',MODE_PREDPIT + MODE_ORACLE_LAPONLY,False],\n",
    "          'deepAR':['deepAR',MODE_ORACLE,False],\n",
    "          'naive':['zero',MODE_ORACLE, True],\n",
    "         }\n",
    "\n",
    "cols = ['runid','acc','mae']\n",
    "\n",
    "result = []\n",
    "for modelname in config.keys():\n",
    "    acc, mae = runtest(modelname, config[modelname][0],\n",
    "                       config[modelname][1],config[modelname][2])\n",
    "    \n",
    "    result.append([modelname, acc, mae])\n",
    "    \n",
    "retd = pd.DataFrame(result,columns=cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retd.to_csv(f'stint_longterm_{_test_event}.csv', float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = run_exp(2,2, train_ratio =0.1 , trainid = \"2018\", \n",
    "                   datamode=MODE_ORACLE, model='oracle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_et2 = eval_stint_rank(forecast, 2, \n",
    "                            global_start_offset[_test_event])\n",
    "df2 = get_stint_acc(forecasts_et2)\n",
    "correct = df2[df2['sign']==df2['pred_sign']]\n",
    "acc = len(correct)/len(df2)\n",
    "mae = np.sum(np.abs(df2['pred_diff'].values - df2['diff'].values))/len(df2)\n",
    "print('oracle:', acc, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['carno']==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = df[df['sign']==df['pred_sign']]\n",
    "acc = len(correct)/len(df)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.sum(np.abs(df['pred_diff'].values - df['diff'].values))/len(df)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = run_exp(2,2, train_ratio =0.1 , trainid = \"2018\", \n",
    "                   datamode=MODE_ORACLE, model='zero')\n",
    "forecasts_et = eval_stint_rank(forecast, 2, \n",
    "                            global_start_offset[_test_event])\n",
    "df = get_stint_acc(forecasts_et,currank=True)\n",
    "correct = df[df['sign']==df['pred_sign']]\n",
    "acc = len(correct)/len(df)\n",
    "mae = np.sum(np.abs(df['pred_diff'].values - df['diff'].values))/len(df)\n",
    "print('zero:', acc, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pits = np.where(forecasts_et[12][0]==1)[0]\n",
    "print(pits)\n",
    "pits = pits-2\n",
    "print(pits)\n",
    "forecasts_et[12][3][pits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_et[12][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_et[12][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['carno']==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = run_exp(2,2, train_ratio =0.1 , trainid = \"2018\", \n",
    "                   datamode=MODE_ORACLE, model='deepAR')\n",
    "forecasts_et = eval_stint_rank(forecast, 2, \n",
    "                            global_start_offset[_test_event])\n",
    "df = get_stint_acc(forecasts_et)\n",
    "correct = df[df['sign']==df['pred_sign']]\n",
    "acc = len(correct)/len(df)\n",
    "mae = np.sum(np.abs(df['pred_diff'].values - df['diff'].values))/len(df)\n",
    "print('deepAR:', acc, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = run_exp(2,2, train_ratio =0.1 , trainid = \"2018\", \n",
    "                   datamode=MODE_NOTRACK + MODE_NOLAP, model='oracle')\n",
    "forecasts_et = eval_stint_rank(forecast, 2, \n",
    "                            global_start_offset[_test_event])\n",
    "df = get_stint_acc(forecasts_et)\n",
    "correct = df[df['sign']==df['pred_sign']]\n",
    "acc = len(correct)/len(df)\n",
    "mae = np.sum(np.abs(df['pred_diff'].values - df['diff'].values))/len(df)\n",
    "print('pred:', acc, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = run_exp(2,2, train_ratio =0.1 , trainid = \"2018\", \n",
    "                   datamode=MODE_PREDTRACK + MODE_PREDPIT, model='oracle')\n",
    "forecasts_et = eval_stint_rank(forecast, 2, \n",
    "                            global_start_offset[_test_event])\n",
    "df = get_stint_acc(forecasts_et)\n",
    "correct = df[df['sign']==df['pred_sign']]\n",
    "acc = len(correct)/len(df)\n",
    "mae = np.sum(np.abs(df['pred_diff'].values - df['diff'].values))/len(df)\n",
    "print('pred:', acc, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = run_exp(2,2, train_ratio =0.1 , trainid = \"2018\", \n",
    "                   datamode=MODE_ORACLE_LAPONLY, model='oracle')\n",
    "forecasts_et = eval_stint_rank(forecast, 2, \n",
    "                            global_start_offset[_test_event])\n",
    "df = get_stint_acc(forecasts_et)\n",
    "correct = df[df['sign']==df['pred_sign']]\n",
    "acc = len(correct)/len(df)\n",
    "mae = np.sum(np.abs(df['pred_diff'].values - df['diff'].values))/len(df)\n",
    "print('pred:', acc, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### long term predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,0,1,0,0,0,1,0,0,0,0,0])\n",
    "np.where(a==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[1,2],[3,4],[np.nan,5],[2,9],[np.nan,1]])\n",
    "idx = np.argsort(X, axis=0)\n",
    "true_rank = np.argsort(idx, axis=0)\n",
    "true_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['carno']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['carno']==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['carno']==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['carno']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
