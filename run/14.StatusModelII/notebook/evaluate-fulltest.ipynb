{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate-fulltest\n",
    "\n",
    "based on: Laptime2Rank-evaluate-fulltest-disturbance\n",
    "\n",
    "+ rank prediction directly\n",
    "+ rank prediction by laptime2rank\n",
    "+ laptime prediction\n",
    "\n",
    "\n",
    "support:\n",
    "+ train/test split by ratio or event\n",
    "+ incremental training evaluation(adjust ratio)\n",
    "+ go beyond curtrack and zerotrack by modeling the track status\n",
    "+ halfwin mode(0:no, 1:halfwin, 2:continous)\n",
    "+ split by stage, support all events (todo)\n",
    "\n",
    "+ disturbance analysis by adding disturbance to oracle trackstatus and lapstatus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import inspect\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pathlib import Path\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.deep_factor import DeepFactorEstimator\n",
    "from gluonts.model.deepstate import DeepStateEstimator\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "from gluonts.distribution.multivariate_gaussian import MultivariateGaussianOutput\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.model.prophet import ProphetPredictor\n",
    "from gluonts.model.r_forecast import RForecastPredictor\n",
    "from indycar.model.NaivePredictor import NaivePredictor\n",
    "from indycar.model.ZeroPredictor import ZeroPredictor\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch_hdd/hpda/indycar/notebook/14.StatusModelII'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "random.seed()\n",
    "os.getcwd()\n",
    "#GPUID = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# remove NaN at the tail\n",
    "# there should be no nans in the middle of the ts\n",
    "COL_LAPTIME=0\n",
    "COL_RANK=1\n",
    "COL_TRACKSTATUS=2\n",
    "COL_LAPSTATUS=3\n",
    "COL_TIMEDIFF=4\n",
    "COL_CAUTION_LAPS_INSTINT=5\n",
    "COL_LAPS_INSTINT= 6\n",
    "\n",
    "# oracle mode\n",
    "MODE_ORACLE = 1024  # oracle = track + lap\n",
    "MODE_ORACLE_TRACKONLY = 1\n",
    "MODE_ORACLE_LAPONLY = 2   \n",
    "   \n",
    "\n",
    "# oracle mode for training\n",
    "MODE_NOLAP = 1   \n",
    "MODE_NOTRACK = 2\n",
    "\n",
    "# predicting mode\n",
    "MODE_TESTZERO = 4\n",
    "MODE_TESTCURTRACK = 8\n",
    "\n",
    "MODE_PREDTRACK = 16\n",
    "MODE_PREDPIT = 32\n",
    "\n",
    "# disturbe analysis\n",
    "MODE_DISTURB_CLEARTRACK = 64\n",
    "MODE_DISTURB_ADJUSTTRACK = 128\n",
    "MODE_DISTURB_ADJUSTPIT = 256\n",
    "\n",
    "\n",
    "_mode_map = {MODE_ORACLE:'MODE_ORACLE',MODE_ORACLE_TRACKONLY:'MODE_ORACLE_TRACKONLY',\n",
    "            MODE_ORACLE_LAPONLY:'MODE_ORACLE_LAPONLY',\n",
    "             MODE_TESTZERO:'MODE_TESTZERO',MODE_TESTCURTRACK:'MODE_TESTCURTRACK',\n",
    "             MODE_PREDTRACK:'MODE_PREDTRACK',MODE_PREDPIT:'MODE_PREDPIT',\n",
    "            MODE_DISTURB_CLEARTRACK:'MODE_DISTURB_CLEARTRACK',MODE_DISTURB_ADJUSTTRACK:'MODE_DISTURB_ADJUSTTRACK',\n",
    "            MODE_DISTURB_ADJUSTPIT:'MODE_DISTURB_ADJUSTPIT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# configurataion\n",
    "#\n",
    "# model path:  <_dataset_id>/<_task_id>-<trainid>/\n",
    "#_dataset_id = 'indy2013-2018-nocarid'\n",
    "_dataset_id = 'indy2013-2018'\n",
    "_test_event = 'Indy500-2018'\n",
    "\n",
    "#_task_id = 'rank'  # rank,laptime, the trained model's task\n",
    "#_run_ts = COL_RANK   #COL_LAPTIME,COL_RANK\n",
    "#_exp_id='rank'  #rank, laptime, laptim2rank, timediff2rank... \n",
    "\n",
    "_task_id = 'laptime'  # rank,laptime, the trained model's task\n",
    "_run_ts = COL_LAPTIME   #COL_LAPTIME,COL_RANK\n",
    "_exp_id='laptime2rank'  #rank, laptime, laptim2rank, timediff2rank... \n",
    "\n",
    "#_task_id = 'laptime'  # rank,laptime, the trained model's task\n",
    "#_run_ts = COL_LAPTIME   #COL_LAPTIME,COL_RANK\n",
    "#_exp_id='laptime'  #rank, laptime, laptim2rank, timediff2rank... \n",
    "\n",
    "_task_id = 'timediff'  # rank,laptime, the trained model's task\n",
    "_run_ts = COL_TIMEDIFF   #COL_LAPTIME,COL_RANK\n",
    "_exp_id='timediff2rank'  #rank, laptime, laptim2rank, timediff2rank... \n",
    "\n",
    "_task_id = 'lapstatus'  # rank,laptime, the trained model's task\n",
    "_run_ts = COL_LAPSTATUS   #COL_LAPTIME,COL_RANK\n",
    "_exp_id='lapstatus'  #rank, laptime, laptim2rank, timediff2rank... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(event, year=0):\n",
    "    #inputfile = '../data/final/C_'+ event +'-' + year + '-final.csv'\n",
    "    if year>0:\n",
    "        inputfile = '../data/final/C_'+ event +'-' + year + '.csv'\n",
    "    else:\n",
    "        inputfile = '../data/final/C_'+ event +'.csv'\n",
    "    #outputprefix = year +'-' + event + '-'\n",
    "    dataset = pd.read_csv(inputfile)\n",
    "    #dataset.info(verbose=True)    \n",
    "    \n",
    "    final_lap = max(dataset.completed_laps)\n",
    "    total_laps = final_lap + 1\n",
    "\n",
    "    # get records for the cars that finish the race\n",
    "    completed_car_numbers= dataset[dataset.completed_laps == final_lap].car_number.values\n",
    "    completed_car_count = len(completed_car_numbers)\n",
    "\n",
    "    #print('count of completed cars:', completed_car_count)\n",
    "    #print('completed cars:', completed_car_numbers)\n",
    "\n",
    "    #make a copy\n",
    "    alldata = dataset.copy()\n",
    "    dataset = dataset[dataset['car_number'].isin(completed_car_numbers)]\n",
    "    rankdata = alldata.rename_axis('MyIdx').sort_values(by=['elapsed_time','MyIdx'], ascending=True)\n",
    "    rankdata = rankdata.drop_duplicates(subset=['car_number', 'completed_laps'], keep='first')\n",
    "    \n",
    "    cldata = make_cl_data(dataset)\n",
    "    acldata = make_cl_data(alldata)\n",
    "\n",
    "    return alldata, rankdata, acldata\n",
    "\n",
    "# make indy car completed_laps dataset\n",
    "# car_number, completed_laps, rank, elapsed_time, rank_diff, elapsed_time_diff \n",
    "def make_cl_data(dataset):\n",
    "\n",
    "    # pick up data with valid rank\n",
    "    rankdata = dataset.rename_axis('MyIdx').sort_values(by=['elapsed_time','MyIdx'], ascending=True)\n",
    "    rankdata = rankdata.drop_duplicates(subset=['car_number', 'completed_laps'], keep='first')\n",
    "\n",
    "    # resort by car_number, lap\n",
    "    uni_ds = rankdata.sort_values(by=['car_number', 'completed_laps', 'elapsed_time'], ascending=True)    \n",
    "    #uni_ds = uni_ds.drop([\"unique_id\", \"best_lap\", \"current_status\", \"track_status\", \"lap_status\",\n",
    "    #                  \"laps_behind_leade\",\"laps_behind_prec\",\"overall_rank\",\"pit_stop_count\",\n",
    "    #                  \"last_pitted_lap\",\"start_position\",\"laps_led\"], axis=1)\n",
    "    \n",
    "    uni_ds = uni_ds.drop([\"unique_id\", \"best_lap\", \n",
    "                      \"laps_behind_leade\",\"laps_behind_prec\",\"overall_rank\",\"pit_stop_count\",\n",
    "                      \"last_pitted_lap\",\"start_position\",\"laps_led\"], axis=1)\n",
    "        \n",
    "    carnumber = set(uni_ds['car_number'])\n",
    "    #print('cars:', carnumber)\n",
    "    #print('#cars=', len(carnumber))\n",
    "   \n",
    "    # faster solution , uni_ds already sorted by car_number and lap\n",
    "    uni_ds['rank_diff'] = uni_ds['rank'].diff()\n",
    "    mask = uni_ds.car_number != uni_ds.car_number.shift(1)\n",
    "    uni_ds['rank_diff'][mask] = 0\n",
    "    \n",
    "    uni_ds['time_diff'] = uni_ds['elapsed_time'].diff()\n",
    "    mask = uni_ds.car_number != uni_ds.car_number.shift(1)\n",
    "    uni_ds['time_diff'][mask] = 0\n",
    "    \n",
    "    #df = uni_ds[['car_number','completed_laps','rank','elapsed_time','rank_diff','time_diff']]\n",
    "    df = uni_ds[['car_number','completed_laps','rank','elapsed_time',\n",
    "                 'rank_diff','time_diff',\"current_status\", \"track_status\", \"lap_status\"]]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def test_flag(a, bitflag):\n",
    "    return (a & bitflag) ==  bitflag\n",
    "\n",
    "#\n",
    "# remove NaN at the tail\n",
    "# there should be no nans in the middle of the ts\n",
    "def get_modestr(a):\n",
    "    modestr = ''\n",
    "    for key in _mode_map:\n",
    "        if test_flag(a, key):\n",
    "            modestr += '%s,'%(_mode_map[key])\n",
    "            \n",
    "    return modestr\n",
    "\n",
    "# endpos -> vector of prediction_length\n",
    "_track_pred  = {}\n",
    "_track_true  = {}\n",
    "def init_track_model():\n",
    "    global _track_pred,_track_true\n",
    "    _track_pred = {}\n",
    "    _track_true  = {}\n",
    "    \n",
    "def get_track_model(track_rec, endpos, prediction_length, context_len=10):\n",
    "    \"\"\"\n",
    "    return the predicted track status\n",
    "    \"\"\"\n",
    "    global _track_pred,_track_true\n",
    "    # this is the perfect track model for Indy500 2018\n",
    "    track_model = [6,4,4,5,6,6,4]\n",
    "    if endpos in _track_pred:\n",
    "        return _track_pred[endpos]\n",
    "    else:\n",
    "        #get yflag lap count from the start pred point\n",
    "        yflaplen = 0\n",
    "        for i in range(1, context_len):\n",
    "            if track_rec[- prediction_length - i] == 1:\n",
    "                yflaplen += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        #laps remain, fill into the future\n",
    "        trackpred = np.array([0 for x in range(prediction_length)])\n",
    "        \n",
    "        yflap_pred = random.choice(track_model)\n",
    "        if yflaplen > 0 and yflap_pred > yflaplen:\n",
    "            trackpred[:(yflap_pred - yflaplen)] = 1\n",
    "        _track_pred[endpos] = trackpred\n",
    "        \n",
    "        _track_true[endpos]  = track_rec[- prediction_length:].copy()\n",
    "        \n",
    "        return trackpred\n",
    "\n",
    "    \n",
    "# endpos -> vector of prediction_length\n",
    "_track_adjust  = {}\n",
    "def init_adjust_track_model():\n",
    "    global _track_adjust\n",
    "    _track_adjust = {}\n",
    "    \n",
    "def adjust_track_model(track_rec, endpos, prediction_length, tailpos):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        tailpos ; <0 end pos of 1\n",
    "    return the predicted track status\n",
    "    \"\"\"\n",
    "    global _track_adjust\n",
    "    # this is the perfect track model for Indy500 2018\n",
    "    track_model = [-1,0,1]\n",
    "    if endpos in _track_adjust:\n",
    "        return _track_adjust[endpos]\n",
    "    else:\n",
    "        yflap_adjust = random.choice(track_model)\n",
    "        \n",
    "        #laps remain, fill into the future\n",
    "        trackadjust = track_rec[-prediction_length:].copy()\n",
    "        if yflap_adjust == -1:\n",
    "            trackadjust[tailpos] = 0\n",
    "        elif yflap_adjust == 1:\n",
    "            trackadjust[tailpos] = 0\n",
    "            if (tailpos + 1) <= -1:\n",
    "                trackadjust[tailpos+1] = 1\n",
    "        \n",
    "        _track_adjust[endpos] = trackadjust\n",
    "        \n",
    "        return trackadjust\n",
    "\n",
    "# carno -> lap_status\n",
    "_lap_adjust = {}    \n",
    "_empirical_model = {}\n",
    "def init_adjust_pitmodel():\n",
    "    global _lap_adjust\n",
    "    _lap_adjust = {}    \n",
    "    _empirical_model = {}\n",
    "\n",
    "def get_adjust_lapstatus(carno, lapstatus, force = True):\n",
    "    \"\"\"\n",
    "    init the lapstatus for each car, save it for future reference\n",
    "    \n",
    "    input:\n",
    "        carno;\n",
    "        lapstatus  ; the trueth\n",
    "    \n",
    "    \"\"\"\n",
    "    if carno not in _lap_adjust:\n",
    "        #adjust it\n",
    "        lapadjust = lapstatus.copy()\n",
    "        for pos in range(0, len(lapstatus)):\n",
    "            if lapadjust[pos] == 1:\n",
    "\n",
    "                success = False\n",
    "\n",
    "                while(not success):\n",
    "                    # adjust this pit lap position\n",
    "                    pos_adjust = get_random_choice(_adjust_model)\n",
    "\n",
    "                    new_pos = pos + pos_adjust\n",
    "\n",
    "                    if new_pos >= 0 and new_pos < len(lapstatus):\n",
    "                        #valid\n",
    "                        lapadjust[pos] = 0\n",
    "                        lapadjust[new_pos] = 1\n",
    "                        success = True\n",
    "                        \n",
    "                        #add statistics\n",
    "                        if pos_adjust not in _empirical_model:\n",
    "                            _empirical_model[pos_adjust] = 1\n",
    "                        else:\n",
    "                            _empirical_model[pos_adjust] += 1\n",
    "\n",
    "                    if force==False:\n",
    "                        break\n",
    "\n",
    "        _lap_adjust[carno] = lapadjust\n",
    "\n",
    "    return _lap_adjust[carno]\n",
    "        \n",
    "        \n",
    "def build_random_model(modeldict):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        modeldict ; {val: probability}\n",
    "    return:\n",
    "        model  ;  [val, cdf]\n",
    "    \"\"\"\n",
    "    # val, cdf\n",
    "    cdf = 0\n",
    "    model = np.zeros((len(modeldict), 2))\n",
    "    for idx, val in enumerate(sorted(modeldict.keys())):\n",
    "        model[idx, 0] = val\n",
    "        model[idx, 1] = cdf + modeldict[val]\n",
    "        cdf = model[idx, 1]\n",
    "        \n",
    "    #normalize\n",
    "    model[:, 1] = model[:, 1]/cdf\n",
    "    return model\n",
    "    \n",
    "def print_model(model, iscdf=True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model  ;  [val, cdf]\n",
    "    \"\"\"\n",
    "    sorted_model = model[np.argsort(model[:, 0])]\n",
    "    cdf = 0\n",
    "    \n",
    "    sumval = 1.\n",
    "    if not iscdf:\n",
    "        sumval = np.sum(sorted_model[:,1])\n",
    "    \n",
    "    ret = []\n",
    "    for row in sorted_model:\n",
    "        ret.append((row[0], (row[1]-cdf)/sumval))\n",
    "        if iscdf:\n",
    "            cdf = row[1]\n",
    "    #output\n",
    "    print(['%d:%.3f'%(x[0],x[1]) for x in ret])\n",
    "    \n",
    "    \n",
    "def get_random_choice(model):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        model  ;  [val, cdf]\n",
    "    return:\n",
    "        val according to its probability\n",
    "    \"\"\"\n",
    "    \n",
    "    target = np.random.rand()\n",
    "    idx = np.sum(model[:,1] < target)\n",
    "    return int(model[idx,0])\n",
    "    \n",
    "#_modeldict={-2:0.1,-1:0.2,0:0.4, 1:0.2, 2:0.1 }\n",
    "_modeldict={-2:0.1,-1:0.2,0:0.05, 1:0.2, 2:0.1 }\n",
    "_adjust_model = build_random_model(_modeldict)\n",
    "\n",
    "def adjust_pit_model(lap_rec, prediction_length, force=True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        tailpos ; <0 end pos of 1\n",
    "    return the predicted lap status\n",
    "    \"\"\"\n",
    "    #laps remain, fill into the future\n",
    "    lapadjust = lap_rec[-prediction_length:].copy()\n",
    "    for pos in range(0, prediction_length):\n",
    "        if lapadjust[pos] == 1:\n",
    "            \n",
    "            success = False\n",
    "            \n",
    "            while(not success):\n",
    "                # adjust this pit lap position\n",
    "                pos_adjust = get_random_choice(_adjust_model)\n",
    "\n",
    "                new_pos = pos + pos_adjust\n",
    "\n",
    "                if new_pos >= 0 and new_pos < prediction_length:\n",
    "                    #valid\n",
    "                    lapadjust[pos] = 0\n",
    "                    lapadjust[new_pos] = 1\n",
    "                    success = True\n",
    "                    \n",
    "                if force==False:\n",
    "                    break\n",
    "                    \n",
    "    return lapadjust\n",
    "\n",
    "def adjust_pit_model_fix(lap_rec, endpos, prediction_length):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        tailpos ; <0 end pos of 1\n",
    "    return the predicted lap status\n",
    "    \"\"\"\n",
    "    adjust_model = [-1,0,1]\n",
    "    lap_adjust = random.choice(adjust_model)\n",
    "        \n",
    "    #laps remain, fill into the future\n",
    "    lapadjust = lap_rec[-prediction_length:].copy()\n",
    "    for pos in range(0, prediction_length):\n",
    "        if lapadjust[pos] == 1:\n",
    "            # adjust this pit lap position\n",
    "            pos_adjust = random.choice(adjust_model)\n",
    "\n",
    "            if pos_adjust == -1:\n",
    "                if (pos - 1 >= 0):\n",
    "                    lapadjust[pos] = 0\n",
    "                    lapadjust[pos - 1] = 1\n",
    "            elif pos_adjust == 1:\n",
    "                if (pos + 1 < prediction_length):\n",
    "                    lapadjust[pos] = 0\n",
    "                    lapadjust[pos + 1] = 1\n",
    "\n",
    "    return lapadjust\n",
    "    \n",
    "# pit model is separate for each car\n",
    "def get_pit_model(cuation_laps_instint, laps_instint, prediction_length):\n",
    "    \"\"\"\n",
    "    return the predicted pit status\n",
    "    \"\"\"\n",
    "    # this is the perfect empirical pit model for Indy500 2018\n",
    "    pit_model_all = [[33, 32, 35, 32, 35, 34, 35, 34, 37, 32, 37, 30, 33, 36, 35, 33, 36, 30, 31, 33, 36, 37, 35, 34, 34, 33, 37, 35, 39, 32, 36, 35, 34, 32, 36, 32, 31, 36, 33, 33, 35, 37, 40, 32, 32, 34, 35, 36, 33, 37, 35, 37, 34, 35, 39, 32, 31, 37, 32, 35, 36, 39, 35, 36, 34, 35, 33, 33, 34, 32, 33, 34],\n",
    "                [45, 44, 46, 44, 43, 46, 45, 43, 41, 48, 46, 43, 47, 45, 49, 44, 48, 42, 44, 46, 45, 45, 43, 44, 44, 43, 46]]\n",
    "    pit_model_top8 = [[33, 32, 35, 33, 36, 33, 36, 33, 37, 35, 36, 33, 37, 34],\n",
    "                 [46, 45, 43, 48, 46, 45, 45, 43]]\n",
    "    \n",
    "    pit_model = pit_model_all\n",
    "    \n",
    "    if cuation_laps_instint>10:\n",
    "        #use low model\n",
    "        pred_pit_laps = random.choice(pit_model[0])\n",
    "    else:\n",
    "        pred_pit_laps = random.choice(pit_model[1])\n",
    "                \n",
    "    #laps remain, fill into the future\n",
    "    pitpred = np.array([0 for x in range(prediction_length)])\n",
    "    \n",
    "    if (pred_pit_laps > laps_instint) and (pred_pit_laps <= laps_instint + prediction_length):\n",
    "        pitpred[pred_pit_laps - laps_instint - 1] = 1\n",
    "         \n",
    "    return pitpred    \n",
    "    \n",
    "def make_dataset_byevent(runs, prediction_length, freq, \n",
    "                       useeid = False,\n",
    "                       run_ts= _run_ts, \n",
    "                       test_event = _test_event,\n",
    "                       test_cars = [],  \n",
    "                       use_global_dict = True,\n",
    "                       oracle_mode = MODE_ORACLE,\n",
    "                       half_moving_win = 0,\n",
    "                       train_ratio=0.8,\n",
    "                       log_transform = False,\n",
    "                       verbose = False\n",
    "                ):\n",
    "    \"\"\"\n",
    "    split the ts to train and test part by the ratio\n",
    "    \n",
    "    input:\n",
    "        oracle_mode: false to simulate prediction in real by \n",
    "                set the covariates of track and lap status as nan in the testset\n",
    "        half_moving_win  ; extend to 0:-1 ,1:-1/2plen, 2:-plen\n",
    "    \n",
    "    \"\"\"    \n",
    "    init_track_model()\n",
    "    init_adjust_track_model()\n",
    "    \n",
    "    start = pd.Timestamp(\"01-01-2019\", freq=freq)  # can be different for each time series\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    #select run\n",
    "    if runs>=0:\n",
    "        _laptime_data = [laptime_data[runs].copy()]\n",
    "    else:\n",
    "        _laptime_data = laptime_data.copy()\n",
    "    \n",
    "   \n",
    "    #add statistics for adjust test\n",
    "    # trackstatus, lapstatus\n",
    "    mae = [0,0]\n",
    "    \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    for _data in _laptime_data:\n",
    "        _train = []\n",
    "        _test = []\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        \n",
    "        else:\n",
    "            test_mode = False\n",
    "            \n",
    "            \n",
    "        #statistics on the ts length\n",
    "        ts_len = [ _entry.shape[1] for _entry in _data[2]]\n",
    "        max_len = int(np.max(ts_len))\n",
    "        train_len = int(np.max(ts_len) * train_ratio)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'====event:{events[_data[0]]}, train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)}')\n",
    "                \n",
    "        # process for each ts\n",
    "        for rowid in range(_data[2].shape[0]):\n",
    "            # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "            rec = _data[2][rowid].copy()\n",
    "            rec_raw = _data[2][rowid].copy()\n",
    "            \n",
    "            #remove nan(only tails)\n",
    "            nans, x= nan_helper(rec[run_ts,:])\n",
    "            nan_count = np.sum(nans)             \n",
    "            rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "            \n",
    "            # remove short ts\n",
    "            totallen = rec.shape[1]\n",
    "            if ( totallen < train_len + prediction_length):\n",
    "                if verbose:\n",
    "                    print(f'a short ts: carid={_data[1][rowid]}ï¼Œlen={totallen}')\n",
    "                continue                \n",
    "            \n",
    "            if use_global_dict:\n",
    "                carno = _data[1][rowid]\n",
    "                carid = global_carids[_data[1][rowid]]\n",
    "            else:\n",
    "                #simulation dataset, todo, fix the carids as decoder\n",
    "                carno = rowid\n",
    "                carid = rowid\n",
    "                \n",
    "            \n",
    "            if useeid:\n",
    "                static_cat = [carid, _data[0]]    \n",
    "            else:\n",
    "                static_cat = [carid]    \n",
    "                \n",
    "            #first, get target a copy    \n",
    "            # target can be COL_XXSTATUS\n",
    "            target_val = rec[run_ts,:].copy().astype(np.float32)\n",
    "            if log_transform:\n",
    "                target_val = np.log(target_val + 1.0)                \n",
    "            \n",
    "            # adjust for disturbance analysis\n",
    "            if test_mode and test_flag(oracle_mode, MODE_DISTURB_ADJUSTPIT):\n",
    "                lap_status = rec[COL_LAPSTATUS, :].copy()\n",
    "                rec[COL_LAPSTATUS, :] = get_adjust_lapstatus(carno, lap_status)\n",
    "                \n",
    "            # selection of features\n",
    "            if test_flag(oracle_mode, MODE_NOTRACK) or test_flag(oracle_mode, MODE_ORACLE_LAPONLY):                \n",
    "                rec[COL_TRACKSTATUS, :] = 0\n",
    "            if test_flag(oracle_mode, MODE_NOLAP) or test_flag(oracle_mode, MODE_ORACLE_TRACKONLY):                \n",
    "                rec[COL_LAPSTATUS, :] = 0\n",
    "\n",
    "            test_rec_cnt = 0\n",
    "            if not test_mode:\n",
    "                \n",
    "                # all go to train set\n",
    "                _train.append({'target': target_val, \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [rec[COL_TRACKSTATUS,:],\n",
    "                                       rec[COL_LAPSTATUS,:]]\n",
    "                              }\n",
    "                              )\n",
    "            else:\n",
    "                # reset train_len\n",
    "                #context_len = prediction_length*2\n",
    "                #if context_len < 10:\n",
    "                #    context_len = 10\n",
    "                \n",
    "                context_len = train_len\n",
    "                \n",
    "                # multiple test ts(rolling window as half of the prediction_length)\n",
    "\n",
    "                #step = -int(prediction_length/2) if half_moving_win else -prediction_length\n",
    "                if half_moving_win == 1:\n",
    "                    step = -int(prediction_length/2)\n",
    "                elif half_moving_win == 2:\n",
    "                    step = -prediction_length\n",
    "                else:\n",
    "                    step = -1\n",
    "                \n",
    "                #bug fix, fixed the split point for all cars/ts\n",
    "                for endpos in range(max_len, context_len+prediction_length, \n",
    "                                    step):\n",
    "\n",
    "                    #check if enough for this ts\n",
    "                    if endpos > totallen:\n",
    "                        continue\n",
    "                    \n",
    "                    track_rec = rec[COL_TRACKSTATUS, :endpos].copy()\n",
    "                    lap_rec = rec[COL_LAPSTATUS, :endpos].copy()\n",
    "                    \n",
    "                    caution_laps_instint = int(rec[COL_CAUTION_LAPS_INSTINT, endpos -prediction_length - 1])\n",
    "                    laps_instint = int(rec[COL_LAPS_INSTINT, endpos -prediction_length - 1])\n",
    "                    \n",
    "\n",
    "                    # test mode\n",
    "                    if test_flag(oracle_mode, MODE_TESTCURTRACK):\n",
    "                        # since nan does not work, use cur-val instead\n",
    "                        track_rec[-prediction_length:] = track_rec[-prediction_length - 1]\n",
    "                        #track_rec[-prediction_length:] = random.randint(0,1)\n",
    "                        #lap_rec[-prediction_length:] = lap_rec[-prediction_length - 1]\n",
    "                        lap_rec[-prediction_length:] = 0\n",
    "                    elif test_flag(oracle_mode, MODE_TESTZERO):\n",
    "                        #set prediction part as nan\n",
    "                        #track_rec[-prediction_length:] = np.nan\n",
    "                        #lap_rec[-prediction_length:] = np.nan\n",
    "                        track_rec[-prediction_length:] = 0\n",
    "                        lap_rec[-prediction_length:] = 0        \n",
    "\n",
    "                    # predicting with status model\n",
    "                    if test_flag(oracle_mode, MODE_PREDTRACK):\n",
    "                        predrec = get_track_model(track_rec, endpos, prediction_length)\n",
    "                        track_rec[-prediction_length:] = predrec\n",
    "                        #lap_rec[-prediction_length:] = 0\n",
    "                        \n",
    "                    if test_flag(oracle_mode, MODE_PREDPIT):\n",
    "                        #predrec = get_track_model(track_rec, endpos, prediction_length)\n",
    "                        #track_rec[-prediction_length:] = predrec\n",
    "                        lap_rec[-prediction_length:] = get_pit_model(caution_laps_instint,\n",
    "                                                                    laps_instint,prediction_length)\n",
    "                        \n",
    "                        \n",
    "                    # disturbe analysis\n",
    "                    if test_flag(oracle_mode, MODE_DISTURB_CLEARTRACK):\n",
    "                        # clear the oracle track status\n",
    "                        # future 1s in trackstatus\n",
    "                        # pattern like 0 1 xx\n",
    "                        for _pos in range(-prediction_length + 1, -1):\n",
    "                            if track_rec[_pos - 1] == 0:\n",
    "                                track_rec[_pos] = 0\n",
    "                                \n",
    "                    if test_flag(oracle_mode, MODE_DISTURB_ADJUSTTRACK):\n",
    "                        # adjust the end position of track, or caution lap length\n",
    "                        # find the end of caution laps\n",
    "                        _tail = 0\n",
    "                        for _pos in range(-1,-prediction_length + 1,-1):\n",
    "                            if track_rec[_pos] == 1:\n",
    "                                #find the tail\n",
    "                                _tail = _pos\n",
    "                                break\n",
    "                        if _tail != 0:\n",
    "                            #found\n",
    "                            adjustrec = adjust_track_model(track_rec, endpos, prediction_length, _tail)\n",
    "                            track_rec[-prediction_length:] = adjustrec\n",
    "                        \n",
    "                    #if test_flag(oracle_mode, MODE_DISTURB_ADJUSTPIT):\n",
    "                    #    # adjust the position of pit\n",
    "                    #    if np.sum(lap_rec[-prediction_length:]) > 0:\n",
    "                    #        adjustrec = adjust_pit_model(lap_rec, endpos, prediction_length)\n",
    "                    #        lap_rec[-prediction_length:] = adjustrec\n",
    "                        \n",
    "                    #okay, end of adjustments, test difference here\n",
    "                    # rec_raw .vs. track_rec, lap_rec\n",
    "                    track_rec_raw = rec_raw[COL_TRACKSTATUS, :endpos]\n",
    "                    lap_rec_raw = rec_raw[COL_LAPSTATUS, :endpos]\n",
    "                    \n",
    "                    mae[0] = mae[0] + np.nansum(np.abs(track_rec[-prediction_length:] - track_rec_raw[-prediction_length:]))\n",
    "                    mae[1] = mae[1] + np.nansum(np.abs(lap_rec[-prediction_length:] - lap_rec_raw[-prediction_length:]))\n",
    "\n",
    "                    _test.append({'target': target_val[:endpos], \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [track_rec,lap_rec]\n",
    "                                #'feat_dynamic_real': [rec[COL_TRACKSTATUS,:endpos],\n",
    "                                #       rec[COL_LAPSTATUS,:endpos]] \n",
    "                                 }\n",
    "                              )   \n",
    "                    test_rec_cnt += 1\n",
    "            \n",
    "            #add one ts\n",
    "            if verbose:\n",
    "                print(f'carno:{carno}, totallen:{totallen}, nancount:{nan_count}, test_reccnt:{test_rec_cnt}')\n",
    "\n",
    "        train_set.extend(_train)\n",
    "        test_set.extend(_test)\n",
    "\n",
    "    print(f'train len:{len(train_set)}, test len:{len(test_set)}, mae_track:{mae[0]},mae_lap:{mae[1]},')\n",
    "    \n",
    "    train_ds = ListDataset(train_set, freq=freq)\n",
    "    test_ds = ListDataset(test_set, freq=freq)    \n",
    "    \n",
    "    return train_ds, test_ds, train_set, test_set\n",
    "\n",
    "def save_dataset(datafile,freq, prediction_length, cardinality, train_ds, test_ds):\n",
    "    with open(datafile, 'wb') as f:\n",
    "        #pack [global_carids, laptime_data]\n",
    "        savedata = [freq, prediction_length, cardinality, train_ds, test_ds]\n",
    "        #savedata = [freq, train_set, test_set]\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test for Indy500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_ds,predictor):\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    print(f'tss len={len(tss)}, forecasts len={len(forecasts)}')\n",
    "    \n",
    "    return tss, forecasts\n",
    "\n",
    "   \n",
    "def run_prediction_ex(test_ds, prediction_length, model_name,trainid):\n",
    "    with mx.Context(mx.gpu(7)):    \n",
    "        pred_ret = []\n",
    "\n",
    "        rootdir = f'../models/remote/{_dataset_id}/{_task_id}-{trainid}/'\n",
    "        # deepAR-Oracle\n",
    "        if model_name == 'curtrack':\n",
    "            model=f'deepAR-Oracle-{_task_id}-curtrack-indy-f1min-t{prediction_length}-e1000-r1_curtrack_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "\n",
    "        elif model_name == 'zerotrack':\n",
    "            model=f'deepAR-Oracle-{_task_id}-nolap-zerotrack-indy-f1min-t{prediction_length}-e1000-r1_zerotrack_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "            \n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle-laponly':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle-laponly_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        # deepAR-Oracle\n",
    "        elif model_name == 'oracle-trackonly':\n",
    "            model=f'deepAR-Oracle-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_oracle-trackonly_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        # deepAR\n",
    "        elif model_name == 'deepAR':\n",
    "            model=f'deepAR-{_task_id}-all-indy-f1min-t{prediction_length}-e1000-r1_deepar_t{prediction_length}'\n",
    "            modeldir = rootdir + model\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  Predictor.deserialize(Path(modeldir))\n",
    "            print(f'loading model...done!, ctx:{predictor.ctx}')\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "\n",
    "        # naive\n",
    "        elif model_name == 'naive':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  NaivePredictor(freq= freq, prediction_length = prediction_length)\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        # zero, zero keeps the rank unchange\n",
    "        elif model_name == 'zero':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  ZeroPredictor(freq= freq, prediction_length = prediction_length)\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "\n",
    "        # arima\n",
    "        elif model_name == 'arima':\n",
    "            print(f'predicting model={model_name}, plen={prediction_length}')\n",
    "            predictor =  RForecastPredictor(method_name='arima',freq= freq, \n",
    "                                            prediction_length = prediction_length,trunc_length=60)\n",
    "            tss, forecasts = predict(test_ds,predictor)\n",
    "            pred_ret = [tss, forecasts]\n",
    "        else:\n",
    "            print(f'error: model {model_name} not support yet!')\n",
    "\n",
    "        return pred_ret     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc rank\n",
    "def eval_rank_bytimediff(test_ds,tss,forecasts,prediction_length):\n",
    "    \"\"\"\n",
    "    timediff models\n",
    "    \n",
    "    works for one event only\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    carlist = []\n",
    "\n",
    "    # carno-lap# -> elapsed_time[] array\n",
    "    forecasts_et = dict()\n",
    "\n",
    "    ds_iter =  iter(test_ds)\n",
    "    for idx in range(len(test_ds)):\n",
    "        test_rec = next(ds_iter)\n",
    "        #global carid\n",
    "        carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "        #print('car no:', carno)\n",
    "\n",
    "        if carno not in carlist:\n",
    "            carlist.append(carno)\n",
    "\n",
    "        # calc elapsed time\n",
    "        prediction_len = forecasts[idx].samples.shape[1]\n",
    "        if prediction_length != prediction_len:\n",
    "            print('error: prediction_len does not match, {prediction_length}:{prediction_len}')\n",
    "            return []\n",
    "        \n",
    "        #forecast_laptime_mean = np.mean(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "        forecast_laptime_mean = np.median(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "        \n",
    "        timediff_array = tss[idx].values.copy()\n",
    "\n",
    "        #save the prediction\n",
    "        completed_laps = len(tss[idx]) - prediction_len + 1\n",
    "        #print('car no:', carno, 'completed_laps:', completed_laps)\n",
    "        #key = '%s-%s'%(carno, completed_laps)\n",
    "        #forecasts_et[key] = elapsed_time[-prediction_len:].copy()\n",
    "        if completed_laps not in forecasts_et:\n",
    "            forecasts_et[completed_laps] = {}\n",
    "        forecasts_et[completed_laps][carno] = [timediff_array[-prediction_len:].copy(),\n",
    "                                                   forecast_laptime_mean.copy()]\n",
    "\n",
    "\n",
    "    # calc rank\n",
    "    rank_ret = []\n",
    "    for lap in forecasts_et.keys():\n",
    "        #get car list for this lap\n",
    "        carlist = list(forecasts_et[lap].keys())\n",
    "        #print('carlist:', carlist)\n",
    "\n",
    "        caridmap={key:idx for idx, key in enumerate(carlist)}\n",
    "\n",
    "        #fill in data\n",
    "        time_diff = np.zeros((2, len(carlist), prediction_len))\n",
    "        for carno in carlist:\n",
    "            carid = caridmap[carno]\n",
    "            time_diff[0, carid, :] = forecasts_et[lap][carno][0].reshape((prediction_len))\n",
    "            time_diff[1, carid, :] = forecasts_et[lap][carno][1].reshape((prediction_len))\n",
    "\n",
    "        #calculate rank    \n",
    "        idx = np.argsort(time_diff[0], axis=0)\n",
    "        true_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "        idx = np.argsort(time_diff[1], axis=0)\n",
    "        pred_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "        rank_ret.append([lap, time_diff, true_rank, pred_rank])\n",
    "        \n",
    "    return rank_ret,forecasts_et\n",
    "    \n",
    "#calc rank\n",
    "def eval_laptime(test_ds,tss,forecasts,prediction_length, start_offset):\n",
    "    \"\"\"\n",
    "    evaluate rank by laptime forecasting\n",
    "    \n",
    "    input:\n",
    "        test_ds       ; must be test set for a single event, because test_ds itself does not \n",
    "                        contain features to identify the eventid\n",
    "        start_offset[]; elapsed time for lap0, for one specific event\n",
    "        tss,forecasts ; forecast result\n",
    "        prediction_length ; \n",
    "    return:\n",
    "        rank_ret      ;  [lap, elapsed_time, true_rank, pred_rank] \n",
    "        forecasts_et  ;  {[completed_laps][carno]} ->(elapsed_time, elapsed_time_pred)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    carlist = []\n",
    "\n",
    "    # carno-lap# -> elapsed_time[] array\n",
    "    forecasts_et = dict()\n",
    "\n",
    "    ds_iter =  iter(test_ds)\n",
    "    for idx in range(len(test_ds)):\n",
    "        test_rec = next(ds_iter)\n",
    "        #global carid\n",
    "        carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "        #print('car no:', carno)\n",
    "\n",
    "        if carno not in carlist:\n",
    "            carlist.append(carno)\n",
    "\n",
    "        #start_offset is global var\n",
    "        #offset = start_offset[(start_offset['car_number']==carno)].elapsed_time.values[0] \n",
    "        #print('start_offset:', offset)\n",
    "\n",
    "        # calc elapsed time\n",
    "        prediction_len = forecasts[idx].samples.shape[1]\n",
    "        if prediction_length != prediction_len:\n",
    "            print('error: prediction_len does not match, {prediction_length}:{prediction_len}')\n",
    "            return []\n",
    "        \n",
    "        forecast_laptime_mean = np.mean(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "        #forecast_laptime_mean = np.median(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "\n",
    "        laptime_array = tss[idx].values.copy()\n",
    "        #elapsed_time = np.cumsum(laptime_array) + offset\n",
    "\n",
    "        laptime_array_hat = tss[idx].values.copy()\n",
    "        laptime_array_hat[-prediction_len:] = forecast_laptime_mean \n",
    "        #elapsed_time_hat = np.cumsum(laptime_array) + offset\n",
    "\n",
    "        #save the prediction\n",
    "        completed_laps = len(tss[idx]) - prediction_len + 1\n",
    "        #print('car no:', carno, 'completed_laps:', completed_laps)\n",
    "        #key = '%s-%s'%(carno, completed_laps)\n",
    "        #forecasts_et[key] = elapsed_time[-prediction_len:].copy()\n",
    "        if completed_laps not in forecasts_et:\n",
    "            forecasts_et[completed_laps] = {}\n",
    "        #forecasts_et[completed_laps][carno] = [elapsed_time[-prediction_len-1:].copy(),\n",
    "        #                                           elapsed_time_hat[-prediction_len-1:].copy()]\n",
    "        forecasts_et[completed_laps][carno] = [laptime_array[-prediction_len:].copy(),\n",
    "                                                   laptime_array_hat[-prediction_len:].copy()]\n",
    "\n",
    "\n",
    "    # calc rank\n",
    "    rank_ret = []\n",
    "    for lap in forecasts_et.keys():\n",
    "        #get car list for this lap\n",
    "        carlist = list(forecasts_et[lap].keys())\n",
    "        #print('carlist:', carlist)\n",
    "\n",
    "        caridmap={key:idx for idx, key in enumerate(carlist)}\n",
    "\n",
    "        #fill in data\n",
    "        #elapsed_time = np.zeros((2, len(carlist), prediction_len+1))\n",
    "        lap_time = np.zeros((2, len(carlist), prediction_len))\n",
    "        for carno in carlist:\n",
    "            carid = caridmap[carno]\n",
    "            lap_time[0, carid, :] = forecasts_et[lap][carno][0].reshape((prediction_len))\n",
    "            lap_time[1, carid, :] = forecasts_et[lap][carno][1].reshape((prediction_len))\n",
    "\n",
    "        #calculate rank    \n",
    "        #idx = np.argsort(elapsed_time[0], axis=0)\n",
    "        #true_rank = np.argsort(idx, axis=0)\n",
    "        true_laptime = lap_time[0]\n",
    "\n",
    "        #idx = np.argsort(elapsed_time[1], axis=0)\n",
    "        #pred_rank = np.argsort(idx, axis=0)\n",
    "        pred_laptime = lap_time[1]\n",
    "\n",
    "        rank_ret.append([lap, lap_time, true_laptime, pred_laptime])\n",
    "        \n",
    "    return rank_ret,forecasts_et    \n",
    "\n",
    "#calc rank\n",
    "def eval_rank(test_ds,tss,forecasts,prediction_length, start_offset):\n",
    "    \"\"\"\n",
    "    evaluate rank by laptime forecasting\n",
    "    \n",
    "    input:\n",
    "        test_ds       ; must be test set for a single event, because test_ds itself does not \n",
    "                        contain features to identify the eventid\n",
    "        start_offset[]; elapsed time for lap0, for one specific event\n",
    "        tss,forecasts ; forecast result\n",
    "        prediction_length ; \n",
    "    return:\n",
    "        rank_ret      ;  [lap, elapsed_time, true_rank, pred_rank] \n",
    "        forecasts_et  ;  {[completed_laps][carno]} ->(elapsed_time, elapsed_time_pred)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    carlist = []\n",
    "\n",
    "    # carno-lap# -> elapsed_time[] array\n",
    "    forecasts_et = dict()\n",
    "\n",
    "    ds_iter =  iter(test_ds)\n",
    "    for idx in range(len(test_ds)):\n",
    "        test_rec = next(ds_iter)\n",
    "        #global carid\n",
    "        carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "        #print('car no:', carno)\n",
    "\n",
    "        if carno not in carlist:\n",
    "            carlist.append(carno)\n",
    "\n",
    "        #start_offset is global var\n",
    "        if isinstance(start_offset, pd.core.frame.DataFrame):\n",
    "            offset = start_offset[(start_offset['car_number']==carno)].elapsed_time.values[0] \n",
    "        \n",
    "            \n",
    "        #print('start_offset:', offset)\n",
    "\n",
    "        # calc elapsed time\n",
    "        prediction_len = forecasts[idx].samples.shape[1]\n",
    "        if prediction_length != prediction_len:\n",
    "            print('error: prediction_len does not match, {prediction_length}:{prediction_len}')\n",
    "            return []\n",
    "        \n",
    "        forecast_laptime_mean = np.mean(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "        #forecast_laptime_mean = np.median(forecasts[idx].samples, axis=0).reshape((prediction_len,1))\n",
    "\n",
    "        if isinstance(start_offset, pd.core.frame.DataFrame):\n",
    "            #print('eval_rank:laptime2rank')\n",
    "            laptime_array = tss[idx].values.copy()\n",
    "            elapsed_time = np.cumsum(laptime_array) + offset\n",
    "\n",
    "            laptime_array = tss[idx].values.copy()\n",
    "            laptime_array[-prediction_len:] = forecast_laptime_mean \n",
    "            elapsed_time_hat = np.cumsum(laptime_array) + offset\n",
    "        else:\n",
    "            #print('eval_rank:rank-direct')\n",
    "            # rank directly\n",
    "            elapsed_time  = tss[idx].values.copy()\n",
    "\n",
    "            elapsed_time_hat = tss[idx].values.copy()\n",
    "            elapsed_time_hat[-prediction_len:] = forecast_laptime_mean             \n",
    "\n",
    "        #save the prediction\n",
    "        completed_laps = len(tss[idx]) - prediction_len + 1\n",
    "        #print('car no:', carno, 'completed_laps:', completed_laps)\n",
    "        #key = '%s-%s'%(carno, completed_laps)\n",
    "        #forecasts_et[key] = elapsed_time[-prediction_len:].copy()\n",
    "        if completed_laps not in forecasts_et:\n",
    "            forecasts_et[completed_laps] = {}\n",
    "        #forecasts_et[completed_laps][carno] = [elapsed_time[-prediction_len-1:].copy(),\n",
    "        #                                           elapsed_time_hat[-prediction_len-1:].copy()]\n",
    "        forecasts_et[completed_laps][carno] = [elapsed_time[-prediction_len:].copy(),\n",
    "                                                   elapsed_time_hat[-prediction_len:].copy()]\n",
    "\n",
    "\n",
    "    # calc rank\n",
    "    rank_ret = []\n",
    "    for lap in forecasts_et.keys():\n",
    "        #get car list for this lap\n",
    "        carlist = list(forecasts_et[lap].keys())\n",
    "        #print('carlist:', carlist)\n",
    "\n",
    "        caridmap={key:idx for idx, key in enumerate(carlist)}\n",
    "\n",
    "        #fill in data\n",
    "        #elapsed_time = np.zeros((2, len(carlist), prediction_len+1))\n",
    "        elapsed_time = np.zeros((2, len(carlist), prediction_len))\n",
    "        for carno in carlist:\n",
    "            carid = caridmap[carno]\n",
    "            elapsed_time[0, carid, :] = forecasts_et[lap][carno][0].reshape((prediction_len))\n",
    "            elapsed_time[1, carid, :] = forecasts_et[lap][carno][1].reshape((prediction_len))\n",
    "\n",
    "        #calculate rank    \n",
    "        idx = np.argsort(elapsed_time[0], axis=0)\n",
    "        true_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "        idx = np.argsort(elapsed_time[1], axis=0)\n",
    "        pred_rank = np.argsort(idx, axis=0)\n",
    "\n",
    "        rank_ret.append([lap, elapsed_time, true_rank, pred_rank])\n",
    "        \n",
    "    return rank_ret,forecasts_et    \n",
    "    \n",
    "def get_acc(rank_ret,prediction_length, verbose = False):   \n",
    "    \"\"\"\n",
    "    input:\n",
    "        rank_ret: [lap, elapsed_time, true_rank, pred_rank], use [2][3] columns\n",
    "    return:\n",
    "        ((metrics...)\n",
    "         (record count...))\n",
    "         \n",
    "    the result can be used to calculate micro/macro metrics\n",
    "    \"\"\"\n",
    "    # evaluate\n",
    "    #top1 accuracy\n",
    "    top1acc = 0\n",
    "    top1acc_farmost = 0\n",
    "    top5acc = 0\n",
    "    top5acc_farmost = 0\n",
    "    tau = 0\n",
    "    rmse = 0.\n",
    "    mae = 0.\n",
    "    \n",
    "    for rec in rank_ret:\n",
    "        trueRank = rec[2]\n",
    "        predRank = rec[3]\n",
    "\n",
    "        #top1 , rank = 0, first col is not prediction\n",
    "        top1acc += np.sum((trueRank==0) & (predRank==0)) \n",
    "        \n",
    "        top1acc_farmost += np.sum((trueRank[:,-1]==0) & (predRank[:,-1]==0))\n",
    "        \n",
    "        #top5\n",
    "        top5acc += np.sum((trueRank<5) & (predRank<5)) \n",
    "        \n",
    "        top5acc_farmost += np.sum((trueRank[:,-1]<5) & (predRank[:,-1]<5))\n",
    "        \n",
    "        # tau\n",
    "        tao, _ = stats.kendalltau(trueRank, predRank)\n",
    "        tau += tao\n",
    "        \n",
    "        #rmse\n",
    "        rmse += mean_squared_error(predRank,trueRank)\n",
    "        \n",
    "        #mae\n",
    "        mae += np.sum(np.abs(predRank - trueRank))\n",
    "        \n",
    "    recnt = len(rank_ret)\n",
    "    if recnt > 0:\n",
    "        top1acc = top1acc *1.0/ (recnt*prediction_length)\n",
    "        top1acc_farmost = top1acc_farmost *1.0/ recnt\n",
    "        top5acc = top5acc *1.0/ (5*recnt*prediction_length)\n",
    "        top5acc_farmost = top5acc_farmost *1.0/ (5*recnt)\n",
    "        tau = tau/recnt\n",
    "        rmse = rmse/recnt\n",
    "        \n",
    "        mae = mae/recnt\n",
    "\n",
    "        #debug only\n",
    "        if _run_ts == COL_LAPSTATUS:\n",
    "            tau = mae\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'total:{len(rank_ret)}, prediction_length:{prediction_length}') \n",
    "            print('top1acc=', top1acc,\n",
    "                  'top1acc_farmost=', top1acc_farmost,\n",
    "                  'top5acc=', top5acc,\n",
    "                  'top5acc_farmost=', top5acc_farmost,\n",
    "                 )\n",
    "            print('tau = ', tau,\n",
    "                 'rmse = ', rmse,\n",
    "                 'mae = ', mae)\n",
    "    \n",
    "    return ((top1acc,top1acc_farmost,top5acc,top5acc_farmost,tau,rmse),\n",
    "            (recnt*prediction_length,recnt,5*recnt*prediction_length,5*recnt,recnt,recnt))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(prediction_length, half_moving_win, train_ratio=0.8, trainid=\"r0.8\",\n",
    "                   test_event=_test_event, test_cars = [], \n",
    "                   datamode = MODE_ORACLE,models = ['oracle']):\n",
    "    \"\"\"\n",
    "    dependency: test_event, test on one event only\n",
    "    \n",
    "    \"\"\"\n",
    "    retdf = []\n",
    "    pred_ret = {}\n",
    "    ds_ret = {}\n",
    "    rank_result = {}\n",
    "    \n",
    "    ### create test dataset\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(events_id[test_event], prediction_length,freq, \n",
    "                                         oracle_mode=datamode,\n",
    "                                         run_ts = _run_ts,\n",
    "                                         test_cars=test_cars,\n",
    "                                         half_moving_win= half_moving_win,\n",
    "                                         train_ratio=train_ratio)\n",
    "    \n",
    "    for model in models:\n",
    "        print('exp:',inspect.stack()[0][3],'model:', model, \n",
    "              'datamode:', get_modestr(datamode),'eval:', _exp_id )\n",
    "        tss, forecasts = run_prediction_ex(test_ds, prediction_length, model,\n",
    "                                           trainid=trainid)\n",
    "        pred_ret[model] = [tss, forecasts]\n",
    "        ds_ret[model] = test_ds\n",
    "\n",
    "        \n",
    "        if _exp_id=='rank' or _exp_id=='timediff2rank': \n",
    "            #rank prediction\n",
    "            rank_ret, forecast_ret = eval_rank(test_ds,tss,forecasts,prediction_length,\n",
    "                                               0)\n",
    "        elif _exp_id=='laptime2rank':\n",
    "            rank_ret, forecast_ret = eval_rank(test_ds,tss,forecasts,prediction_length,\n",
    "                                               global_start_offset[test_event])\n",
    "        elif _exp_id in ['laptime','timediff','lapstatus']:\n",
    "            #laptime instead\n",
    "            rank_ret, forecast_ret = eval_laptime(test_ds,tss,forecasts,prediction_length,\n",
    "                                                  global_start_offset[test_event])\n",
    "        else:\n",
    "            print(f'Error, {_exp_id} evaluation not support yet')\n",
    "            break\n",
    "        \n",
    "        metrics = get_acc(rank_ret,prediction_length)\n",
    "        ret = [model, prediction_length, half_moving_win,get_modestr(datamode),trainid]\n",
    "        ret.extend(metrics[0])\n",
    "        retdf.append(ret)\n",
    "        \n",
    "        rank_result[model] = (rank_ret,forecast_ret)\n",
    "    \n",
    "    return pred_ret, ds_ret, rank_result, retdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(runs, plens, half, trainids, train_ratio, testfunc, datamode='', models=[]):\n",
    "    \"\"\"\n",
    "    \n",
    "    input:\n",
    "        plens=[2,5,10]\n",
    "        half=[False]\n",
    "        #trainids = [\"indy500-r0.2\",\"indy500-r0.4\",\"indy500\"]\n",
    "        trainids = [\"r0.5\"]\n",
    "        #half=[True,False]\n",
    "        #plens=[2]\n",
    "        runs = 5\n",
    "        train_ratio=0.5 \n",
    "        exp_id='mean-splitbystage-predpit'\n",
    "        \n",
    "        testfunc ; run_exp_predpit, run_exp_predtrack, run_exp ...\n",
    "\n",
    "    return:\n",
    "    \n",
    "        dfret  ; average result of multiple runs\n",
    "                 dataframe['model' , 'prediction_length', 'halfmode','datamode','trainid',\n",
    "                         'top1acc','top1acc_farmost','top5acc','top5acc_farmost','tau','rmse',\n",
    "                         'top1acc_std','top1acc_farmost_std','top5acc_std','top5acc_farmost_std','tau_std','rmse_std']\n",
    "                          \n",
    "        alldata_ret  ; for debug\n",
    "            [runid][halfmode,plen,trainid] -> (pred_ret, test_ds, rank_ret)\n",
    "                pred_ret[model] ->ã€€[tss, forecasts]\n",
    "                test_ds[model] ->ã€€test_ds\n",
    "                rank_ret[model] -> ([lap, elapsed_time, true_rank, pred_rank],forecast_ret)\n",
    "                    forecast_ret[completed_laps][carno] -> (elapsed_time, elapsed_time_hat)\n",
    "    \n",
    "    \"\"\"\n",
    "    if plens == [] or half == [] or trainids == []:\n",
    "        print(\"error with empty settings\")\n",
    "        return\n",
    "    \n",
    "    #testfunc or (datamode & models)\n",
    "    if isinstance(testfunc,str) and (datamode == '' or models == []):\n",
    "        print(\"error with testfunc\")\n",
    "        return\n",
    "\n",
    "    allret = []\n",
    "    alldata_ret = []\n",
    "    for runid in range(runs):\n",
    "        exp_data = []\n",
    "        exp_result = []\n",
    "\n",
    "        for halfmode in half:\n",
    "            for plen in plens:\n",
    "                for trainid in trainids:\n",
    "                    print('='*10)\n",
    "                    if not isinstance(testfunc,str):\n",
    "                        pred_ret, test_ds, rank_ret, metric_ret = testfunc(plen, halfmode, \n",
    "                                                            train_ratio=train_ratio,\n",
    "                                                            trainid=trainid)\n",
    "                    else:\n",
    "                        pred_ret, test_ds, rank_ret, metric_ret = run_exp(plen, halfmode, \n",
    "                                                            train_ratio=train_ratio,\n",
    "                                                            trainid=trainid, \n",
    "                                                            datamode=datamode,\n",
    "                                                            models=models)\n",
    "                        \n",
    "\n",
    "                    #save \n",
    "                    exp_data.append((pred_ret, test_ds, rank_ret))\n",
    "                    exp_result.extend(metric_ret)\n",
    "\n",
    "        #save result\n",
    "        result = pd.DataFrame(exp_result, columns = ['model' , 'prediction_length', 'halfmode',\n",
    "                                           'datamode','trainid',\n",
    "                                           'top1acc','top1acc_farmost','top5acc',\n",
    "                                           'top5acc_farmost','tau','rmse'])\n",
    "\n",
    "        #result['runid'] = [runid for x in range(len(result))]\n",
    "        allret.append(result)\n",
    "        alldata_ret.append(exp_data)\n",
    "\n",
    "    #final\n",
    "    rowcnt = len(allret[0])\n",
    "    metrics = np.empty((runs, rowcnt, 6))\n",
    "    for runid, ret in enumerate(allret):\n",
    "        metrics[runid, :,:] = ret[['top1acc','top1acc_farmost','top5acc',\n",
    "                                           'top5acc_farmost','tau','rmse']].values\n",
    "\n",
    "\n",
    "    #average\n",
    "    averagemat = np.mean(metrics[:,:,:], axis=0)\n",
    "    stdmat = np.std(metrics[:,:,:], axis=0)\n",
    "    dfhead = allret[0][['model' , 'prediction_length', 'halfmode', 'datamode','trainid']]\n",
    "    \n",
    "    \n",
    "    dfaverage = pd.DataFrame(averagemat, columns = ['top1acc','top1acc_farmost','top5acc',\n",
    "                                       'top5acc_farmost','tau','rmse'])\n",
    "    dfstd = pd.DataFrame(stdmat, columns = ['top1acc_std','top1acc_farmost_std','top5acc_std',\n",
    "                                       'top5acc_farmost_std','tau_std','rmse_std'])\n",
    "    dfret = pd.concat([dfhead, dfaverage, dfstd], axis=1)\n",
    "\n",
    "    #if exp_id != '':\n",
    "    #    dfret.to_csv(f'laptime2rank-evaluate-indy500-{exp_id}-result.csv', float_format='%.3f')\n",
    "\n",
    "    return dfret, alldata_ret\n",
    "\n",
    "\n",
    "def checkret_status(dataret, runid = 0, idx = 0,model='oracle'):\n",
    "    \"\"\"\n",
    "    check the test_ds track and lap status\n",
    "    \n",
    "        alldata_ret  ; for debug\n",
    "            [runid][halfmode,plen,trainid] -> (pred_ret, test_ds, rank_ret)\n",
    "                pred_ret[model] ->ã€€[tss, forecasts]\n",
    "                test_ds[model] ->ã€€test_ds\n",
    "                rank_ret[model] -> ([lap, elapsed_time, true_rank, pred_rank],forecast_ret)\n",
    "                    forecast_ret[completed_laps][carno] -> (elapsed_time, elapsed_time_hat)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _, plen = dataret[runid][idx][0][model][1][0].samples.shape\n",
    "    test_ds = dataret[runid][idx][1][model]\n",
    "   \n",
    "    \n",
    "    ds_iter =  iter(test_ds)\n",
    "    yfcnt = 0\n",
    "    pitcnt = 0\n",
    "    for recid in range(len(test_ds)):\n",
    "        test_rec = next(ds_iter)\n",
    "        \n",
    "        carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "        \n",
    "        track_rec,lap_rec = test_rec['feat_dynamic_real']\n",
    "        yfcnt += np.sum(track_rec[-plen:])\n",
    "        pitcnt += np.sum(lap_rec[-plen:])\n",
    "        \n",
    "    print('yfcnt:', yfcnt, 'pitcnt:',pitcnt)\n",
    "\n",
    "\n",
    "def get_ref_oracle_testds(plens, halfs, train_ratio=0.8, \n",
    "                   test_event=_test_event, test_cars = []):           \n",
    "    \n",
    "    testset = {}\n",
    "    for prediction_length in plens:\n",
    "        for half_moving_win in halfs:\n",
    "            train_ds, test_ds,_,_ = make_dataset_byevent(events_id[test_event], prediction_length,freq, \n",
    "                                         oracle_mode=MODE_ORACLE,\n",
    "                                         run_ts = _run_ts,\n",
    "                                         test_cars=test_cars,\n",
    "                                         half_moving_win= half_moving_win,\n",
    "                                         train_ratio=train_ratio)    \n",
    "            \n",
    "            # get key\n",
    "            key = '%d-%d'%(prediction_length,half_moving_win)\n",
    "            testset[key] = test_ds\n",
    "            \n",
    "    return testset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def checkret_confusionmat(dataret, ref_testset, runid= 0, testid = '', model='oracle'):\n",
    "    \"\"\"\n",
    "    output the 4x4 confusion matrix split by track and lap status\n",
    "    \n",
    "    input:\n",
    "        ref_oracle_testds  ; oracle test ds\n",
    "    \n",
    "    \"\"\"\n",
    "    plen_length = len(dataret[runid])\n",
    "    \n",
    "    dflist = []\n",
    "    for idx in range(plen_length):\n",
    "        _, plen = dataret[runid][idx][0][model][1][0].samples.shape\n",
    "        test_ds = dataret[runid][idx][1][model]\n",
    "        rank_ret = dataret[runid][idx][2][model][0]\n",
    "\n",
    "        key = '%d-%d'%(plen,0)\n",
    "        if key not in ref_testset:\n",
    "            print(f'error, {key} not found in ref_testset')\n",
    "            continue\n",
    "        \n",
    "        ref_oracle_testds = ref_testset[key]\n",
    "        if len(ref_oracle_testds) != len(test_ds):\n",
    "            print('error, size of testds mismatch', len(ref_oracle_testds), len(test_ds))\n",
    "            continue\n",
    "\n",
    "        # confusion matrix for <trackstatus, lapstatus> type: 00,01,10,11\n",
    "        # lap(start lap of prediction)  -> type\n",
    "        lapmap = {}\n",
    "        ds_iter =  iter(ref_oracle_testds)\n",
    "        for recid in range(len(ref_oracle_testds)):\n",
    "            test_rec = next(ds_iter) \n",
    "\n",
    "            carno = decode_carids[test_rec['feat_static_cat'][0]]\n",
    "\n",
    "            track_rec,lap_rec = test_rec['feat_dynamic_real']\n",
    "            yfcnt = np.sum(track_rec[-plen:])\n",
    "            pitcnt = np.sum(lap_rec[-plen:])    \n",
    "\n",
    "            #laptype = ('0' if yfcnt==0 else '1') + ('0' if pitcnt==0 else '1')\n",
    "\n",
    "            lap = len(track_rec) - plen + 1\n",
    "            if lap not in lapmap:\n",
    "                #lapmap[lap] = laptype\n",
    "                lapmap[lap] = (yfcnt, pitcnt)\n",
    "            else:\n",
    "                oldtype = lapmap[lap]\n",
    "                lapmap[lap] = (yfcnt + oldtype[0], pitcnt + oldtype[1])\n",
    "\n",
    "\n",
    "        #split the rank_ret by laptype\n",
    "        types=['00','10','01','11']\n",
    "        acc_ret = []\n",
    "        for laptype in types:\n",
    "            check_ret = []\n",
    "            for item in rank_ret:\n",
    "                typecnt = lapmap[item[0]]\n",
    "\n",
    "                thetype = ('0' if typecnt[0]==0 else '1') + ('0' if typecnt[1]==0 else '1')\n",
    "\n",
    "                if thetype == laptype:\n",
    "                    check_ret.append(item)\n",
    "            # get acc\n",
    "            metrics = get_acc(check_ret,plen)\n",
    "            recret = [testid, plen, laptype, len(check_ret)]\n",
    "            recret.extend(metrics[0])\n",
    "            acc_ret.append(recret)\n",
    "\n",
    "        #add all test\n",
    "        metrics = get_acc(rank_ret,plen)\n",
    "        recret = [testid, plen, 'aa', len(rank_ret)]\n",
    "        recret.extend(metrics[0])\n",
    "        acc_ret.append(recret)\n",
    "        \n",
    "        _dfacc = pd.DataFrame(acc_ret, columns = ['testid','plen',\n",
    "                                'type','reccnt','top1acc','top1acc_farmost','top5acc',\n",
    "                                'top5acc_farmost','tau','rmse'])\n",
    "        \n",
    "        dflist.append(_dfacc)\n",
    "    \n",
    "    dfacc = pd.concat(dflist, axis=0)\n",
    "    \n",
    "    return dfacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_ssd/hpda/anaconda3/envs/gluonts/lib/python3.6/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/scratch_ssd/hpda/anaconda3/envs/gluonts/lib/python3.6/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# parameters\n",
    "#\n",
    "stagedata = {}\n",
    "global_carids = {}\n",
    "traindata = None\n",
    "cur_carid = 0\n",
    "#years = ['2011','2012','2013', '2014', '2015', '2016', '2017']\n",
    "\n",
    "years = ['2013','2014','2015','2016','2017','2018']\n",
    "#events = ['Indy500']\n",
    "events = [f'Indy500-{x}' for x in years]\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "dbid = f'Indy500_{years[0]}_{years[-1]}'\n",
    "\n",
    "global_start_offset = {}\n",
    "\n",
    "for event in events:\n",
    "    #dataid = f'{event}-{year}'\n",
    "    #alldata, rankdata, acldata, flagdata\n",
    "    stagedata[event] = load_data(event)\n",
    "\n",
    "    alldata, rankdata, acldata = stagedata[event]\n",
    "    \n",
    "    #offset\n",
    "    global_start_offset[event] = rankdata[rankdata['completed_laps']==0][['car_number','elapsed_time']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from here\n",
    "import pickle\n",
    "#with open('laptime_rank_timediff_fulltest-oracle-%s.pickle'%year, 'rb') as f:\n",
    "with open(f'laptime_rank_timediff_pit-oracle-{dbid}.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    global_carids, laptime_data = pickle.load(f, encoding='latin1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"1min\"\n",
    "#decode global_carids\n",
    "decode_carids={carid:carno for carno, carid in global_carids.items()}\n",
    "    \n",
    "#useeid = False\n",
    "#interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "#ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "#if useeid:\n",
    "#    cardinality = [len(global_carids), len(laptime_data)]\n",
    "#else:\n",
    "#    cardinality = [len(global_carids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oracle test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_testds(datamode, test_event=_test_event, test_cars=[]):\n",
    "    \"\"\"\n",
    "    report mae, etc\n",
    "    \"\"\"\n",
    "    for prediction_length in plens:\n",
    "        for half_moving_win in half:\n",
    "            train_ds, test_ds,_,_ = make_dataset_byevent(events_id[test_event], prediction_length,freq, \n",
    "                                         oracle_mode=datamode,\n",
    "                                         run_ts = _run_ts,\n",
    "                                         test_cars=test_cars,\n",
    "                                         half_moving_win= half_moving_win,\n",
    "                                         train_ratio=train_ratio)\n",
    "            \n",
    "def dotest(config):\n",
    "    acclist = []\n",
    "    dflist = []\n",
    "    for model in config.keys():\n",
    "        conf = config[model]\n",
    "        for teststr in conf.keys():\n",
    "            testfunc = teststr\n",
    "            datamode = conf[teststr]\n",
    "\n",
    "\n",
    "            df, dataret = run_test(runs, plens, half, trainids, \n",
    "                                   train_ratio, testfunc, datamode=datamode,models=[model])\n",
    "\n",
    "            #concat\n",
    "            acc = checkret_confusionmat(dataret, ref_testset, \n",
    "                                        testid = teststr, model=model)\n",
    "            dflist.append(df)\n",
    "            acclist.append(acc)\n",
    "\n",
    "    dfret = pd.concat(dflist, axis=0)\n",
    "    dfacc = pd.concat(acclist, axis=0)\n",
    "    return dfret, dfacc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len:0, test len:3142, mae_track:0.0,mae_lap:0.0,\n",
      "train len:0, test len:3055, mae_track:0.0,mae_lap:0.0,\n",
      "train len:0, test len:2910, mae_track:0.0,mae_lap:0.0,\n",
      "==========\n",
      "train len:0, test len:3142, mae_track:0.0,mae_lap:0.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3142, forecasts len=3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train len:0, test len:3055, mae_track:0.0,mae_lap:0.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=5\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3055, forecasts len=3055\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len:0, test len:2910, mae_track:0.0,mae_lap:0.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=10\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=2910, forecasts len=2910\n",
      "==========\n",
      "train len:0, test len:3142, mae_track:1306.0,mae_lap:222.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE_TRACKONLY,MODE_ORACLE_LAPONLY, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3142, forecasts len=3142\n",
      "==========\n",
      "train len:0, test len:3055, mae_track:3256.0,mae_lap:547.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE_TRACKONLY,MODE_ORACLE_LAPONLY, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=5\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3055, forecasts len=3055\n",
      "==========\n",
      "train len:0, test len:2910, mae_track:6439.0,mae_lap:1055.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE_TRACKONLY,MODE_ORACLE_LAPONLY, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=10\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=2910, forecasts len=2910\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len:0, test len:3142, mae_track:0.0,mae_lap:222.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE_TRACKONLY, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3142, forecasts len=3142\n",
      "==========\n",
      "train len:0, test len:3055, mae_track:0.0,mae_lap:547.0,\n",
      "exp: run_exp model: oracle-trackonly"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " datamode: MODE_ORACLE_TRACKONLY, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=5\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3055, forecasts len=3055\n",
      "==========\n",
      "train len:0, test len:2910, mae_track:0.0,mae_lap:1055.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE_TRACKONLY, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=10\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=2910, forecasts len=2910\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len:0, test len:3142, mae_track:773.0,mae_lap:222.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE_TRACKONLY,MODE_PREDTRACK, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3142, forecasts len=3142\n",
      "==========\n",
      "train len:0, test len:3055, mae_track:2331.0,mae_lap:547.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE_TRACKONLY,MODE_PREDTRACK, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=5\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3055, forecasts len=3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train len:0, test len:2910, mae_track:5525.0,mae_lap:1055.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_ORACLE_TRACKONLY,MODE_PREDTRACK, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=10\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=2910, forecasts len=2910\n",
      "==========\n",
      "train len:0, test len:3142, mae_track:508.0,mae_lap:222.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle-trackonly datamode: MODE_TESTCURTRACK, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3142, forecasts len=3142\n",
      "==========\n",
      "train len:0, test len:3055, mae_track:2008.0,mae_lap:547.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: run_exp model: oracle-trackonly datamode: MODE_TESTCURTRACK, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=5\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3055, forecasts len=3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train len:0, test len:2910, mae_track:4887.0,mae_lap:1055.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_TESTCURTRACK, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=10\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=2910, forecasts len=2910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train len:0, test len:3142, mae_track:1306.0,mae_lap:222.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_TESTZERO, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3142, forecasts len=3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train len:0, test len:3055, mae_track:3256.0,mae_lap:547.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_TESTZERO, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=5\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3055, forecasts len=3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train len:0, test len:2910, mae_track:6439.0,mae_lap:1055.0,\n",
      "exp: run_exp model: oracle-trackonly datamode: MODE_TESTZERO, eval: lapstatus\n",
      "predicting model=oracle-trackonly, plen=10\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=2910, forecasts len=2910\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len:0, test len:3142, mae_track:0.0,mae_lap:0.0,\n",
      "exp: run_exp model: deepAR datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=deepAR, plen=2\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3142, forecasts len=3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train len:0, test len:3055, mae_track:0.0,mae_lap:0.0,\n",
      "exp: run_exp model: deepAR datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=deepAR, plen=5\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=3055, forecasts len=3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "train len:0, test len:2910, mae_track:0.0,mae_lap:0.0,\n",
      "exp: run_exp model: deepAR datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=deepAR, plen=10\n",
      "loading model...done!, ctx:gpu(0)\n",
      "tss len=2910, forecasts len=2910\n",
      "==========\n",
      "train len:0, test len:3142, mae_track:0.0,mae_lap:0.0,\n",
      "exp: run_exp model: zero datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=zero, plen=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/ZeroPredictor.py:62: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  start_date=start + target_len,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tss len=3142, forecasts len=3142\n",
      "==========\n",
      "train len:0, test len:3055, mae_track:0.0,mae_lap:0.0,\n",
      "exp: run_exp model: zero datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=zero, plen=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/ZeroPredictor.py:62: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  start_date=start + target_len,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tss len=3055, forecasts len=3055\n",
      "==========\n",
      "train len:0, test len:2910, mae_track:0.0,mae_lap:0.0,\n",
      "exp: run_exp model: zero datamode: MODE_ORACLE, eval: lapstatus\n",
      "predicting model=zero, plen=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/ZeroPredictor.py:62: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  start_date=start + target_len,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tss len=2910, forecasts len=2910\n"
     ]
    }
   ],
   "source": [
    "### test status model\n",
    " \n",
    "\n",
    "#half=[True, False]\n",
    "#plens=[2,5,10,20,30]\n",
    "plens=[2,5,10]\n",
    "half=[0]\n",
    "trainids = [\"2018\"]\n",
    "#trainids = [\"r0.5\",\"r0.6\"]\n",
    "runs = 1\n",
    "train_ratio=0.4\n",
    "\n",
    "acc_output = f'{_exp_id}-evaluate-mean-splitbyevent-fulltest-contigency-d{_dataset_id}-t{_test_event}-r{runs}-tr{train_ratio}-result.csv'\n",
    "ret_output = f'{_exp_id}-evaluate-mean-splitbyevent-fulltest-all-d{_dataset_id}-t{_test_event}-r{runs}-tr{train_ratio}-result.csv'\n",
    "\n",
    "#trainids = [\"indy500\"]\n",
    "#runs = 1\n",
    "#plens=[2]\n",
    "\n",
    "\n",
    "config = {'oracle-trackonly':\n",
    "            {'fulloracle':MODE_ORACLE,'notracklap':MODE_NOTRACK + MODE_NOLAP,\n",
    "             'trackonly':MODE_ORACLE_TRACKONLY,\n",
    "              'predtrack':MODE_PREDTRACK + MODE_ORACLE_TRACKONLY,\n",
    "              'curtrack':MODE_TESTCURTRACK,\n",
    "              'zerotrack':MODE_TESTZERO\n",
    "             },\n",
    "          'deepAR':{'deepAR':MODE_ORACLE},\n",
    "          'zero':{'zero':MODE_ORACLE}\n",
    "         }\n",
    "\n",
    "ref_testset = get_ref_oracle_testds(plens, half, train_ratio=train_ratio)\n",
    "\n",
    "dfret, dfacc = dotest(config)\n",
    "\n",
    "dfret.to_csv(ret_output, float_format='%.3f')\n",
    "dfacc.to_csv(acc_output, float_format='%.3f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>testid</th>\n",
       "      <th>plen</th>\n",
       "      <th>type</th>\n",
       "      <th>reccnt</th>\n",
       "      <th>top1acc</th>\n",
       "      <th>top1acc_farmost</th>\n",
       "      <th>top5acc</th>\n",
       "      <th>top5acc_farmost</th>\n",
       "      <th>tau</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fulloracle</td>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>1.886972</td>\n",
       "      <td>0.035195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fulloracle</td>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>4.774371</td>\n",
       "      <td>0.035731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fulloracle</td>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>9.599202</td>\n",
       "      <td>0.036068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>notracklap</td>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>1.883813</td>\n",
       "      <td>0.035188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>notracklap</td>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>4.774627</td>\n",
       "      <td>0.035731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>notracklap</td>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>9.599488</td>\n",
       "      <td>0.036068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trackonly</td>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>1.887286</td>\n",
       "      <td>0.035192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trackonly</td>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>4.774373</td>\n",
       "      <td>0.035731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trackonly</td>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>9.599223</td>\n",
       "      <td>0.036068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>predtrack</td>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>1.885587</td>\n",
       "      <td>0.035189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>predtrack</td>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>4.774574</td>\n",
       "      <td>0.035731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>predtrack</td>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>9.599448</td>\n",
       "      <td>0.036068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>curtrack</td>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>1.893812</td>\n",
       "      <td>0.035256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>curtrack</td>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>4.774360</td>\n",
       "      <td>0.035731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>curtrack</td>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>9.599215</td>\n",
       "      <td>0.036068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zerotrack</td>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>1.884415</td>\n",
       "      <td>0.035188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zerotrack</td>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>4.774658</td>\n",
       "      <td>0.035731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zerotrack</td>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>9.599525</td>\n",
       "      <td>0.036068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepAR</td>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>1.901281</td>\n",
       "      <td>0.035213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepAR</td>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>4.774052</td>\n",
       "      <td>0.035731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepAR</td>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>9.604525</td>\n",
       "      <td>0.036069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zero</td>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>118</td>\n",
       "      <td>25.686441</td>\n",
       "      <td>25.677966</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>5.325424</td>\n",
       "      <td>1.881356</td>\n",
       "      <td>0.035188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zero</td>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>115</td>\n",
       "      <td>25.613913</td>\n",
       "      <td>25.591304</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>5.313043</td>\n",
       "      <td>4.756522</td>\n",
       "      <td>0.035723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zero</td>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>110</td>\n",
       "      <td>25.495455</td>\n",
       "      <td>25.454545</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>5.290909</td>\n",
       "      <td>9.590909</td>\n",
       "      <td>0.036067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       testid  plen type  reccnt    top1acc  top1acc_farmost   top5acc  \\\n",
       "4  fulloracle     2   aa     118   0.000000         0.000000  5.325424   \n",
       "4  fulloracle     5   aa     115   0.000000         0.000000  5.313043   \n",
       "4  fulloracle    10   aa     110   0.000000         0.000000  5.290909   \n",
       "4  notracklap     2   aa     118   0.000000         0.000000  5.325424   \n",
       "4  notracklap     5   aa     115   0.000000         0.000000  5.313043   \n",
       "4  notracklap    10   aa     110   0.000000         0.000000  5.290909   \n",
       "4   trackonly     2   aa     118   0.000000         0.000000  5.325424   \n",
       "4   trackonly     5   aa     115   0.000000         0.000000  5.313043   \n",
       "4   trackonly    10   aa     110   0.000000         0.000000  5.290909   \n",
       "4   predtrack     2   aa     118   0.000000         0.000000  5.325424   \n",
       "4   predtrack     5   aa     115   0.000000         0.000000  5.313043   \n",
       "4   predtrack    10   aa     110   0.000000         0.000000  5.290909   \n",
       "4    curtrack     2   aa     118   0.000000         0.000000  5.325424   \n",
       "4    curtrack     5   aa     115   0.000000         0.000000  5.313043   \n",
       "4    curtrack    10   aa     110   0.000000         0.000000  5.290909   \n",
       "4   zerotrack     2   aa     118   0.000000         0.000000  5.325424   \n",
       "4   zerotrack     5   aa     115   0.000000         0.000000  5.313043   \n",
       "4   zerotrack    10   aa     110   0.000000         0.000000  5.290909   \n",
       "4      deepAR     2   aa     118   0.000000         0.000000  5.325424   \n",
       "4      deepAR     5   aa     115   0.000000         0.000000  5.313043   \n",
       "4      deepAR    10   aa     110   0.000000         0.000000  5.290909   \n",
       "4        zero     2   aa     118  25.686441        25.677966  5.325424   \n",
       "4        zero     5   aa     115  25.613913        25.591304  5.313043   \n",
       "4        zero    10   aa     110  25.495455        25.454545  5.290909   \n",
       "\n",
       "   top5acc_farmost       tau      rmse  \n",
       "4         5.325424  1.886972  0.035195  \n",
       "4         5.313043  4.774371  0.035731  \n",
       "4         5.290909  9.599202  0.036068  \n",
       "4         5.325424  1.883813  0.035188  \n",
       "4         5.313043  4.774627  0.035731  \n",
       "4         5.290909  9.599488  0.036068  \n",
       "4         5.325424  1.887286  0.035192  \n",
       "4         5.313043  4.774373  0.035731  \n",
       "4         5.290909  9.599223  0.036068  \n",
       "4         5.325424  1.885587  0.035189  \n",
       "4         5.313043  4.774574  0.035731  \n",
       "4         5.290909  9.599448  0.036068  \n",
       "4         5.325424  1.893812  0.035256  \n",
       "4         5.313043  4.774360  0.035731  \n",
       "4         5.290909  9.599215  0.036068  \n",
       "4         5.325424  1.884415  0.035188  \n",
       "4         5.313043  4.774658  0.035731  \n",
       "4         5.290909  9.599525  0.036068  \n",
       "4         5.325424  1.901281  0.035213  \n",
       "4         5.313043  4.774052  0.035731  \n",
       "4         5.290909  9.604525  0.036069  \n",
       "4         5.325424  1.881356  0.035188  \n",
       "4         5.313043  4.756522  0.035723  \n",
       "4         5.290909  9.590909  0.036067  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfacc[dfacc['type']=='aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_ssd/hpda/anaconda3/envs/gluonts/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    " \n",
    "\n",
    "#half=[True, False]\n",
    "#plens=[2,5,10,20,30]\n",
    "plens=[2,5,10]\n",
    "half=[0]\n",
    "trainids = [\"2018\"]\n",
    "#trainids = [\"r0.5\",\"r0.6\"]\n",
    "runs = 1\n",
    "train_ratio=0.4\n",
    "\n",
    "acc_output = f'{_exp_id}-evaluate-mean-splitbyevent-fulltest-contigency-d{_dataset_id}-t{_test_event}-r{runs}-tr{train_ratio}-result.csv'\n",
    "ret_output = f'{_exp_id}-evaluate-mean-splitbyevent-fulltest-all-d{_dataset_id}-t{_test_event}-r{runs}-tr{train_ratio}-result.csv'\n",
    "\n",
    "#trainids = [\"indy500\"]\n",
    "#runs = 1\n",
    "#plens=[2]\n",
    "\n",
    "\n",
    "config = {'oracle':\n",
    "            {'fulloracle':MODE_ORACLE,'notracklap':MODE_NOTRACK + MODE_NOLAP,\n",
    "             'laponly':MODE_ORACLE_LAPONLY, 'trackonly':MODE_ORACLE_TRACKONLY,\n",
    "              'fullpred':MODE_PREDTRACK + MODE_PREDPIT,\n",
    "              'predtrack':MODE_PREDTRACK + MODE_ORACLE_TRACKONLY,\n",
    "              'predpit':MODE_PREDPIT + MODE_ORACLE_LAPONLY,\n",
    "              'curtrack':MODE_TESTCURTRACK,\n",
    "              'zerotrack':MODE_TESTZERO\n",
    "             },\n",
    "          'deepAR':{'deepAR':MODE_ORACLE},\n",
    "          'naive':{'naive':MODE_ORACLE},\n",
    "          'zero':{'zero':MODE_ORACLE}\n",
    "         }\n",
    "\n",
    "ref_testset = get_ref_oracle_testds(plens, half, train_ratio=train_ratio)\n",
    "\n",
    "dfret, dfacc = dotest(config)\n",
    "\n",
    "dfret.to_csv(ret_output, float_format='%.3f')\n",
    "dfacc.to_csv(acc_output, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfacc[dfacc['type']=='aa']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjust pit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODE_DISTURB_CLEARTRACK = 64\n",
    "#MODE_DISTURB_ADJUSTTRACK = 128\n",
    "#MODE_DISTURB_ADJUSTPIT = 256\n",
    "config_adjust = {'cleartrack':MODE_DISTURB_CLEARTRACK,'adjusttrack':MODE_DISTURB_CLEARTRACK + MODE_DISTURB_ADJUSTTRACK,\n",
    "         'adjustpit':MODE_DISTURB_ADJUSTPIT, 'adjustall':MODE_DISTURB_CLEARTRACK+ MODE_DISTURB_ADJUSTTRACK+MODE_DISTURB_ADJUSTPIT,\n",
    "         }\n",
    "adjust_dfret, adjust_dfacc = dotest(config_adjust)\n",
    "config_t = {'adjust_laponly':MODE_DISTURB_ADJUSTPIT + MODE_ORACLE_LAPONLY\n",
    "         }\n",
    "\n",
    "t_dfret, t_dfacc = dotest(config_t)\n",
    "config_t = {'predpit':MODE_PREDPIT + MODE_ORACLE_LAPONLY\n",
    "         }\n",
    "\n",
    "t2_dfret, t2_dfacc = dotest(config_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_modeldict={-3:0.1, -2:0.15,-1:0.2,0:0.05, 1:0.2, 2:0.15,3:0.1 }\n",
    "_adjust_model = build_random_model(_modeldict)\n",
    "datamode = MODE_DISTURB_ADJUSTPIT + MODE_ORACLE_LAPONLY\n",
    "check_testds(datamode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_t = {'newadjust_laponly':MODE_DISTURB_ADJUSTPIT + MODE_ORACLE_LAPONLY\n",
    "         }\n",
    "\n",
    "t3_dfret, t3_dfacc = dotest(config_t)\n",
    "test_dfacc = pd.concat([t_dfacc,t2_dfacc,t3_dfacc], axis=0)\n",
    "test_dfacc[test_dfacc['type']=='aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_modeldict={-3:0.0, -2:0.15,-1:0.2,0:0.2, 1:0.2, 2:0.15,3:0.0 }\n",
    "_adjust_model = build_random_model(_modeldict)\n",
    "init_adjust_pitmodel()\n",
    "datamode = MODE_DISTURB_ADJUSTPIT + MODE_ORACLE_LAPONLY\n",
    "check_testds(datamode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_t = {'newadjust_laponly':MODE_DISTURB_ADJUSTPIT + MODE_ORACLE_LAPONLY\n",
    "         }\n",
    "\n",
    "t3_dfret, t3_dfacc = dotest(config_t)\n",
    "test_dfacc = pd.concat([t_dfacc,t2_dfacc,t3_dfacc], axis=0)\n",
    "test_dfacc[test_dfacc['type']=='aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_modeldict={-4:0.05, -3:0.1, -2:0.2,-1:0.3, 0:0.2, 1:0.1, 2:0.1 }\n",
    "_adjust_model = build_random_model(_modeldict)\n",
    "init_adjust_pitmodel()\n",
    "datamode = MODE_DISTURB_ADJUSTPIT + MODE_ORACLE_LAPONLY\n",
    "check_testds(datamode)\n",
    "\n",
    "real_model = build_random_model(_empirical_model)\n",
    "print('adjust model:')\n",
    "print_model(_adjust_model, iscdf=True)\n",
    "print('real adjust model:')\n",
    "print_model(real_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_t = {'newadjust_laponly':MODE_DISTURB_ADJUSTPIT + MODE_ORACLE_LAPONLY\n",
    "         }\n",
    "\n",
    "t3_dfret, t3_dfacc = dotest(config_t)\n",
    "test_dfacc = pd.concat([t_dfacc,t2_dfacc,t3_dfacc], axis=0)\n",
    "test_dfacc[test_dfacc['type']=='aa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dataret = run_test(1, [10],[0], trainids, 0.4, 'zero', datamode=MODE_ORACLE,models=['zero'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkret_status(dataret,model='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_ret = dataret[0][0][2]['zero'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_ret[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(global_start_offset[_test_event], pd.core.frame.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(global_start_offset[_test_event])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dataret = run_test(1, [10],[0], trainids, 0.4, 'oracle', datamode=MODE_ORACLE,models=['oracle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
