{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalute a lstmts model on rank prediction by lap time \n",
    "\n",
    "### The Problem\n",
    "\n",
    "Rank is calculated by the elapsed time when car crosses the start-finish line. The order of the cars for the same lap number is its rank.\n",
    "\n",
    "This experiment evaluates the performance of a model which predicts the lap time of the next lap, compared with a navie prediction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from indycar.notebook import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/hpda/indycar/predictor/notebook/CalculateRank'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scratch/hpda/anaconda3/envs/predictor/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "load 2017-Indy500-completed_laps_diff.csv, len=3216\n",
      "load 2018-Indy500-completed_laps_diff.csv, len=3618\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6834 entries, 0 to 3617\n",
      "Data columns (total 10 columns):\n",
      "MyIdx             6834 non-null int64\n",
      "car_number        6834 non-null int64\n",
      "completed_laps    6834 non-null int64\n",
      "rank              6834 non-null int64\n",
      "elapsed_time      6834 non-null float64\n",
      "rank_diff         6834 non-null float64\n",
      "time_diff         6834 non-null float64\n",
      "dbid              6834 non-null int64\n",
      "rank_diff_raw     6834 non-null float64\n",
      "time_diff_raw     6834 non-null float64\n",
      "dtypes: float64(5), int64(5)\n",
      "memory usage: 587.3 KB\n",
      "dataset shape (6834, 10)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# model trained in 'lstmts_20172018-completedcars' \n",
    "#\n",
    "D = 5\n",
    "datalist = ['2017-Indy500-completed_laps_diff.csv','2018-Indy500-completed_laps_diff.csv' ]\n",
    "model_name = 'lstmts'\n",
    "output_prefix = 'indy500-train2017-test2018-completed_laps-M%s-D%d'%(model_name, D)\n",
    "pred_outputfile = output_prefix + '-pred.csv'\n",
    "model_outputfile = output_prefix + '-model.h5'\n",
    "trainhist_outputfile = output_prefix + '-trainhist.jpg'\n",
    "eval_outputfile = output_prefix + '-eval.csv'\n",
    "\n",
    "lstmts_model = load_model(model_outputfile)\n",
    "\n",
    "scaler, dataset, dblens = load_data(datalist)\n",
    "\n",
    "dataset.info(verbose=True)\n",
    "print('dataset shape', dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carNumber = 34, max T =201\n",
      "train=16, val=0, test=18\n",
      "train shape: (16, 196, 1)\n"
     ]
    }
   ],
   "source": [
    "# generate train/test dataset\n",
    "\n",
    "# post-padding with 0\n",
    "X, y, w = generate_data(dataset, D=D, target='time')\n",
    "\n",
    "total = np.sum(dblens)\n",
    "ntrain = np.sum(dblens[:-1])\n",
    "#nval = int(dblens[-1] / 2)\n",
    "nval = 0\n",
    "ntest = total - ntrain - nval\n",
    "\n",
    "\n",
    "print('train=%d, val=%d, test=%d'%(ntrain, nval, total-ntrain-nval))\n",
    "\n",
    "X_train, X_val, X_test = X[:ntrain], X[ntrain:ntrain + nval], X[ntrain + nval:]\n",
    "y_train, y_val, y_test = y[:ntrain], y[ntrain:ntrain + nval], y[ntrain + nval:]\n",
    "#weights\n",
    "w_train, w_val, w_test = w[:ntrain], w[ntrain:ntrain + nval], w[ntrain+nval:]\n",
    "print('train shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# parameters\n",
    "#\n",
    "def load_pred_fromfile():\n",
    "    #year = '2017'\n",
    "    year = '2018'\n",
    "    #event = 'Toronto'\n",
    "    event = 'indy500'\n",
    "    #'indy500-2018-completed_laps'\n",
    "    inputfile = event +'-' + year + '-completed_laps-pred.csv'\n",
    "    outputprefix = year +'-' + event + '-eval-'\n",
    "    pred = pd.read_csv(inputfile)\n",
    "    pred_timediff = pred['pred']\n",
    "    return pred_timediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstmts model mae=0.065154, raw mae=6.575450, raw mape=9.081444\n"
     ]
    }
   ],
   "source": [
    "# X_test, y_test, w_test\n",
    "#\n",
    "#pred_timediff = load_pred_fromfile()\n",
    "lstmts_result = predict('lstmts', lstmts_model, X_test, y_test, scaler)\n",
    "pred_timediff = lstmts_result[1][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred shape (3528,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27.20769278, 35.47653671, 39.83164303, 41.75833875])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('pred shape',pred_timediff.shape)\n",
    "pred_timediff[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true/predicted elapsedtime\n",
    "def get_elapsedtime(dataset, pred_timediff, testsize, D):\n",
    "    \"\"\"\n",
    "    dataset ; raw data frame\n",
    "    pred_timediff;  prediction result of lap time\n",
    "    testsize ; size of testset\n",
    "    \"\"\"\n",
    "    n = pred_timediff.shape[0]\n",
    "\n",
    "    cars = []\n",
    "    groups = []\n",
    "    for car, group in dataset.groupby('car_number'):\n",
    "        cars.append(car)\n",
    "        groups.append(groups)\n",
    "\n",
    "    testset = cars[-testsize:]\n",
    "    rankdata = dataset[dataset['car_number'].isin(testset)]\n",
    "    carnum = len(testset)\n",
    "    print('testset car number', carnum)\n",
    "\n",
    "    print('rankdata shape', rankdata.shape)\n",
    "    rankdata[:4]\n",
    "\n",
    "    # reshape to <carno, data>\n",
    "    true_elapsedtime = np.array(rankdata['elapsed_time']).reshape((carnum, -1))\n",
    "    pred_laptime = np.array(pred_timediff).reshape((carnum, -1))\n",
    "\n",
    "    pred_elapsedtime = np.zeros_like(true_elapsedtime)\n",
    "    pred_elapsedtime[:,D:] = true_elapsedtime[:,0:-D]\n",
    "    pred_elapsedtime[:,D:] += pred_laptime\n",
    "    \n",
    "    return true_elapsedtime,pred_elapsedtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testset car number 18\n",
      "rankdata shape (3618, 10)\n",
      "[[  0.415   42.7829  83.8813 124.9756 166.0936 207.1653 248.3523 289.6172\n",
      "  330.9584 372.3228]\n",
      " [  1.4313  46.9758  88.5802 130.104  171.3932 212.7971 254.3784 295.9577\n",
      "  337.8812 379.6526]]\n",
      "[[  0.           0.           0.           0.           0.\n",
      "   27.62269278  78.25943671 123.71294303 166.73393875 208.34503361]\n",
      " [  0.           0.           0.           0.           0.\n",
      "   28.63899278  82.64872649 128.54215698 171.9548852  213.70780401]]\n"
     ]
    }
   ],
   "source": [
    "true_elapsedtime, pred_elapsedtime = get_elapsedtime(dataset, pred_timediff, ntest, D)\n",
    "print(true_elapsedtime[0:2,:10])\n",
    "print(pred_elapsedtime[0:2,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(rankdata[rankdata['completed_laps']<7]['rank']).reshape((ntest,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank caculation\n",
    "# evalutate rank after 5 laps\n",
    "idx = np.argsort(true_elapsedtime, axis=0)\n",
    "true_rank = np.argsort(idx, axis=0)\n",
    "idx = np.argsort(pred_elapsedtime, axis=0)\n",
    "pred_rank = np.argsort(idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  3,  3,  3,  3,  3,  3],\n",
       "       [ 7,  7,  7,  7,  7,  7,  7],\n",
       "       [ 9,  9,  9,  9,  9,  9,  9],\n",
       "       [ 4,  4,  4,  4,  4,  4,  4],\n",
       "       [ 2,  2,  2,  2,  2,  2,  2],\n",
       "       [13, 13, 13, 12, 12, 12, 12],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 8,  8,  8,  8,  8,  8,  8],\n",
       "       [11, 11, 11, 11, 11, 11, 11],\n",
       "       [15, 15, 15, 15, 14, 14, 14],\n",
       "       [ 6,  6,  6,  6,  6,  6,  6],\n",
       "       [10, 10, 10, 10, 10, 10, 10],\n",
       "       [17, 17, 17, 17, 17, 17, 17],\n",
       "       [14, 14, 14, 14, 15, 15, 15],\n",
       "       [16, 16, 16, 16, 16, 16, 16],\n",
       "       [12, 12, 12, 13, 13, 13, 13],\n",
       "       [ 5,  5,  5,  5,  5,  5,  5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple model\n",
    "simple_rank = np.zeros_like(true_rank)\n",
    "simple_rank[:,D:] = true_rank[:,0:-D] \n",
    "simple_rank[:,28:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  3,  1,  0,  1,  7,  2],\n",
       "       [ 7,  7,  7,  7,  0,  0,  9],\n",
       "       [ 9,  9,  9,  8,  5, 14, 14],\n",
       "       [ 4,  4,  4,  2,  3, 11,  5],\n",
       "       [ 2,  2,  3,  1,  2, 10,  4],\n",
       "       [12, 12, 12, 11,  8,  3,  0],\n",
       "       [ 0,  0,  0,  5, 12,  6,  1],\n",
       "       [ 1,  1,  2,  6, 13,  8,  3],\n",
       "       [ 8,  8,  8, 12, 16, 15,  8],\n",
       "       [11, 11, 11, 10,  7,  2, 11],\n",
       "       [14, 14, 14, 14,  9,  4, 12],\n",
       "       [ 6,  6,  6,  4, 10, 13,  7],\n",
       "       [10, 10, 10,  9,  6,  1, 10],\n",
       "       [17, 17, 16, 16, 14,  9, 15],\n",
       "       [15, 15, 15, 15, 11,  5, 13],\n",
       "       [16, 16, 17, 17, 15, 17, 17],\n",
       "       [13, 13, 13, 13, 17, 16, 16],\n",
       "       [ 5,  5,  5,  3,  4, 12,  6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_rank[:,28:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  3,  3,  3,  3,  3,  3],\n",
       "       [ 7,  7,  7,  7,  7,  7,  7],\n",
       "       [ 9,  9,  9,  9,  9,  9,  9],\n",
       "       [ 4,  4,  4,  4,  4,  4,  4],\n",
       "       [ 2,  2,  2,  2,  2,  2,  2],\n",
       "       [13, 13, 13, 12, 12, 12, 12],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  1,  1,  1,  1,  1],\n",
       "       [ 8,  8,  8,  8,  8,  8,  8],\n",
       "       [11, 11, 11, 11, 11, 11, 11],\n",
       "       [15, 15, 15, 15, 14, 14, 14],\n",
       "       [ 6,  6,  6,  6,  6,  6,  6],\n",
       "       [10, 10, 10, 10, 10, 10, 10],\n",
       "       [17, 17, 17, 17, 17, 17, 16],\n",
       "       [14, 14, 14, 14, 15, 15, 15],\n",
       "       [16, 16, 16, 16, 16, 16, 17],\n",
       "       [12, 12, 12, 13, 13, 13, 13],\n",
       "       [ 5,  5,  5,  5,  5,  5,  5]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rank[:,28:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evluate the rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval on model:  lstmts\n",
      "accuracy: 0.37258153676064126\n",
      "top1 accuracy: 0.6020408163265306\n",
      "top5 accuracy: 0.45510204081632655\n",
      "top5 precision: 0.7183673469387755\n",
      "eval on model:  simple\n",
      "accuracy: 0.4508015478164732\n",
      "top1 accuracy: 0.6989795918367347\n",
      "top5 accuracy: 0.5714285714285714\n",
      "top5 precision: 0.7887755102040817\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "def eval_model(name, trueth, pred, startlap = 5):\n",
    "    match_mask = (trueth == pred)\n",
    "    print('eval on model: ', name)\n",
    "    accuracy = np.sum(match_mask)/(match_mask.shape[1]*match_mask.shape[0])\n",
    "    print('accuracy:', accuracy)\n",
    "\n",
    "    top1 = np.logical_and((trueth==0),match_mask)[:,startlap:]\n",
    "    top1_accuracy = np.sum(top1)/top1.shape[1]\n",
    "    print('top1 accuracy:', top1_accuracy)\n",
    "    \n",
    "    top5 = np.logical_and((trueth < 5),match_mask)[:,startlap:]\n",
    "    top5_accuracy = np.sum(top5)/top5.shape[1]/5\n",
    "    print('top5 accuracy:', top5_accuracy)\n",
    "    # precision is more useful\n",
    "    top5 = np.logical_and((trueth < 5),(pred < 5))[:,startlap:]\n",
    "    top5_precision = np.sum(top5)/top5.shape[1]/5\n",
    "    print('top5 precision:', top5_precision)\n",
    "    \n",
    "    return accuracy,top1_accuracy,top5_accuracy,top5_precision\n",
    "    \n",
    "models ={'lstmts':pred_rank, 'simple':simple_rank}\n",
    "ret = []\n",
    "for m in models:\n",
    "    ret.append(eval_model(m, true_rank, models[m]))\n",
    "    \n",
    "result = np.array(ret)\n",
    "#ret1 = eval_model('lstmts', true_rank, pred_rank)\n",
    "#ret2 = eval_model('simple', true_rank, simple_rank)\n",
    "\n",
    "df = pd.DataFrame({'model':list(models.keys()), 'accuracy':result[:,0],\n",
    "                   'top1_accuracy':result[:,1],'top5_accuracy':result[:,2],\n",
    "                   'top5_precision':result[:,3]})\n",
    "df\n",
    "df.to_csv(eval_outputfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>top1_accuracy</th>\n",
       "      <th>top5_accuracy</th>\n",
       "      <th>top5_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lstmts</td>\n",
       "      <td>0.372582</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.455102</td>\n",
       "      <td>0.718367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple</td>\n",
       "      <td>0.450802</td>\n",
       "      <td>0.698980</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.788776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  accuracy  top1_accuracy  top5_accuracy  top5_precision\n",
       "0  lstmts  0.372582       0.602041       0.455102        0.718367\n",
       "1  simple  0.450802       0.698980       0.571429        0.788776"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
