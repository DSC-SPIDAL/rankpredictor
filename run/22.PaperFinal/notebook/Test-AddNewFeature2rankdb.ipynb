{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add New feature to rankdb\n",
    "\n",
    "base:   \n",
    "  22.PaperFinal/RankNet-QuickTest.ipynb  \n",
    "  22.PaperFinal/RankNet-makedb-rankmodel.ipynb\n",
    "    \n",
    "new features:\n",
    "    \n",
    "    1. LeadersPitCnt   ;  how many leaders go to pit stop\n",
    "    2. shift update leaderpitcnt   ; calc leaderpitcnt by previous rank status shift of prediction_len\n",
    "    [todo] 3. dynamically update leaderpitcnt   ; for the prediction_len future laps using current rank status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import random\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import ipdb; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def test_flag(a, bitflag):\n",
    "    return (a & bitflag) ==  bitflag\n",
    "\n",
    "#\n",
    "# remove NaN at the tail\n",
    "# there should be no nans in the middle of the ts\n",
    "COL_LAPTIME=0\n",
    "COL_RANK=1\n",
    "COL_TRACKSTATUS=2\n",
    "COL_LAPSTATUS=3\n",
    "COL_TIMEDIFF=4\n",
    "COL_CAUTION_LAPS_INSTINT=5\n",
    "COL_LAPS_INSTINT= 6\n",
    "COL_ELAPSED_TIME= 7\n",
    "COL_LAP2NEXTPIT = 8\n",
    "#_featureCnt = 9\n",
    "\n",
    "# added new features\n",
    "COL_LEADER_PITCNT = 9\n",
    "COL_TOTAL_PITCNT = 10\n",
    "COL_SHIFT_TRACKSTATUS = 11\n",
    "COL_SHIFT_LAPSTATUS = 12\n",
    "COL_SHIFT_LEADER_PITCNT = 13\n",
    "COL_SHIFT_TOTAL_PITCNT = 14\n",
    "\n",
    "COL_LASTFEATURE = 14\n",
    "# dynamically extended space in simulation\n",
    "COL_TRACKSTATUS_SAVE = COL_LASTFEATURE+1\n",
    "COL_LAPSTATUS_SAVE = COL_LASTFEATURE+2\n",
    "COL_CAUTION_LAPS_INSTINT_SAVE = COL_LASTFEATURE+3\n",
    "COL_LAPS_INSTINT_SAVE= COL_LASTFEATURE+4\n",
    "\n",
    "COL_ENDPOS = COL_LASTFEATURE+5\n",
    "\n",
    "\n",
    "FEATURE_STATUS = 2\n",
    "FEATURE_PITAGE = 4\n",
    "FEATURE_LEADER_PITCNT = 8\n",
    "FEATURE_TOTAL_PITCNT = 16\n",
    "FEATURE_SHIFT_TRACKSTATUS = 32\n",
    "FEATURE_SHIFT_LAPSTATUS = 64\n",
    "FEATURE_SHIFT_LEADER_PITCNT = 128\n",
    "FEATURE_SHIFT_TOTAL_PITCNT  = 256\n",
    "\n",
    "_feature2str= {\n",
    "    FEATURE_STATUS : (\"FEATURE_STATUS\",'S'),\n",
    "    FEATURE_PITAGE : (\"FEATURE_PITAGE\",'A'),\n",
    "    FEATURE_LEADER_PITCNT : (\"FEATURE_LEADER_PITCNT\",'L'),\n",
    "    FEATURE_TOTAL_PITCNT : (\"FEATURE_TOTAL_PITCNT\",'T'),\n",
    "    FEATURE_SHIFT_TRACKSTATUS : (\"FEATURE_SHIFT_TRACKSTATUS\",'Y'),\n",
    "    FEATURE_SHIFT_LAPSTATUS : (\"FEATURE_SHIFT_LAPSTATUS\",'P'),\n",
    "    FEATURE_SHIFT_LEADER_PITCNT : (\"FEATURE_SHIFT_LEADER_PITCNT\",'L'),\n",
    "    FEATURE_SHIFT_TOTAL_PITCNT  : (\"FEATURE_SHIFT_TOTAL_PITCNT\",'T')\n",
    "    }\n",
    "\n",
    "\n",
    "MODE_ORACLE = 0\n",
    "MODE_NOLAP = 1\n",
    "MODE_NOTRACK = 2\n",
    "MODE_TESTZERO = 4\n",
    "MODE_TESTCURTRACK = 8\n",
    "#MODE_STR={MODE_ORACLE:'oracle', MODE_NOLAP:'nolap',MODE_NOTRACK:'notrack',MODE_TEST:'test'}\n",
    "\n",
    "#_feature_mode = FEATURE_STATUS\n",
    "def decode_feature_mode(feature_mode):\n",
    "    \n",
    "    retstr = []\n",
    "    short_ret = []\n",
    "    for feature in _feature2str.keys():\n",
    "        if test_flag(feature_mode, feature):\n",
    "            retstr.append(_feature2str[feature][0])\n",
    "            short_ret.append(_feature2str[feature][1])\n",
    "        else:\n",
    "            short_ret.append('0')\n",
    "\n",
    "    print(' '.join(retstr))\n",
    "    \n",
    "    return ''.join(short_ret)\n",
    "\n",
    "\n",
    "def add_leader_cnt(selmat, rank_col=COL_RANK, pit_col=COL_LAPSTATUS, shift_len = 0, \n",
    "                   dest_col = COL_LEADER_PITCNT,\n",
    "                   verbose = False):\n",
    "    \"\"\"\n",
    "    add a new feature into mat(car, feature, lap)\n",
    "    \n",
    "    shift rank status\n",
    "    \n",
    "    input:\n",
    "        sel_mat : laptime_data array [car, feature, lap]\n",
    "    \n",
    "    \"\"\"\n",
    "    dim1, dim2, dim3 = selmat.shape\n",
    "    \n",
    "    # rerank by the rank_col\n",
    "    idx = np.argsort(selmat[:, rank_col,:], axis=0)\n",
    "    true_rank = np.argsort(idx, axis=0).astype(np.float)\n",
    "\n",
    "    # get leaderCnt by sorted pits\n",
    "    pits = np.zeros((dim1,dim3))\n",
    "    \n",
    "    for lap in range(shift_len, dim3):\n",
    "        col = idx[:, lap-shift_len]\n",
    "        pits[:, lap] = selmat[col, pit_col, lap]\n",
    "    \n",
    "    leaderCnt = np.nancumsum(pits, axis=0) - pits\n",
    "    \n",
    "    if verbose:\n",
    "        print('pits:\\n')\n",
    "        print(pits[:,190:])\n",
    "        print('leaderCnt raw:\\n')\n",
    "        print(leaderCnt[:,190:])\n",
    "    \n",
    "    #remove nans\n",
    "    nanidx = np.isnan(leaderCnt)\n",
    "    leaderCnt[nanidx] = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print('leaderCnt after remove nan:\\n')\n",
    "        print(leaderCnt[:,190:])\n",
    "    \n",
    "    if dest_col == -1:\n",
    "        #create a new data\n",
    "        newmat = np.zeros((dim1,dim2+1,dim3))\n",
    "        dest_col = dim2\n",
    "        newmat[:,:dim2,:] = selmat.copy()\n",
    "    else:\n",
    "        #update mode\n",
    "        newmat = selmat\n",
    "    \n",
    "    for lap in range(dim3):\n",
    "        col = idx[:, lap]\n",
    "        newmat[col, dest_col, lap] = leaderCnt[:, lap]\n",
    "        \n",
    "    # sync length to COL_RANK\n",
    "    for rec in newmat:\n",
    "        nans, x= nan_helper(rec[rank_col,:])\n",
    "        nan_count = np.sum(nans)\n",
    "        if nan_count > 0:\n",
    "            #todo, some invalid nan, remove them\n",
    "            #rec[dim2, np.isnan(rec[dim2,:])] = 0\n",
    "            rec[dest_col, -nan_count:] = np.nan\n",
    "    \n",
    "    return newmat\n",
    "\n",
    "def add_allpit_cnt(selmat, rank_col=COL_RANK, pit_col=COL_LAPSTATUS, \n",
    "                   dest_col = COL_TOTAL_PITCNT,verbose = False):\n",
    "    \"\"\"\n",
    "    add a new feature into mat(car, feature, lap)\n",
    "    \n",
    "    total pits in a lap\n",
    "    \n",
    "    input:\n",
    "        sel_mat : laptime_data array [car, feature, lap]\n",
    "    \n",
    "    \"\"\"\n",
    "    dim1, dim2, dim3 = selmat.shape\n",
    "\n",
    "    #calc totalCnt vector for \n",
    "    totalCnt = np.nansum(selmat[:, pit_col, :], axis=0).reshape((-1))\n",
    "    \n",
    "    if verbose:\n",
    "        print('pits:\\n')\n",
    "        print(pits[:,190:])\n",
    "        print('totalCnt raw:\\n')\n",
    "        print(totalCnt[190:])\n",
    "    \n",
    "    #remove nans\n",
    "    nanidx = np.isnan(totalCnt)\n",
    "    totalCnt[nanidx] = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print('totalCnt after remove nan:\\n')\n",
    "        print(totalCnt[190:])\n",
    "    \n",
    "    if dest_col == -1:\n",
    "        #create a new data\n",
    "        newmat = np.zeros((dim1,dim2+1,dim3))\n",
    "        dest_col = dim2\n",
    "        newmat[:,:dim2,:] = selmat.copy()\n",
    "    else:\n",
    "        #update mode\n",
    "        newmat = selmat\n",
    "\n",
    "    for car in range(dim1):\n",
    "        newmat[car, dest_col, :] = totalCnt\n",
    "        \n",
    "    # sync length to COL_RANK\n",
    "    for rec in newmat:\n",
    "        nans, x= nan_helper(rec[rank_col,:])\n",
    "        nan_count = np.sum(nans)\n",
    "        if nan_count > 0:\n",
    "            #todo, some invalid nan, remove them\n",
    "            #rec[dim2, np.isnan(rec[dim2,:])] = 0\n",
    "            rec[dest_col, -nan_count:] = np.nan\n",
    "    \n",
    "    return newmat\n",
    "\n",
    "def add_shift_feature(selmat, rank_col=COL_RANK, shift_col=COL_LAPSTATUS, shift_len = 2, \n",
    "                      dest_col = -1,verbose = False):\n",
    "    \"\"\"\n",
    "    add a new feature into mat(car, feature, lap)\n",
    "    \n",
    "    shift features left in a lap\n",
    "    \n",
    "    warning: these are oracle features, be careful not to let future rank positions leaking\n",
    "    \n",
    "    input:\n",
    "        sel_mat : laptime_data array [car, feature, lap]\n",
    "    \n",
    "    \"\"\"\n",
    "    dim1, dim2, dim3 = selmat.shape\n",
    "\n",
    "    if dest_col == -1:\n",
    "        #create a new data\n",
    "        newmat = np.zeros((dim1,dim2+1,dim3))\n",
    "        dest_col = dim2\n",
    "        newmat[:,:dim2,:] = selmat.copy()\n",
    "    else:\n",
    "        #update mode\n",
    "        newmat = selmat\n",
    "    \n",
    "    for car in range(dim1):\n",
    "        # set empty status by default\n",
    "        newmat[car, dest_col, :] = np.nan\n",
    "        \n",
    "        # get valid laps\n",
    "        rec = selmat[car]\n",
    "        nans, x= nan_helper(rec[rank_col,:])\n",
    "        nan_count = np.sum(nans)\n",
    "        recnnz = rec[shift_col, ~np.isnan(rec[rank_col,:])]\n",
    "        reclen = len(recnnz)\n",
    "\n",
    "        #shift copy\n",
    "        newmat[car, dest_col, :reclen] = 0\n",
    "        #newmat[car, dim2, :-shift_len] = selmat[car, shift_col, shift_len:]\n",
    "        newmat[car, dest_col, :reclen-shift_len] = recnnz[shift_len:]\n",
    "        \n",
    "    # sync length to COL_RANK\n",
    "    #for rec in newmat:\n",
    "    #    nans, x= nan_helper(rec[rank_col,:])\n",
    "    #    nan_count = np.sum(nans)\n",
    "    #    if nan_count > 0:\n",
    "    #        #todo, some invalid nan, remove them\n",
    "    #        #rec[dim2, np.isnan(rec[dim2,:])] = 0\n",
    "    #        rec[dim2, -nan_count:] = np.nan\n",
    "    \n",
    "    return newmat\n",
    "\n",
    "\n",
    "def prepare_laptimedata(prediction_length, freq, \n",
    "                       test_event = 'Indy500-2018',\n",
    "                       train_ratio=0.8,\n",
    "                       context_ratio = 0.,\n",
    "                       shift_len = -1):\n",
    "    \"\"\"\n",
    "    prepare the laptime data for training\n",
    "    \n",
    "    1. remove short ts\n",
    "    2. rerank the tss\n",
    "    3. create new features\n",
    "    \n",
    "    input: \n",
    "        laptime_data   ; global var\n",
    "    output:\n",
    "        data  ; new representation of laptime_data\n",
    "    \n",
    "    \"\"\"\n",
    "    _laptime_data = laptime_data.copy()\n",
    "    \n",
    "    test_eventid = events_id[test_event]\n",
    "    run_ts = COL_RANK\n",
    "    \n",
    "    # check shift len\n",
    "    if shift_len < 0:\n",
    "        shift_len = prediction_length\n",
    "    print('prepare_laptimedata shift len:', shift_len)\n",
    "    \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    new_data = []\n",
    "    for _data in _laptime_data:\n",
    "        #skip eid > test_eventid\n",
    "        if _data[0] > test_eventid:\n",
    "            print('skip this event:', events[_data[0]])\n",
    "            break\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        else:\n",
    "            test_mode = False        \n",
    "        \n",
    "        #statistics on the ts length\n",
    "        ts_len = [ _entry.shape[1] for _entry in _data[2]]\n",
    "        train_len = int(np.max(ts_len) * train_ratio)\n",
    "        if train_len == 0:\n",
    "            #use global train_len\n",
    "            train_len = _train_len if not test_mode else _test_train_len\n",
    "        \n",
    "        if context_ratio != 0.:\n",
    "            # add this part to train set\n",
    "            context_len = int(np.max(ts_len) * context_ratio)\n",
    "        else:    \n",
    "            context_len = prediction_length*2\n",
    "        if context_len < 10:\n",
    "            context_len = 10\n",
    "        \n",
    "        print(f'before ====event:{events[_data[0]]}, prediction_len={prediction_length},train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)},context_len={context_len}')\n",
    "\n",
    "        #rerank due to short ts removed\n",
    "        #if run_ts == COL_RANK and dorerank == True:\n",
    "        if True:\n",
    "            sel_rows = []\n",
    "            \n",
    "            # use to check the dimension of features\n",
    "            input_feature_cnt = _data[2].shape[1]\n",
    "            if input_feature_cnt < COL_LASTFEATURE + 1:\n",
    "                print('create new features mode, feature_cnt:', input_feature_cnt)\n",
    "            else:\n",
    "                print('update features mode, feature_cnt:', input_feature_cnt)\n",
    "            \n",
    "            for rowid in range(_data[2].shape[0]):\n",
    "                # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "                rec = _data[2][rowid].copy()\n",
    "                #remove nan(only tails)\n",
    "                nans, x= nan_helper(rec[run_ts,:])\n",
    "                nan_count = np.sum(nans)             \n",
    "                rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "                \n",
    "                totallen = rec.shape[1]\n",
    "                if ( totallen < train_len + prediction_length):\n",
    "                    print(f'rerank a short ts: carid={_data[1][rowid]}，len={totallen}')\n",
    "                    continue \n",
    "                else:\n",
    "                    sel_rows.append(rowid)\n",
    "                    \n",
    "            #get selected matrix\n",
    "            sel_idx = np.array(sel_rows)\n",
    "            selmat = _data[2][sel_idx]\n",
    "            \n",
    "            # check the format of _data\n",
    "            #ipdb.set_trace()\n",
    "            \n",
    "            mask = np.isnan(selmat[:,COL_RANK,:])\n",
    "            \n",
    "            idx = np.argsort(selmat[:,COL_RANK,:], axis=0)\n",
    "            true_rank = np.argsort(idx, axis=0).astype(np.float)\n",
    "            true_rank[mask] = np.nan\n",
    "            \n",
    "            if test_mode:\n",
    "                #\n",
    "                # for historical code mismatch, simulation does not run rerank\n",
    "                #\n",
    "                _data[2][sel_idx,COL_RANK,:] = true_rank + 1\n",
    "            else:\n",
    "                _data[2][sel_idx,COL_RANK,:] = true_rank\n",
    "            \n",
    "            # update the carno dict\n",
    "            new_carids = {}\n",
    "            for rowid in range(len(sel_idx)):\n",
    "                carid = sel_idx[rowid]\n",
    "                carno = _data[1][carid]\n",
    "                new_carids[rowid] = carno\n",
    "\n",
    "                \n",
    "            # add new features\n",
    "            # add leaderPitCnt\n",
    "            if _data[0]==0:\n",
    "                verbose = True\n",
    "            else:\n",
    "                verbose = False\n",
    "                \n",
    "\n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_LEADER_PITCNT\n",
    "            data2_intermediate = add_leader_cnt(_data[2][sel_idx], shift_len = shift_len, dest_col=dest_col, verbose = verbose)\n",
    "            \n",
    "            # add totalPit\n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_TOTAL_PITCNT\n",
    "            data2_intermediate = add_allpit_cnt(data2_intermediate, dest_col=dest_col)\n",
    "            \n",
    "            #\n",
    "            # add shift features, a fixed order, see the MACROS \n",
    "            #COL_SHIFT_TRACKSTATUS = 11\n",
    "            #COL_SHIFT_LAPSTATUS = 12\n",
    "            #COL_SHIFT_LEADER_PITCNT = 13\n",
    "            #COL_SHIFT_TOTAL_PITCNT = 14\n",
    "            #\n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_SHIFT_TRACKSTATUS\n",
    "            data2_intermediate = add_shift_feature(data2_intermediate, dest_col=dest_col,\n",
    "                                                   shift_col=COL_TRACKSTATUS, shift_len = shift_len)\n",
    "            \n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_SHIFT_LAPSTATUS\n",
    "            data2_intermediate = add_shift_feature(data2_intermediate, dest_col=dest_col,\n",
    "                                                   shift_col=COL_LAPSTATUS, shift_len = shift_len)\n",
    "            \n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_SHIFT_LEADER_PITCNT\n",
    "            data2_intermediate = add_shift_feature(data2_intermediate, dest_col=dest_col,\n",
    "                                                   shift_col=COL_LEADER_PITCNT, shift_len = shift_len)\n",
    "            \n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_SHIFT_TOTAL_PITCNT\n",
    "            data2_intermediate = add_shift_feature(data2_intermediate, dest_col=dest_col,\n",
    "                                                   shift_col=COL_TOTAL_PITCNT, shift_len = shift_len)\n",
    "            \n",
    "            # final\n",
    "            data2_newfeature = data2_intermediate\n",
    "            \n",
    "        new_data.append([_data[0], new_carids, data2_newfeature])\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "\n",
    "def get_real_features(feature_mode, rec, endpos):\n",
    "    \"\"\"\n",
    "    construct the real value feature vector from feature_mode\n",
    "\n",
    "    legacy code:\n",
    "        real_features = {\n",
    "            FEATURE_STATUS:[rec[COL_TRACKSTATUS,:],rec[COL_LAPSTATUS,:]],\n",
    "            FEATURE_PITAGE:[rec[COL_TRACKSTATUS,:],rec[COL_LAPSTATUS,:],rec[COL_LAPS_INSTINT,:]],\n",
    "            FEATURE_LEADERPITCNT:[rec[COL_TRACKSTATUS,:],rec[COL_LAPSTATUS,:],rec[COL_LEADER_PITCNT,:]],\n",
    "            FEATURE_TOTALPITCNT:[rec[COL_TRACKSTATUS,:],rec[COL_LAPSTATUS,:],rec[COL_TOTAL_PITCNT,:]]\n",
    "        }    \n",
    "    \n",
    "        real_features[feature_mode]\n",
    "        \n",
    "        \n",
    "        COL_LEADER_PITCNT = 9\n",
    "        COL_TOTAL_PITCNT = 10\n",
    "        COL_SHIFT_TRACKSTATUS = 11\n",
    "        COL_SHIFT_LAPSTATUS = 12\n",
    "        COL_SHIFT_LEADER_PITCNT = 13\n",
    "        COL_SHIFT_TOTAL_PITCNT = 14\n",
    "\n",
    "\n",
    "        FEATURE_STATUS = 2\n",
    "        FEATURE_PITAGE = 4\n",
    "        FEATURE_LEADER_PITCNT = 8\n",
    "        FEATURE_TOTAL_PITCNT = 16\n",
    "        FEATURE_SHIFT_TRACKSTATUS = 32\n",
    "        FEATURE_SHIFT_LAPSTATUS = 64\n",
    "        FEATURE_SHIFT_LEADER_PITCNT = 128\n",
    "        FEATURE_SHIFT_TOTAL_PITCNT  = 256        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    #check endpos\n",
    "    if endpos <=0 :\n",
    "        endpos = rec.shape[1]\n",
    "    \n",
    "    if test_flag(feature_mode, FEATURE_STATUS):\n",
    "        features.append(rec[COL_TRACKSTATUS,:endpos])\n",
    "        features.append(rec[COL_LAPSTATUS,:endpos])\n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_PITAGE):\n",
    "        features.append(rec[COL_LAPS_INSTINT,:endpos])\n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_LEADER_PITCNT):\n",
    "        features.append(rec[COL_LEADER_PITCNT,:endpos])\n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_TOTAL_PITCNT):\n",
    "        features.append(rec[COL_TOTAL_PITCNT,:endpos])    \n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_SHIFT_TRACKSTATUS):\n",
    "        features.append(rec[COL_SHIFT_TRACKSTATUS,:endpos])    \n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_SHIFT_LAPSTATUS):\n",
    "        features.append(rec[COL_SHIFT_LAPSTATUS,:endpos])    \n",
    "\n",
    "    if test_flag(feature_mode, FEATURE_SHIFT_LEADER_PITCNT):\n",
    "        features.append(rec[COL_SHIFT_LEADER_PITCNT,:endpos])    \n",
    "\n",
    "    if test_flag(feature_mode, FEATURE_SHIFT_TOTAL_PITCNT):\n",
    "        features.append(rec[COL_SHIFT_TOTAL_PITCNT,:endpos])    \n",
    "        \n",
    "        \n",
    "    return features\n",
    "\n",
    "def make_dataset_byevent(_laptime_data, prediction_length, freq, \n",
    "                       useeid = False,\n",
    "                       run_ts=COL_LAPTIME, \n",
    "                       test_event = 'Indy500-2018',\n",
    "                       use_global_dict = True,\n",
    "                       oracle_mode = MODE_ORACLE,\n",
    "                       half_moving_win = True,\n",
    "                       train_ratio=0.8,\n",
    "                       log_transform = False,\n",
    "                       context_ratio = 0.,\n",
    "                       dorerank = True,\n",
    "                       test_cars = []  \n",
    "                ):\n",
    "    \"\"\"\n",
    "    split the ts to train and test part by the ratio\n",
    "    \n",
    "    oracle_mode: false to simulate prediction in real by \n",
    "        set the covariates of track and lap status as nan in the testset\n",
    "            \n",
    "    \n",
    "    \"\"\"    \n",
    "    #global setting\n",
    "    feature_mode = _feature_mode\n",
    "    \n",
    "    start = pd.Timestamp(\"01-01-2019\", freq=freq)  # can be different for each time series\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    \n",
    "    totalTSCnt = 0\n",
    "    totalTSLen = 0\n",
    "    test_eventid = events_id[test_event]\n",
    "    \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    for _data in _laptime_data:\n",
    "        _train = []\n",
    "        _test = []\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        else:\n",
    "            test_mode = False\n",
    "            \n",
    "        #statistics on the ts length\n",
    "        ts_len = [ _entry.shape[1] for _entry in _data[2]]\n",
    "        train_len = int(np.max(ts_len) * train_ratio)\n",
    "        if train_len == 0:\n",
    "            #use global train_len\n",
    "            train_len = _train_len if not test_mode else _test_train_len\n",
    "        \n",
    "        if context_ratio != 0.:\n",
    "            # add this part to train set\n",
    "            context_len = int(np.max(ts_len) * context_ratio)\n",
    "        else:    \n",
    "            context_len = prediction_length*2\n",
    "        if context_len < 10:\n",
    "            context_len = 10\n",
    "        \n",
    "        print(f'after ====event:{events[_data[0]]}, prediction_len={prediction_length},train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)},context_len={context_len}')\n",
    "\n",
    "        # process for each ts\n",
    "        for rowid in range(_data[2].shape[0]):\n",
    "            # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "            rec = _data[2][rowid].copy()\n",
    "            \n",
    "            #remove nan(only tails)\n",
    "            nans, x= nan_helper(rec[run_ts,:])\n",
    "            nan_count = np.sum(nans)             \n",
    "            rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "            \n",
    "            # remove short ts\n",
    "            totallen = rec.shape[1]\n",
    "            \n",
    "            totalTSCnt += 1\n",
    "            totalTSLen += totallen\n",
    "            \n",
    "            if ( totallen < train_len + prediction_length):\n",
    "                print(f'a short ts: carid={_data[1][rowid]}，len={totallen}')\n",
    "                continue                \n",
    "            \n",
    "            if use_global_dict:\n",
    "                carno = _data[1][rowid]\n",
    "                carid = global_carids[_data[1][rowid]]\n",
    "            else:\n",
    "                #simulation dataset, todo, fix the carids as decoder\n",
    "                carno = rowid\n",
    "                carid = rowid\n",
    "                \n",
    "            #check carno in test_cars, testmode only\n",
    "            if len(test_cars)>0 and carno not in test_cars:\n",
    "                continue\n",
    "                \n",
    "            if useeid:\n",
    "                static_cat = [carid, _data[0]]    \n",
    "            else:\n",
    "                static_cat = [carid]    \n",
    "                \n",
    "            #first, get target a copy    \n",
    "            # target can be COL_XXSTATUS\n",
    "            target_val = rec[run_ts,:].copy().astype(np.float32)\n",
    "            if log_transform:\n",
    "                target_val = np.log(target_val + 1.0)\n",
    "            \n",
    "            # selection of features\n",
    "            if test_flag(oracle_mode, MODE_NOTRACK):                \n",
    "                rec[COL_TRACKSTATUS, :] = 0\n",
    "            if test_flag(oracle_mode, MODE_NOLAP):                \n",
    "                rec[COL_LAPSTATUS, :] = 0\n",
    "\n",
    "            test_rec_cnt = 0\n",
    "            if not test_mode:\n",
    "                # all go to train set\n",
    "                real_features = get_real_features(feature_mode, rec, -1)\n",
    "                \n",
    "                _train.append({'target': target_val, \n",
    "                            'start': start, \n",
    "                            'feat_static_cat': static_cat,\n",
    "                            'feat_dynamic_real': real_features\n",
    "                          })\n",
    "                    \n",
    "            else:\n",
    "                # reset train_len\n",
    "                if context_ratio != 0.:\n",
    "                    # all go to train set\n",
    "                    #add [0, context_len] to train set \n",
    "                    # all go to train set\n",
    "                    _train.append({'target': target_val[:context_len],  \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': get_real_features(feature_mode, rec, context_len)\n",
    "                              })\n",
    "                              \n",
    "                # testset\n",
    "                # multiple test ts(rolling window as half of the prediction_length)\n",
    "                #step = -int(prediction_length/2) if half_moving_win else -prediction_length\n",
    "                step = -1\n",
    "                for endpos in range(totallen, context_len+prediction_length, \n",
    "                                    step):\n",
    "\n",
    "                    track_rec = rec[COL_TRACKSTATUS, :endpos].copy()\n",
    "                    lap_rec = rec[COL_LAPSTATUS, :endpos].copy()\n",
    "                    pitage_rec = rec[COL_LAPS_INSTINT, :endpos].copy()\n",
    "\n",
    "                    real_features = get_real_features(feature_mode, rec, endpos)\n",
    "                    \n",
    "                    _test.append({'target': rec[run_ts,:endpos].astype(np.float32), \n",
    "                            'start': start, \n",
    "                            'feat_static_cat': static_cat,\n",
    "                            'feat_dynamic_real': real_features\n",
    "                             })\n",
    "                                 \n",
    "                    test_rec_cnt += 1\n",
    "            \n",
    "            #check feature cnt\n",
    "            featureCnt = len(real_features)\n",
    "            \n",
    "            #add one ts\n",
    "            print(f'carno:{carno}, totallen:{totallen}, nancount:{nan_count}, test_reccnt:{test_rec_cnt},featureCnt:{featureCnt}')\n",
    "\n",
    "        train_set.extend(_train)\n",
    "        test_set.extend(_test)\n",
    "\n",
    "    print(f'train len:{len(train_set)}, test len:{len(test_set)}, totsl TsCnt:{totalTSCnt}, total ts len:{totalTSLen}')\n",
    "    \n",
    "    train_ds = ListDataset(train_set, freq=freq)\n",
    "    test_ds = ListDataset(test_set, freq=freq)    \n",
    "    \n",
    "    return train_ds, test_ds, train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE_STATUS FEATURE_LEADER_PITCNT FEATURE_TOTAL_PITCNT FEATURE_SHIFT_TRACKSTATUS FEATURE_SHIFT_LAPSTATUS FEATURE_SHIFT_LEADER_PITCNT FEATURE_SHIFT_TOTAL_PITCNT\n",
      "feature_mode: 506 S0LTYPLT\n"
     ]
    }
   ],
   "source": [
    "#inlap status = \n",
    "# 0 , no inlap\n",
    "# 1 , set previous lap\n",
    "# 2 , set the next lap\n",
    "_inlap_status = 0\n",
    "\n",
    "#\n",
    "# featuremode in [FEATURE_STATUS, FEATURE_PITAGE]:\n",
    "#\n",
    "_feature_mode = 506\n",
    "_featureCnt = 9\n",
    "\n",
    "#\n",
    "# training parameters\n",
    "#\n",
    "freq = \"1min\"\n",
    "_train_len = 40\n",
    "prediction_length = 2\n",
    "\n",
    "context_ratio = 0.\n",
    "context_length =  40\n",
    "contextlen = context_length\n",
    "\n",
    "dataset='rank'\n",
    "_run_ts = COL_RANK\n",
    "\n",
    "_test_event = 'Indy500-2018'\n",
    "year = '2018'\n",
    "\n",
    "#\n",
    "# string map\n",
    "#\n",
    "inlapstr = {0:'noinlap',1:'inlap',2:'outlap'}\n",
    "weightstr = {True:'weighted',False:'noweighted'}\n",
    "catestr = {True:'cate',False:'nocate'}\n",
    "cur_featurestr = decode_feature_mode(_feature_mode)\n",
    "print('feature_mode:', _feature_mode, cur_featurestr)\n",
    "\n",
    "#\n",
    "# input data parameters\n",
    "#\n",
    "years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "events = [f'Indy500-{x}' for x in years]\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "dbid = f'Indy500_{years[0]}_{years[-1]}_v{_featureCnt}_p{_inlap_status}'\n",
    "_dataset_id = '%s-%s'%(inlapstr[_inlap_status], cur_featurestr)\n",
    "\n",
    "# standard output file names\n",
    "LAPTIME_DATASET = f'laptime_rank_timediff_pit-oracle-{dbid}.pickle' \n",
    "STAGE_DATASET = f'stagedata-{dbid}.pickle' \n",
    "EVALUATION_RESULT_DF = f'evaluation_result_d{dataset}.csv'\n",
    "LONG_FORECASTING_DFS = f'long_forecasting_dfs_d{dataset}.pickle'\n",
    "FORECAST_FIGS_DIR = f'forecast-figs-d{dataset}/'   \n",
    "\n",
    "_test_train_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load laptime and stage dataset: test/laptime_rank_timediff_pit-oracle-Indy500_2013_2019_v9_p0.pickle test/stagedata-Indy500_2013_2019_v9_p0.pickle\n",
      "prepare_laptimedata shift len: 2\n",
      "before ====event:Indy500-2013, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "create new features mode, feature_cnt: 9\n",
      "rerank a short ts: carid=4，len=3\n",
      "rerank a short ts: carid=6，len=34\n",
      "pits:\n",
      "\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. nan nan  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. nan nan  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. nan]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. nan]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. nan]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. nan nan]\n",
      " [ 0.  0.  0.  0.  0.  0. nan nan nan nan]\n",
      " [ 0.  0.  0.  0.  0. nan nan nan nan nan]\n",
      " [ 0.  0.  0. nan nan nan nan nan nan nan]\n",
      " [ 0.  0. nan nan nan nan nan nan nan nan]\n",
      " [ 0. nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "leaderCnt raw:\n",
      "\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. nan nan  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. nan nan  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0. nan]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0. nan]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0. nan]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0. nan nan]\n",
      " [ 0.  1.  0.  0.  0.  1. nan nan nan nan]\n",
      " [ 0.  1.  0.  0.  0. nan nan nan nan nan]\n",
      " [ 0.  1.  0. nan nan nan nan nan nan nan]\n",
      " [ 0.  1. nan nan nan nan nan nan nan nan]\n",
      " [ 0. nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "leaderCnt after remove nan:\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "before ====event:Indy500-2014, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "create new features mode, feature_cnt: 9\n",
      "before ====event:Indy500-2015, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "create new features mode, feature_cnt: 9\n",
      "rerank a short ts: carid=8，len=0\n",
      "rerank a short ts: carid=43，len=0\n",
      "before ====event:Indy500-2016, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "create new features mode, feature_cnt: 9\n",
      "before ====event:Indy500-2017, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "create new features mode, feature_cnt: 9\n",
      "before ====event:Indy500-2018, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "create new features mode, feature_cnt: 9\n",
      "skip this event: Indy500-2019\n",
      "prepare_laptimedata shift len: 2\n",
      "before ====event:Indy500-2013, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "update features mode, feature_cnt: 15\n",
      "pits:\n",
      "\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. nan nan  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. nan nan  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. nan]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. nan]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0. nan]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. nan nan]\n",
      " [ 0.  0.  0.  0.  0.  0. nan nan nan nan]\n",
      " [ 0.  0.  0.  0.  0. nan nan nan nan nan]\n",
      " [ 0.  0.  0. nan nan nan nan nan nan nan]\n",
      " [ 0.  0. nan nan nan nan nan nan nan nan]\n",
      " [ 0. nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "leaderCnt raw:\n",
      "\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. nan nan  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. nan nan  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0. nan]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0. nan]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0.  0. nan]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.  0. nan nan]\n",
      " [ 0.  1.  0.  0.  0.  1. nan nan nan nan]\n",
      " [ 0.  1.  0.  0.  0. nan nan nan nan nan]\n",
      " [ 0.  1.  0. nan nan nan nan nan nan nan]\n",
      " [ 0.  1. nan nan nan nan nan nan nan nan]\n",
      " [ 0. nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "leaderCnt after remove nan:\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "before ====event:Indy500-2014, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "update features mode, feature_cnt: 15\n",
      "before ====event:Indy500-2015, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "update features mode, feature_cnt: 15\n",
      "before ====event:Indy500-2016, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "update features mode, feature_cnt: 15\n",
      "before ====event:Indy500-2017, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "update features mode, feature_cnt: 15\n",
      "before ====event:Indy500-2018, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "update features mode, feature_cnt: 15\n",
      "after ====event:Indy500-2013, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "carno:1, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:2, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:3, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:5, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:7, totallen:178, nancount:22, test_reccnt:0,featureCnt:8\n",
      "carno:8, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:9, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:10, totallen:197, nancount:3, test_reccnt:0,featureCnt:8\n",
      "carno:11, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:12, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:14, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:15, totallen:193, nancount:7, test_reccnt:0,featureCnt:8\n",
      "carno:16, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:18, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:19, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:20, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:21, totallen:191, nancount:9, test_reccnt:0,featureCnt:8\n",
      "carno:22, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:25, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:26, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:27, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:41, totallen:198, nancount:2, test_reccnt:0,featureCnt:8\n",
      "carno:55, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:60, totallen:192, nancount:8, test_reccnt:0,featureCnt:8\n",
      "carno:63, totallen:46, nancount:154, test_reccnt:0,featureCnt:8\n",
      "carno:77, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:78, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:81, totallen:193, nancount:7, test_reccnt:0,featureCnt:8\n",
      "carno:83, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:91, totallen:44, nancount:156, test_reccnt:0,featureCnt:8\n",
      "carno:98, totallen:196, nancount:4, test_reccnt:0,featureCnt:8\n",
      "after ====event:Indy500-2014, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "carno:2, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carno:3, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:5, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:6, totallen:190, nancount:10, test_reccnt:0,featureCnt:8\n",
      "carno:7, totallen:198, nancount:2, test_reccnt:0,featureCnt:8\n",
      "carno:8, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:9, totallen:168, nancount:32, test_reccnt:0,featureCnt:8\n",
      "carno:10, totallen:177, nancount:23, test_reccnt:0,featureCnt:8\n",
      "carno:11, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:12, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:14, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:15, totallen:44, nancount:156, test_reccnt:0,featureCnt:8\n",
      "carno:16, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:17, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:18, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:19, totallen:198, nancount:2, test_reccnt:0,featureCnt:8\n",
      "carno:20, totallen:175, nancount:25, test_reccnt:0,featureCnt:8\n",
      "carno:21, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:22, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:25, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:26, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:27, totallen:175, nancount:25, test_reccnt:0,featureCnt:8\n",
      "carno:28, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:33, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:34, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:41, totallen:196, nancount:4, test_reccnt:0,featureCnt:8\n",
      "carno:63, totallen:193, nancount:7, test_reccnt:0,featureCnt:8\n",
      "carno:67, totallen:157, nancount:43, test_reccnt:0,featureCnt:8\n",
      "carno:68, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:77, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:83, totallen:149, nancount:51, test_reccnt:0,featureCnt:8\n",
      "carno:91, totallen:87, nancount:113, test_reccnt:0,featureCnt:8\n",
      "carno:98, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "after ====event:Indy500-2015, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "carno:1, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:2, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:3, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:4, totallen:176, nancount:24, test_reccnt:0,featureCnt:8\n",
      "carno:5, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:6, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:7, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:9, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:10, totallen:151, nancount:49, test_reccnt:0,featureCnt:8\n",
      "carno:11, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:14, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:15, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:17, totallen:175, nancount:25, test_reccnt:0,featureCnt:8\n",
      "carno:18, totallen:116, nancount:84, test_reccnt:0,featureCnt:8\n",
      "carno:19, totallen:116, nancount:84, test_reccnt:0,featureCnt:8\n",
      "carno:20, totallen:112, nancount:88, test_reccnt:0,featureCnt:8\n",
      "carno:21, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:22, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:24, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:25, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:26, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:27, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:28, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:29, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:32, totallen:112, nancount:88, test_reccnt:0,featureCnt:8\n",
      "carno:41, totallen:175, nancount:25, test_reccnt:0,featureCnt:8\n",
      "carno:48, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:63, totallen:197, nancount:3, test_reccnt:0,featureCnt:8\n",
      "carno:83, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:88, totallen:62, nancount:138, test_reccnt:0,featureCnt:8\n",
      "carno:98, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "after ====event:Indy500-2016, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "carno:2, totallen:63, nancount:137, test_reccnt:0,featureCnt:8\n",
      "carno:3, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:4, totallen:100, nancount:100, test_reccnt:0,featureCnt:8\n",
      "carno:5, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:6, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:7, totallen:126, nancount:74, test_reccnt:0,featureCnt:8\n",
      "carno:8, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:9, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:10, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:11, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:12, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:14, totallen:163, nancount:37, test_reccnt:0,featureCnt:8\n",
      "carno:15, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:16, totallen:195, nancount:5, test_reccnt:0,featureCnt:8\n",
      "carno:18, totallen:115, nancount:85, test_reccnt:0,featureCnt:8\n",
      "carno:19, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:20, totallen:98, nancount:102, test_reccnt:0,featureCnt:8\n",
      "carno:21, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:22, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:24, totallen:93, nancount:107, test_reccnt:0,featureCnt:8\n",
      "carno:25, totallen:119, nancount:81, test_reccnt:0,featureCnt:8\n",
      "carno:26, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:27, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:28, totallen:198, nancount:2, test_reccnt:0,featureCnt:8\n",
      "carno:29, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:35, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:41, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:42, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:61, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:63, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:77, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:88, totallen:198, nancount:2, test_reccnt:0,featureCnt:8\n",
      "carno:98, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "after ====event:Indy500-2017, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "carno:1, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:2, totallen:186, nancount:14, test_reccnt:0,featureCnt:8\n",
      "carno:3, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:4, totallen:65, nancount:135, test_reccnt:0,featureCnt:8\n",
      "carno:5, totallen:183, nancount:17, test_reccnt:0,featureCnt:8\n",
      "carno:7, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:8, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:9, totallen:52, nancount:148, test_reccnt:0,featureCnt:8\n",
      "carno:10, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:11, totallen:194, nancount:6, test_reccnt:0,featureCnt:8\n",
      "carno:12, totallen:183, nancount:17, test_reccnt:0,featureCnt:8\n",
      "carno:14, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:15, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:16, totallen:183, nancount:17, test_reccnt:0,featureCnt:8\n",
      "carno:17, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:18, totallen:183, nancount:17, test_reccnt:0,featureCnt:8\n",
      "carno:19, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:20, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:21, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:22, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:24, totallen:125, nancount:75, test_reccnt:0,featureCnt:8\n",
      "carno:26, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:27, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:28, totallen:136, nancount:64, test_reccnt:0,featureCnt:8\n",
      "carno:29, totallen:179, nancount:21, test_reccnt:0,featureCnt:8\n",
      "carno:40, totallen:155, nancount:45, test_reccnt:0,featureCnt:8\n",
      "carno:44, totallen:118, nancount:82, test_reccnt:0,featureCnt:8\n",
      "carno:50, totallen:65, nancount:135, test_reccnt:0,featureCnt:8\n",
      "carno:63, totallen:199, nancount:1, test_reccnt:0,featureCnt:8\n",
      "carno:77, totallen:45, nancount:155, test_reccnt:0,featureCnt:8\n",
      "carno:83, totallen:166, nancount:34, test_reccnt:0,featureCnt:8\n",
      "carno:88, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "carno:98, totallen:200, nancount:0, test_reccnt:0,featureCnt:8\n",
      "after ====event:Indy500-2018, prediction_len=2,train_len=40, max_len=200, min_len=200,context_len=10\n",
      "carno:1, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:3, totallen:146, nancount:54, test_reccnt:134,featureCnt:8\n",
      "carno:4, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:6, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:7, totallen:193, nancount:7, test_reccnt:181,featureCnt:8\n",
      "carno:9, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:10, totallen:57, nancount:143, test_reccnt:45,featureCnt:8\n",
      "carno:12, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:13, totallen:67, nancount:133, test_reccnt:55,featureCnt:8\n",
      "carno:14, totallen:187, nancount:13, test_reccnt:175,featureCnt:8\n",
      "carno:15, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:17, totallen:199, nancount:1, test_reccnt:187,featureCnt:8\n",
      "carno:18, totallen:137, nancount:63, test_reccnt:125,featureCnt:8\n",
      "carno:19, totallen:199, nancount:1, test_reccnt:187,featureCnt:8\n",
      "carno:20, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:21, totallen:199, nancount:1, test_reccnt:187,featureCnt:8\n",
      "carno:22, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:23, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:24, totallen:154, nancount:46, test_reccnt:142,featureCnt:8\n",
      "carno:25, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:26, totallen:198, nancount:2, test_reccnt:186,featureCnt:8\n",
      "carno:27, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:28, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:29, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:30, totallen:46, nancount:154, test_reccnt:34,featureCnt:8\n",
      "carno:32, totallen:110, nancount:90, test_reccnt:98,featureCnt:8\n",
      "carno:33, totallen:46, nancount:154, test_reccnt:34,featureCnt:8\n",
      "carno:59, totallen:198, nancount:2, test_reccnt:186,featureCnt:8\n",
      "carno:60, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:64, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:66, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:88, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "carno:98, totallen:200, nancount:0, test_reccnt:188,featureCnt:8\n",
      "train len:161, test len:5340, totsl TsCnt:194, total ts len:34740\n"
     ]
    }
   ],
   "source": [
    "outdir = 'test/'\n",
    "outputRoot = outdir\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "_task_dir = f'{outdir}/'\n",
    "\n",
    "\n",
    "print('Load laptime and stage dataset:',outputRoot + LAPTIME_DATASET, outputRoot + STAGE_DATASET)\n",
    "with open(outputRoot + LAPTIME_DATASET, 'rb') as f:\n",
    "    global_carids, laptime_data = pickle.load(f, encoding='latin1') \n",
    "with open(outputRoot + STAGE_DATASET, 'rb') as f:\n",
    "    stagedata = pickle.load(f, encoding='latin1') \n",
    "\n",
    "\n",
    "#\n",
    "#dbname, train_ds, test_ds = makedbs()   \n",
    "#\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "dbname = _task_dir + f'gluontsdb-{dataset}-oracle-{ipstr}-all-all-f{freq}-t{prediction_length}-r{_test_event}-indy-{year}.pickle'\n",
    "\n",
    "\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "\n",
    "shift = 'shift'\n",
    "laptimedb = _task_dir + f'gluontsdb-{dataset}-oracle-{ipstr}-all-all-f{freq}-t{prediction_length}-r{_test_event}-indy-{year}-{shift}.pickle'    \n",
    "\n",
    "prepared_laptimedata = prepare_laptimedata(prediction_length, freq, test_event = _test_event,\n",
    "                       train_ratio=0, context_ratio = 0., shift_len = prediction_length)\n",
    "\n",
    "\n",
    "laptime_data = prepared_laptimedata\n",
    "prepared_laptimedata = prepare_laptimedata(prediction_length, freq, test_event = _test_event,\n",
    "                       train_ratio=0, context_ratio = 0., shift_len = prediction_length)\n",
    "\n",
    "\n",
    "train_ds, test_ds,_,_ = make_dataset_byevent(prepared_laptimedata, prediction_length,freq,\n",
    "                                     useeid=useeid, run_ts=_run_ts,\n",
    "                                    test_event=_test_event, log_transform =False,\n",
    "                                    context_ratio=0, train_ratio = 0, dorerank =True)\n",
    "\n",
    "\n",
    "with open(dbname, 'wb') as f:\n",
    "    savedata = [freq, prediction_length, cardinality, train_ds, test_ds]\n",
    "    pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(laptimedb, 'wb') as f:\n",
    "    pickle.dump(prepared_laptimedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestCode: add_leader_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[2, 5, 9, 6, 6, 3, 7, 7, 0, 2],\n",
    "        [0, 0, 1, 1, 0, 1, 1, 0, 1, 1]],\n",
    "        [[5, 8, 8, 6, 4, 4, 9, np.nan, np.nan, np.nan],\n",
    "        [1, 1, 1, 0, 1, 1, 1, 0, 0, 1]],\n",
    "       [[7, 1, 3, 2, 6, 9, 9, 4, 4, 9],\n",
    "        [1, 0, 0, 0, 0, 1, 0, 1, 0, 1]],\n",
    "       [[8, 4, 9, 1, 7, 0, 7, 2, np.nan, np.nan],\n",
    "        [0, 1, 0, 0, 0, 1, 0, 1, 1, 1]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results like this:\n",
    "\n",
    "    array([[[2., 5., 9., 6., 6., 3., 7., 7., 0., 2.],\n",
    "        [0., 0., 1., 1., 0., 1., 1., 0., 1., 1.],\n",
    "        [0., 1., 1., 0., 1., 1., 0., 2., 0., 0.]],\n",
    "\n",
    "       [[5., 8., 8., 6., 4., 4., 9., 3., 8., 7.],\n",
    "        [1., 1., 1., 0., 1., 1., 1., 0., 0., 1.],\n",
    "        [0., 1., 0., 1., 0., 2., 1., 1., 2., 2.]],\n",
    "\n",
    "       [[7., 1., 3., 2., 6., 9., 9., 4., 4., 9.],\n",
    "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 1.],\n",
    "        [1., 0., 0., 0., 1., 3., 2., 1., 2., 3.]],\n",
    "\n",
    "       [[8., 4., 9., 1., 7., 0., 7., 2., 1., 6.],\n",
    "        [0., 1., 0., 0., 0., 1., 0., 1., 1., 1.],\n",
    "        [2., 0., 2., 0., 1., 0., 1., 0., 1., 1.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = add_leader_cnt(a, 0, 1)\n",
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2 = add_allpit_cnt(mat1, rank_col=0, pit_col = 1)\n",
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_shift_feature(mat2, rank_col=0, shift_col = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TestCode: remove nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    " [ 0,  0,  0,  0,  0,  0,  0, np.nan, np.nan,  0],\n",
    " [ 0,  1,  0,  0,  0,  1,  0,  0,  0,  0],\n",
    " [ 0,  1,  0, np.nan, np.nan,  1,  0,  0,  0,  0],\n",
    " [ 0,  1,  0,  0,  0,  1,  0,  0,  0,  0],\n",
    " [ 0,  1,  0,  0,  0, np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    " [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.isnan(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[idx]=0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([FEATURE_STATUS,\n",
    "FEATURE_LEADER_PITCNT,\n",
    "FEATURE_TOTAL_PITCNT,\n",
    "FEATURE_SHIFT_TRACKSTATUS,\n",
    "FEATURE_SHIFT_LAPSTATUS,\n",
    "FEATURE_SHIFT_LEADER_PITCNT,\n",
    "FEATURE_SHIFT_TOTAL_PITCNT])\n",
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
