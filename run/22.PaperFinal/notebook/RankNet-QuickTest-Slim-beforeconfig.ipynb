{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuickTest Slim\n",
    "\n",
    "based on : RankNet-QuickTest-Joint\n",
    "\n",
    "    makedb laptime\n",
    "    makedb gluonts\n",
    "    train model\n",
    "    evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import random\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.deep_factor import DeepFactorEstimator\n",
    "from gluonts.model.deepstate import DeepStateEstimator\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.model.prophet import ProphetPredictor\n",
    "from gluonts.model.r_forecast import RForecastPredictor\n",
    "from gluonts.dataset.util import to_pandas\n",
    "\n",
    "from gluonts.distribution.neg_binomial import NegativeBinomialOutput\n",
    "from gluonts.distribution.student_t import StudentTOutput\n",
    "from gluonts.distribution.multivariate_gaussian import MultivariateGaussianOutput\n",
    "\n",
    "from indycar.model.NaivePredictor import NaivePredictor\n",
    "from indycar.model.deeparw import DeepARWeightEstimator\n",
    "\n",
    "#import indycar.model.stint_simulator_shortterm_pitmodel as stint\n",
    "import indycar.model.quicktest_simulator as stint\n",
    "\n",
    "# import all functions \n",
    "from indycar.model.quicktest_modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WorkRootDir = 'QuickTestOutput'\n",
    "#reference\n",
    "#configname = 'weighted-noinlap-nopitage-nocate-c60-drank'\n",
    "#configname = 'weighted-noinlap-S0LTYP0T-nocate-c60-drank-pitmodel'\n",
    "configname = 'weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle'\n",
    "configfile = f'{configname}.ini'\n",
    "\n",
    "if configfile != '':\n",
    "    config = configparser.RawConfigParser()\n",
    "    config.read(WorkRootDir + '/' + configfile)\n",
    "\n",
    "    #set them back\n",
    "    section = \"RankNet-QuickTest\"\n",
    "    \n",
    "    _savedata = config.getboolean(section, \"_savedata\")\n",
    "    _skip_overwrite = config.getboolean(section, \"_skip_overwrite\")\n",
    "    _inlap_status = config.getint(section, \"_inlap_status\") #0\n",
    "    _feature_mode = config.getint(section, \"_feature_mode\") #FEATURE_STATUS\n",
    "    _featureCnt = config.getint(section, \"_featureCnt\") #9\n",
    "    freq = config.get(section, \"freq\") #\"1min\"\n",
    "    _train_len = config.getint(section, \"_train_len\") #40\n",
    "    prediction_length = config.getint(section, \"prediction_length\") #2\n",
    "    context_ratio = config.getfloat(section, \"context_ratio\") #0.\n",
    "    context_length =  config.getint(section, \"context_length\") #40\n",
    "    \n",
    "    dataset= config.get(section, \"dataset\") #'rank'\n",
    "    epochs = config.getint(section, \"epochs\") #1000\n",
    "    gpuid = config.getint(section, \"gpuid\") #5\n",
    "    _use_weighted_model = config.getboolean(section, \"_use_weighted_model\")\n",
    "    trainmodel = config.get(section, \"trainmodel\") #'deepARW-Oracle' if _use_weighted_model else 'deepAR-Oracle'\n",
    "    \n",
    "    _use_cate_feature = config.getboolean(section, \"_use_cate_feature\")\n",
    "    \n",
    "    distroutput = config.get(section, \"distroutput\") #'student'\n",
    "    batch_size = config.getint(section, \"batch_size\") #32\n",
    "    loopcnt = config.getint(section, \"loopcnt\") #2\n",
    "    _test_event = config.get(section, \"_test_event\") #'Indy500-2018'\n",
    "    testmodel = config.get(section, \"testmodel\") #'oracle'\n",
    "    pitmodel = config.get(section, \"pitmodel\") #'oracle'\n",
    "    year = config.get(section, \"year\") #'2018'\n",
    "    \n",
    "    contextlen = context_length\n",
    "    use_feat_static = _use_cate_feature \n",
    "\n",
    "    #config1 = get_config()\n",
    "    \n",
    "else:\n",
    "    print('Warning, please use config file')\n",
    "    sys.exit(0)\n",
    "    \n",
    "    #\n",
    "    # global settings\n",
    "    #\n",
    "    #_savedata = False\n",
    "    _savedata = True\n",
    "    _skip_overwrite = True\n",
    "\n",
    "    #inlap status = \n",
    "    # 0 , no inlap\n",
    "    # 1 , set previous lap\n",
    "    # 2 , set the next lap\n",
    "    _inlap_status = 0\n",
    "\n",
    "    #\n",
    "    # featuremode in [FEATURE_STATUS, FEATURE_PITAGE]:\n",
    "    #\n",
    "    _feature_mode = FEATURE_LEADERPITCNT\n",
    "    _featureCnt = 9\n",
    "\n",
    "    #\n",
    "    # training parameters\n",
    "    #\n",
    "    freq = \"1min\"\n",
    "    _train_len = 60\n",
    "    prediction_length = 2\n",
    "\n",
    "    context_ratio = 0.\n",
    "    context_length =  60\n",
    "    contextlen = context_length\n",
    "\n",
    "    dataset='rank'\n",
    "    epochs = 1000\n",
    "    #epochs = 10\n",
    "    gpuid = 5\n",
    "\n",
    "    #'deepAR-Oracle','deepARW-Oracle'\n",
    "    _use_weighted_model = True\n",
    "    trainmodel = 'deepARW-Oracle' if _use_weighted_model else 'deepAR-Oracle'\n",
    "\n",
    "    _use_cate_feature = False\n",
    "    use_feat_static = _use_cate_feature \n",
    "\n",
    "    distroutput = 'student'\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "    #\n",
    "    # test parameters\n",
    "    #\n",
    "    loopcnt = 2\n",
    "    _test_event = 'Indy500-2018'\n",
    "    testmodel = 'oracle'\n",
    "    pitmodel = 'oracle'\n",
    "    year = '2018'\n",
    "    \n",
    "    #config2 = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current configfile: weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle.ini\n",
      "FEATURE_STATUS FEATURE_LEADER_PITCNT FEATURE_TOTAL_PITCNT FEATURE_SHIFT_TRACKSTATUS FEATURE_SHIFT_LAPSTATUS FEATURE_SHIFT_TOTAL_PITCNT\n",
      "feature_mode: 378 S0LTYP0T\n",
      "testmodel: standard\n",
      "pitmodel: oracle\n",
      "year: 2019\n",
      "test_event: Indy500-2019\n"
     ]
    }
   ],
   "source": [
    "# debug test\n",
    "_skip_overwrite = False\n",
    "\n",
    "# new added parameters\n",
    "_test_train_len = 40\n",
    "_joint_train = False\n",
    "_pitmodel_bias = 0\n",
    "\n",
    "_test_event = 'Indy500-2019'\n",
    "year = '2019'\n",
    "\n",
    "#shortterm, stint\n",
    "#_forecast_mode = 'stint'\n",
    "_forecast_mode = 'shortterm'\n",
    "\n",
    "# bias of the pitmodel\n",
    "#_pitmodel_bias = 4\n",
    "\n",
    "#train model: [deepARW-Oracle, deepAR]\n",
    "\n",
    "# test the standard deepAR model training and testing\n",
    "\n",
    "# DeepAR\n",
    "trainmodel = 'deepAR'\n",
    "testmodel = 'standard'\n",
    "\n",
    "# Joint \n",
    "#trainmodel = 'deepAR-multi'\n",
    "#testmodel = 'joint'\n",
    "#_joint_train = True\n",
    "#loopcnt = 2\n",
    "\n",
    "if testmodel == 'pitmodel':\n",
    "    testmodel = 'pitmodel%s'%(_pitmodel_bias if _pitmodel_bias!=0 else '')\n",
    "\n",
    "loopcnt = 2    \n",
    "    \n",
    "#featurestr = {FEATURE_STATUS:'nopitage',FEATURE_PITAGE:'pitage',FEATURE_LEADERPITCNT:'leaderpitcnt'}\n",
    "#cur_featurestr = featurestr[_feature_mode]\n",
    "print('current configfile:', configfile)\n",
    "cur_featurestr = decode_feature_mode(_feature_mode)\n",
    "print('feature_mode:', _feature_mode, cur_featurestr)\n",
    "print('testmodel:', testmodel)\n",
    "print('pitmodel:', pitmodel)\n",
    "print('year:', year)\n",
    "print('test_event:', _test_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# string map\n",
    "#\n",
    "inlapstr = {0:'noinlap',1:'inlap',2:'outlap'}\n",
    "weightstr = {True:'weighted',False:'noweighted'}\n",
    "catestr = {True:'cate',False:'nocate'}\n",
    "\n",
    "#\n",
    "# input data parameters\n",
    "#\n",
    "years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "events = [f'Indy500-{x}' for x in years]\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "dbid = f'Indy500_{years[0]}_{years[-1]}_v{_featureCnt}_p{_inlap_status}'\n",
    "_dataset_id = '%s-%s'%(inlapstr[_inlap_status], cur_featurestr)\n",
    "\n",
    "\n",
    "#\n",
    "# internal parameters\n",
    "#\n",
    "distr_outputs ={'student':StudentTOutput(),\n",
    "                'negbin':NegativeBinomialOutput()\n",
    "                }\n",
    "distr_output = distr_outputs[distroutput]\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "experimentid = f'{weightstr[_use_weighted_model]}-{inlapstr[_inlap_status]}-{cur_featurestr}-{catestr[_use_cate_feature]}-c{context_length}'\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "outputRoot = f\"{WorkRootDir}/{experimentid}/\"\n",
    "\n",
    "\n",
    "# standard output file names\n",
    "LAPTIME_DATASET = f'{outputRoot}/laptime_rank_timediff_pit-oracle-{dbid}.pickle' \n",
    "STAGE_DATASET = f'{outputRoot}/stagedata-{dbid}.pickle' \n",
    "# year related\n",
    "SIMULATION_OUTFILE = f'{outputRoot}/{_test_event}/{_forecast_mode}-dfout-{trainmodel}-indy500-{dataset}-{inlapstr[_inlap_status]}-{cur_featurestr}-{testmodel}-l{loopcnt}-alldata.pickle'\n",
    "EVALUATION_RESULT_DF = f'{outputRoot}/{_test_event}/{_forecast_mode}-evaluation_result_d{dataset}_m{testmodel}.csv'\n",
    "LONG_FORECASTING_DFS = f'{outputRoot}/{_test_event}/{_forecast_mode}-long_forecasting_dfs_d{dataset}_m{testmodel}.pickle'\n",
    "FORECAST_FIGS_DIR = f'{outputRoot}/{_test_event}/{_forecast_mode}-forecast-figs-d{dataset}_m{testmodel}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. make laptime dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of completed cars: 19\n",
      "completed cars: [11 26  1 25 19  3  2 77 83 20 22  8 14  9 18 55 78  5 12]\n",
      "cars: {1, 2, 3, 5, 8, 9, 11, 12, 77, 14, 78, 18, 19, 20, 83, 22, 55, 25, 26}\n",
      "#cars= 19\n",
      "count of completed cars: 19\n",
      "completed cars: [11 26  1 25 19  3  2 77 83 20 22  8 14  9 18 55 78  5 12]\n",
      "cars: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 19, 20, 21, 22, 25, 26, 27, 41, 55, 60, 63, 77, 78, 81, 83, 91, 98}\n",
      "#cars= 33\n",
      "Indy500-2013: carno=33, lapnum=201\n",
      "count of completed cars: 20\n",
      "completed cars: [28  3 25 34  2 26 11 12 22 21 16 77 68  5 17 33 18  8 14 98]\n",
      "cars: {33, 2, 3, 34, 5, 68, 98, 8, 11, 12, 77, 14, 16, 17, 18, 21, 22, 25, 26, 28}\n",
      "#cars= 20\n",
      "count of completed cars: 20\n",
      "completed cars: [28  3 25 34  2 26 11 12 22 21 16 77 68  5 17 33 18  8 14 98]\n",
      "cars: {2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 27, 28, 33, 34, 41, 63, 67, 68, 77, 83, 91, 98}\n",
      "#cars= 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  uni_ds['rank_diff'][mask] = 0\n",
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  uni_ds['time_diff'][mask] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indy500-2014: carno=33, lapnum=201\n",
      "count of completed cars: 20\n",
      "completed cars: [ 2  1 83  9 15 27  3  6 21 22 11  5 14 24 28 98 48  7 29 26]\n",
      "cars: {1, 2, 3, 98, 5, 6, 7, 9, 11, 14, 15, 48, 83, 21, 22, 24, 26, 27, 28, 29}\n",
      "#cars= 20\n",
      "count of completed cars: 20\n",
      "completed cars: [ 2  1 83  9 15 27  3  6 21 22 11  5 14 24 28 98 48  7 29 26]\n",
      "cars: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 32, 41, 43, 48, 63, 83, 88, 98}\n",
      "#cars= 33\n",
      "Indy500-2015: carno=33, lapnum=201\n",
      "count of completed cars: 17\n",
      "completed cars: [98 26 21 10 42  6  5  9 11 12  3 77 27 15  8 41 35]\n",
      "cars: {98, 3, 35, 5, 6, 8, 9, 10, 11, 12, 41, 42, 15, 77, 21, 26, 27}\n",
      "#cars= 17\n",
      "count of completed cars: 17\n",
      "completed cars: [98 26 21 10 42  6  5  9 11 12  3 77 27 15  8 41 35]\n",
      "cars: {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 35, 41, 42, 61, 63, 77, 88, 98}\n",
      "#cars= 33\n",
      "Indy500-2016: carno=33, lapnum=201\n",
      "count of completed cars: 16\n",
      "completed cars: [26  3 19  8 10 22 98 27 88 14 20 15  7  1 17 21]\n",
      "cars: {1, 98, 3, 7, 8, 10, 14, 15, 17, 19, 20, 21, 22, 88, 26, 27}\n",
      "#cars= 16\n",
      "count of completed cars: 16\n",
      "completed cars: [26  3 19  8 10 22 98 27 88 14 20 15  7  1 17 21]\n",
      "cars: {1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 40, 44, 50, 63, 77, 83, 88, 98}\n",
      "#cars= 33\n",
      "Indy500-2017: carno=33, lapnum=201\n",
      "count of completed cars: 18\n",
      "completed cars: [12 20  9 27 28 22 29  1  6 15 66 98  4 88 25 60 64 23]\n",
      "cars: {64, 1, 66, 98, 4, 6, 9, 12, 60, 15, 20, 22, 23, 88, 25, 27, 28, 29}\n",
      "#cars= 18\n",
      "count of completed cars: 18\n",
      "completed cars: [12 20  9 27 28 22 29  1  6 15 66 98  4 88 25 60 64 23]\n",
      "cars: {1, 3, 4, 6, 7, 9, 10, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 59, 60, 64, 66, 88, 98}\n",
      "#cars= 33\n",
      "Indy500-2018: carno=33, lapnum=201\n",
      "count of completed cars: 17\n",
      "completed cars: [22 27 30  2 12 20 19 28 14 25  5 33 63 21  4 39  9]\n",
      "cars: {33, 2, 4, 5, 39, 9, 12, 14, 19, 20, 21, 22, 25, 27, 28, 30, 63}\n",
      "#cars= 17\n",
      "count of completed cars: 17\n",
      "completed cars: [22 27 30  2 12 20 19 28 14 25  5 33 63 21  4 39  9]\n",
      "cars: {2, 3, 4, 5, 7, 9, 10, 12, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 33, 39, 42, 48, 60, 63, 77, 81, 88, 98}\n",
      "#cars= 33\n",
      "Indy500-2019: carno=33, lapnum=201\n",
      "start event: Indy500-2013\n",
      "event=Indy500-2013, records=(33, 9, 200)\n",
      "start event: Indy500-2014\n",
      "event=Indy500-2014, records=(33, 9, 200)\n",
      "start event: Indy500-2015\n",
      "event=Indy500-2015, records=(33, 9, 200)\n",
      "start event: Indy500-2016\n",
      "event=Indy500-2016, records=(33, 9, 200)\n",
      "start event: Indy500-2017\n",
      "event=Indy500-2017, records=(33, 9, 200)\n",
      "start event: Indy500-2018\n",
      "event=Indy500-2018, records=(33, 9, 200)\n",
      "start event: Indy500-2019\n",
      "event=Indy500-2019, records=(33, 9, 200)\n",
      "QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60//laptime_rank_timediff_pit-oracle-Indy500_2013_2019_v9_p0.pickle\n",
      "QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60//stagedata-Indy500_2013_2019_v9_p0.pickle\n"
     ]
    }
   ],
   "source": [
    "stagedata = {}\n",
    "global_carids = {}\n",
    "os.makedirs(outputRoot, exist_ok=True)\n",
    "os.makedirs(f'{outputRoot}/{_test_event}', exist_ok=True)\n",
    "\n",
    "#check the dest files first\n",
    "if _skip_overwrite and os.path.exists(LAPTIME_DATASET) and os.path.exists(STAGE_DATASET):\n",
    "        #\n",
    "        # load data\n",
    "        #\n",
    "        print('Load laptime and stage dataset:',LAPTIME_DATASET, STAGE_DATASET)\n",
    "        with open(LAPTIME_DATASET, 'rb') as f:\n",
    "            global_carids, laptime_data = pickle.load(f, encoding='latin1') \n",
    "        with open(STAGE_DATASET, 'rb') as f:\n",
    "            stagedata = pickle.load(f, encoding='latin1') \n",
    "    \n",
    "else:    \n",
    "    cur_carid = 0\n",
    "    for event in events:\n",
    "        #dataid = f'{event}-{year}'\n",
    "        #alldata, rankdata, acldata, flagdata\n",
    "        stagedata[event] = load_data(event)\n",
    "\n",
    "        alldata, rankdata, acldata, flagdata = stagedata[event]\n",
    "        carlist = set(acldata['car_number'])\n",
    "        laplist = set(acldata['completed_laps'])\n",
    "        print('%s: carno=%d, lapnum=%d'%(event, len(carlist), len(laplist)))\n",
    "\n",
    "        #build the carid map\n",
    "        for car in carlist:\n",
    "            if car not in global_carids:\n",
    "                global_carids[car] = cur_carid\n",
    "                cur_carid += 1\n",
    "\n",
    "    laptime_data = get_laptime_dataset(stagedata,events_id, inlap_status = _inlap_status)\n",
    "\n",
    "    if _savedata:\n",
    "        import pickle\n",
    "        #stintdf.to_csv('laptime-%s.csv'%year)\n",
    "        #savefile = outputRoot + f'laptime_rank_timediff_pit-oracle-{dbid}.pickle' \n",
    "        savefile = LAPTIME_DATASET\n",
    "        print(savefile)\n",
    "        with open(savefile, 'wb') as f:\n",
    "            #pack [global_carids, laptime_data]\n",
    "            savedata = [global_carids, laptime_data]\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        #savefile = outputRoot + f'stagedata-{dbid}.pickle' \n",
    "        savefile = STAGE_DATASET\n",
    "        print(savefile)\n",
    "        with open(savefile, 'wb') as f:\n",
    "            #pack [global_carids, laptime_data]\n",
    "            savedata = stagedata\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. make gluonts db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_laptimedata shift len: 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_train_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-198dee4f484f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     prepared_laptimedata = prepare_laptimedata(laptime_data, events_id,\n\u001b[1;32m     50\u001b[0m                            \u001b[0mprediction_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_test_event\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                            train_ratio=0, context_ratio = 0.,shift_len = prediction_length)\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     train_ds, test_ds,_,_ = make_dataset_byevent(prepared_laptimedata, events_id,\n",
      "\u001b[0;32m~/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py\u001b[0m in \u001b[0;36mprepare_laptimedata\u001b[0;34m(laptime_data, events_id, prediction_length, freq, test_event, train_ratio, context_ratio, shift_len)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_len\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;31m#use global train_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0mtrain_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_test_train_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext_ratio\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_train_len' is not defined"
     ]
    }
   ],
   "source": [
    "outdir = outputRoot + _dataset_id\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "if dataset == 'laptime':\n",
    "    subdir = 'laptime-indy500'\n",
    "    os.makedirs(f'{outdir}/{subdir}', exist_ok=True)\n",
    "    _run_ts = COL_LAPTIME\n",
    "elif dataset == 'timediff':\n",
    "    subdir = 'timediff-indy500'\n",
    "    os.makedirs(f'{outdir}/{subdir}', exist_ok=True)\n",
    "    _run_ts = COL_TIMEDIFF\n",
    "elif dataset == 'rank':\n",
    "    subdir = 'rank-indy500'\n",
    "    os.makedirs(f'{outdir}/{subdir}', exist_ok=True)\n",
    "    _run_ts = COL_RANK\n",
    "else:\n",
    "    print('error, dataset not support: ', dataset)\n",
    "    \n",
    "_task_dir = f'{outdir}/{subdir}/'\n",
    "\n",
    "#\n",
    "#dbname, train_ds, test_ds = makedbs()   \n",
    "#\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "jointstr = '-joint' if _joint_train else ''\n",
    "\n",
    "dbname = _task_dir + f'gluontsdb-{dataset}-oracle-{ipstr}-all-all-f{freq}-t{prediction_length}-r{_test_event}-indy-{year}{jointstr}.pickle'\n",
    "laptimedb = _task_dir + f'gluontsdb-{dataset}-oracle-{ipstr}-all-all-f{freq}-t{prediction_length}-r{_test_event}-indy-{year}-newlaptimedata.pickle'\n",
    "\n",
    "#check the dest files first\n",
    "if _skip_overwrite and os.path.exists(dbname) and os.path.exists(laptimedb):\n",
    "        print('Load Gluonts Dataset:',dbname)\n",
    "        with open(dbname, 'rb') as f:\n",
    "            freq, prediction_length, cardinality, train_ds, test_ds = pickle.load(f, encoding='latin1') \n",
    "        print('.......loaded data, freq=', freq, 'prediction_length=', prediction_length)\n",
    "        print('Load New Laptime Dataset:',laptimedb)\n",
    "        with open(laptimedb, 'rb') as f:\n",
    "            prepared_laptimedata = pickle.load(f, encoding='latin1') \n",
    "        \n",
    "else:\n",
    "    if useeid:\n",
    "        cardinality = [len(global_carids), len(laptime_data)]\n",
    "    else:\n",
    "        cardinality = [len(global_carids)]\n",
    "\n",
    "    prepared_laptimedata = prepare_laptimedata(laptime_data, events_id,\n",
    "                           prediction_length, freq, test_event = _test_event,\n",
    "                           train_ratio=0, context_ratio = 0.,shift_len = prediction_length)\n",
    "\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(prepared_laptimedata, events_id,\n",
    "                                        prediction_length,freq,\n",
    "                                         useeid=useeid, run_ts=_run_ts,\n",
    "                                        test_event=_test_event, log_transform =False,\n",
    "                                        context_ratio=0, train_ratio = 0, joint_train = _joint_train)    \n",
    "\n",
    "\n",
    "    if _savedata:\n",
    "        print('Save Gluonts Dataset:',dbname)\n",
    "        with open(dbname, 'wb') as f:\n",
    "            savedata = [freq, prediction_length, cardinality, train_ds, test_ds]\n",
    "            pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Save preprocessed laptime Dataset:',laptimedb)\n",
    "        with open(laptimedb, 'wb') as f:\n",
    "            pickle.dump(prepared_laptimedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id='oracle'\n",
    "run=1\n",
    "runid=f'{trainmodel}-{dataset}-all-indy-f1min-t{prediction_length}-e{epochs}-r{run}_{id}_t{prediction_length}'\n",
    "modelfile = _task_dir + runid\n",
    "\n",
    "if _skip_overwrite and os.path.exists(modelfile):\n",
    "    print('Model checkpoint found at:',modelfile)\n",
    "\n",
    "else:\n",
    "    #get target dim\n",
    "    entry = next(iter(train_ds))\n",
    "    target_dim = entry['target'].shape\n",
    "    target_dim = target_dim[0] if len(target_dim) > 1 else 1\n",
    "    print('target_dim:%s', target_dim)\n",
    "\n",
    "    estimator = init_estimator(trainmodel, gpuid, \n",
    "            epochs, batch_size,target_dim, distr_output = distr_output,use_feat_static = use_feat_static)\n",
    "\n",
    "    predictor = estimator.train(train_ds)\n",
    "\n",
    "    if _savedata:\n",
    "        os.makedirs(modelfile, exist_ok=True)\n",
    "\n",
    "        print('Start to save the model to %s', modelfile)\n",
    "        predictor.serialize(Path(modelfile))\n",
    "        print('End of saving the model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lapmode = _inlap_status\n",
    "fmode = _feature_mode\n",
    "runts = dataset\n",
    "mid = f'{testmodel}-%s-%s-%s-%s'%(runts, year, inlapstr[lapmode], cur_featurestr)\n",
    "datasetid = outputRoot + _dataset_id\n",
    "\n",
    "if _skip_overwrite and os.path.exists(SIMULATION_OUTFILE):\n",
    "    print('Load Simulation Results:',SIMULATION_OUTFILE)\n",
    "    with open(SIMULATION_OUTFILE, 'rb') as f:\n",
    "        dfs,acc,ret,pret = pickle.load(f, encoding='latin1') \n",
    "    print('.......loaded data, ret keys=', ret.keys())\n",
    "    \n",
    "    \n",
    "    # init the stint module\n",
    "    #\n",
    "    # in test mode, set all train_len = 40 to unify the evaluation results\n",
    "    #\n",
    "    init_simulation(datasetid, _test_event, 'rank',stint.COL_RANK,'rank',prediction_length, \n",
    "                    pitmodel=pitmodel, inlapmode=lapmode,featuremode =fmode,\n",
    "                    train_len = _test_train_len, pitmodel_bias= _pitmodel_bias)    \n",
    "\n",
    "else:\n",
    "    #run simulation\n",
    "    acc, ret, pret = {}, {}, {}\n",
    "\n",
    "    #lapmode = _inlap_status\n",
    "    #fmode = _feature_mode\n",
    "    #runts = dataset\n",
    "    #mid = f'{testmodel}-%s-%s-%s-%s'%(runts, year, inlapstr[lapmode], featurestr[fmode])\n",
    "\n",
    "    if runts == 'rank':\n",
    "        acc[mid], ret[mid] = simulation(datasetid, _test_event, \n",
    "                    'rank',stint.COL_RANK,'rank',\n",
    "                   prediction_length, stint.MODE_ORACLE,loopcnt, \n",
    "                      pitmodel=pitmodel, model=testmodel, inlapmode=lapmode,featuremode =fmode,\n",
    "                    train_len = _test_train_len, forecastmode = _forecast_mode, joint_train = _joint_train,\n",
    "                    pitmodel_bias= _pitmodel_bias, prepared_laptimedata = prepared_laptimedata,\n",
    "                    epochs = epochs)\n",
    "    else:\n",
    "        acc[mid], ret[mid] = simulation(datasetid, _test_event, \n",
    "                    'timediff',stint.COL_TIMEDIFF,'timediff2rank',\n",
    "                   prediction_length, stint.MODE_ORACLE,loopcnt, \n",
    "                      pitmodel=pitmodel, model=testmodel, inlapmode=lapmode,featuremode =fmode,\n",
    "                    train_len = _test_train_len, forecastmode = _forecast_mode, joint_train = _joint_train,\n",
    "                    pitmodel_bias= _pitmodel_bias, prepared_laptimedata = prepared_laptimedata,\n",
    "                    epochs = epochs)\n",
    "\n",
    "    if _forecast_mode == 'shortterm':\n",
    "        allsamples, alltss = get_allsamples(ret[mid], year=year)\n",
    "        _, pret[mid]= prisk_direct_bysamples(allsamples, alltss)\n",
    "        print(pret[mid])\n",
    "    \n",
    "\n",
    "    dfs={}\n",
    "\n",
    "    mode=1\n",
    "    df = get_alldf_mode(ret[mid], year=year,mode=mode, forecast_mode = _forecast_mode)\n",
    "    name = '%s_%s'%(testmodel, 'mean' if mode==1 else ('mode' if mode==0 else 'median'))\n",
    "    if year not in dfs:\n",
    "        dfs[year] = {}\n",
    "    dfs[year][name] = df\n",
    "\n",
    "    _trim = 0\n",
    "    _include_final = True\n",
    "    _include_stintlen = True\n",
    "    include_str = '1' if _include_final else '0'\n",
    "    stint_str = '1' if _include_stintlen else ''            \n",
    "    #simulation_outfile=outputRoot + f'shortterm-dfout-oracle-indy500-{dataset}-{inlapstr[_inlap_status]}-{featurestr[_feature_mode]}-2018-oracle-l{loopcnt}-alldata-weighted.pickle'\n",
    "\n",
    "    with open(SIMULATION_OUTFILE, 'wb') as f:\n",
    "        savedata = [dfs,acc,ret,pret]\n",
    "        pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#alias\n",
    "ranknetdf = dfs   \n",
    "ranknet_ret = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _skip_overwrite and os.path.exists(EVALUATION_RESULT_DF):\n",
    "    print('Load Evaluation Results:',EVALUATION_RESULT_DF)\n",
    "    oracle_eval_result = pd.read_csv(EVALUATION_RESULT_DF)\n",
    "\n",
    "else:    \n",
    "    ##-------------------------------------------------------------------------------\n",
    "    if _forecast_mode == 'shortterm':\n",
    "\n",
    "        # get pit laps, pit-covered-laps\n",
    "        # pitdata[year] = [pitlaps, pitcoveredlaps]\n",
    "        with open('pitcoveredlaps-g1.pickle', 'rb') as f:\n",
    "            # The protocol version used is detected automatically, so we do not\n",
    "            # have to specify it.\n",
    "            pitdata = pickle.load(f, encoding='latin1') \n",
    "\n",
    "        #\n",
    "        # Model,SignAcc,MAE,50-Risk,90-Risk\n",
    "        # \n",
    "        cols = ['Year','Model','ExpID','laptype','Top1Acc','MAE','50-Risk','90-Risk']\n",
    "        plen = prediction_length\n",
    "        usemeanstr='mean'\n",
    "\n",
    "        #load data\n",
    "        # dfs,acc,ret,pret\n",
    "\n",
    "        retdata = []\n",
    "\n",
    "        #oracle\n",
    "        dfx = ret[mid]\n",
    "        allsamples, alltss = get_allsamples(dfx, year=year)\n",
    "        #_, pret[mid]= prisk_direct_bysamples(ret[mid][0][1], ret[mid][0][2])\n",
    "        _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)\n",
    "\n",
    "        dfout = do_rerank(ranknetdf[year][f'{testmodel}_mean'])\n",
    "        accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "        #fsamples, ftss = runs2samples_ex(ranknet_ret[f'oracle-RANK-{year}-inlap-nopitage'],[])\n",
    "        #_, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        retdata.append([year,f'{testmodel}',configname,'all', accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "        for laptype in ['normal','pit']:\n",
    "            # select the set\n",
    "            pitcoveredlaps = pitdata[year][1]\n",
    "            normallaps = set([x for x in range(1,201)]) - pitcoveredlaps\n",
    "\n",
    "            if laptype == 'normal':\n",
    "                sellaps = normallaps\n",
    "                clearlaps = pitcoveredlaps\n",
    "            else:\n",
    "                sellaps = pitcoveredlaps\n",
    "                clearlaps = normallaps\n",
    "\n",
    "\n",
    "            # pitcoveredlaps start idx = 1\n",
    "            startlaps = [x-plen-1 for x in sellaps]\n",
    "            #sellapidx = np.array([x-1 for x in sellaps])\n",
    "            clearidx = np.array([x-1 for x in clearlaps])\n",
    "            print('sellaps:', len(sellaps), 'clearlaps:',len(clearlaps))\n",
    "\n",
    "            #oracle\n",
    "            #outfile=f'shortterm-dfout-ranknet-indy500-rank-inlap-nopitage-20182019-oracle-l10-alldata-weighted.pickle'\n",
    "            #_all = load_dfout_all(outfile)[0]\n",
    "            #ranknetdf, acc, ret, pret = _all[0],_all[1],_all[2],_all[3]\n",
    "\n",
    "            dfout = do_rerank(ranknetdf[year][f'{testmodel}_mean'])\n",
    "\n",
    "            allsamples, alltss = get_allsamples(dfx, year=year)\n",
    "\n",
    "\n",
    "            allsamples, alltss = clear_samples(allsamples, alltss,clearidx)\n",
    "\n",
    "            _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)\n",
    "\n",
    "            dfout = dfout[dfout['startlap'].isin(startlaps)]\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "            print(year, laptype,f'RankNet-{testmodel}',accret[0], accret[1], prisk_vals[1], prisk_vals[2])\n",
    "            retdata.append([year, f'{testmodel}',configname,laptype, accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "            \n",
    "    ##-------------------------------------------------------------------------------\n",
    "    elif _forecast_mode == 'stint':\n",
    "        if testmodel == 'oracle':\n",
    "            datafile=f'stint-dfout-mlmodels-indy500-tr2013_2017-te2018_2019-end1-oracle-t0-tuned.pickle'\n",
    "        else:\n",
    "            datafile=f'stint-dfout-mlmodels-indy500-tr2013_2017-te2018_2019-end1-normal-t0-tuned.pickle'\n",
    "        #preddf = load_dfout(outfile)\n",
    "        with open(datafile, 'rb') as f:\n",
    "            preddf = pickle.load(f, encoding='latin1')[0] \n",
    "        #preddf_oracle = load_dfout(outfile)\n",
    "        ranknet_ret = ret \n",
    "\n",
    "        errlist = {}\n",
    "        errcnt, errlist[year] = cmp_df(ranknetdf[year][f'{testmodel}_mean'], preddf[year]['lasso'])\n",
    "        \n",
    "        retdata = []\n",
    "        #\n",
    "        # Model,SignAcc,MAE,50-Risk,90-Risk\n",
    "        # \n",
    "        cols = ['Year','Model','ExpID','laptype','SignAcc','MAE','50-Risk','90-Risk']\n",
    "        models = {'currank':'CurRank','rf':'RandomForest','svr_lin':'SVM','xgb':'XGBoost'}\n",
    "\n",
    "        for clf in ['currank','rf','svr_lin','xgb']:\n",
    "            print('year:',year,'clf:',clf)\n",
    "            dfout, accret = eval_sync(preddf[year][clf],errlist[year])\n",
    "            fsamples, ftss = df2samples_ex(dfout)\n",
    "            _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "\n",
    "            retdata.append([year,models[clf],configname,'all', accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "            \n",
    "        #ml models -oracle\n",
    "        #for clf in ['rf','svr_lin','xgb']:\n",
    "        #    print('year:',year,'clf:',clf)\n",
    "        #    dfout, accret = eval_sync(preddf_oracle[year][clf],errlist[year])\n",
    "        #    fsamples, ftss = df2samples(dfout)\n",
    "        #    _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        #    retdata.append([year,models[clf]+'-Oracle',configname,'all',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "        dfout, accret = eval_sync(ranknetdf[year][f'{testmodel}_mean'], errlist[year],force2int=True)\n",
    "        #fsamples, ftss = df2samples(dfout)\n",
    "        fsamples, ftss = runs2samples(ranknet_ret[mid],errlist[f'{year}'])\n",
    "        _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        retdata.append([year,f'{testmodel}',configname,'all',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "        #dfout, accret = eval_sync(ranknetdf[year]['oracle_mean'], errlist[year],force2int=True)\n",
    "        ##fsamples, ftss = df2samples(dfout)\n",
    "        #fsamples, ftss = runs2samples(ranknet_ret[f'oracle-TIMEDIFF-{year}-noinlap-nopitage'],errlist[f'{year}'])\n",
    "        #_, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        #retdata.append([year,'RankNet-Oracle',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "    oracle_eval_result = pd.DataFrame(data=retdata, columns=cols)\n",
    "    if _savedata:\n",
    "        oracle_eval_result.to_csv(EVALUATION_RESULT_DF)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Draw forecasting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _forecast_mode == 'shortterm' and _joint_train == False:\n",
    "    if _skip_overwrite and os.path.exists(LONG_FORECASTING_DFS):\n",
    "        fname = LONG_FORECASTING_DFS\n",
    "        print('Load Long Forecasting Data:',fname)\n",
    "        with open(fname, 'rb') as f:\n",
    "            alldata = pickle.load(f, encoding='latin1') \n",
    "        print('.......loaded data, alldata keys=', alldata.keys())\n",
    "\n",
    "    else:    \n",
    "\n",
    "        oracle_ret = ret    \n",
    "        mid = f'{testmodel}-%s-%s-%s-%s'%(runts, year, inlapstr[lapmode], cur_featurestr)\n",
    "        print('eval mid:', mid, f'{testmodel}_ret keys:', ret.keys())\n",
    "\n",
    "        ## init predictor\n",
    "        _predictor =  NaivePredictor(freq= freq, prediction_length = prediction_length)\n",
    "\n",
    "        oracle_dfout = do_rerank(dfs[year][f'{testmodel}_mean'])\n",
    "        carlist = set(list(oracle_dfout.carno.values))\n",
    "        carlist = [int(x) for x in carlist]\n",
    "        print('carlist:', carlist,'len:',len(carlist))\n",
    "\n",
    "        #carlist = [13, 7, 3, 12]\n",
    "        #carlist = [13]    \n",
    "\n",
    "        retdata = {}\n",
    "        for carno in carlist:\n",
    "            print(\"*\"*40)\n",
    "            print('Run models for carno=', carno)\n",
    "            # create the test_ds first\n",
    "            test_cars = [carno]\n",
    "\n",
    "            #train_ds, test_ds, trainset, testset = stint.make_dataset_byevent(events_id[_test_event], \n",
    "            #                                 prediction_length,freq, \n",
    "            #                                 oracle_mode=stint.MODE_ORACLE,\n",
    "            #                                 run_ts = _run_ts,\n",
    "            #                                 test_event = _test_event,\n",
    "            #                                 test_cars=test_cars,\n",
    "            #                                 half_moving_win = 0,\n",
    "            #                                 train_ratio = 0.01)\n",
    "\n",
    "            train_ds, test_ds, trainset, testset = make_dataset_byevent(prepared_laptimedata, prediction_length,freq,\n",
    "                                             useeid=useeid, run_ts=_run_ts,\n",
    "                                            test_event=_test_event, log_transform =False,\n",
    "                                            context_ratio=0, train_ratio = 0,\n",
    "                                            joint_train = _joint_train,\n",
    "                                            test_cars = test_cars)    \n",
    "\n",
    "\n",
    "            if (len(testset) <= 10 + prediction_length):\n",
    "                print('ts too short, skip ', len(testset))\n",
    "                continue\n",
    "\n",
    "            #by first run samples\n",
    "            samples = oracle_ret[mid][0][1][test_cars[0]]\n",
    "            tss  = oracle_ret[mid][0][2][test_cars[0]]\n",
    "            target_oracle1, tss_oracle1 = long_predict_bysamples('1run-samples', samples, tss)\n",
    "\n",
    "            #by first run output df(_use_mean = true, already reranked)\n",
    "            df = oracle_ret[mid][0][0]\n",
    "            dfin_oracle = df[df['carno']==test_cars[0]]\n",
    "            target_oracle2, tss_oracle2 = long_predict_bydf(f'{testmodel}-1run-dfout', dfin_oracle)        \n",
    "\n",
    "\n",
    "            #by multi-run mean at oracle_dfout\n",
    "            df = oracle_dfout\n",
    "            dfin_oracle = df[df['carno']==test_cars[0]]\n",
    "            target_oracle3, tss_oracle3 = long_predict_bydf(f'{testmodel}-multimean', dfin_oracle)        \n",
    "\n",
    "\n",
    "            #no rerank\n",
    "            df = ranknetdf[year][f'{testmodel}_mean']\n",
    "            dfin_oracle = df[df['carno']==test_cars[0]]\n",
    "            target_oracle4, tss_oracle4 = long_predict_bydf(f'{testmodel}-norerank-multimean', dfin_oracle)        \n",
    "\n",
    "\n",
    "            #by multiple runs\n",
    "            target_oracle_multirun, tss_oracle_multirun = get_ranknet_multirun(\n",
    "                                    oracle_ret[mid], \n",
    "                                    test_cars[0],sampleCnt=loopcnt)\n",
    "\n",
    "            retdata[carno] = [[tss_oracle1,tss_oracle2,tss_oracle3,tss_oracle4,tss_oracle_multirun],\n",
    "                               [target_oracle1,target_oracle2,target_oracle3,target_oracle4,target_oracle_multirun]]\n",
    "\n",
    "        alldata = retdata    \n",
    "\n",
    "        if _savedata:\n",
    "            with open(LONG_FORECASTING_DFS, 'wb') as f:\n",
    "                pickle.dump(alldata, f, pickle.HIGHEST_PROTOCOL)  \n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    if _forecast_mode == 'shortterm' and _joint_train == False:\n",
    "        destdir = FORECAST_FIGS_DIR\n",
    "\n",
    "        if _skip_overwrite and os.path.exists(destdir):\n",
    "            print('Long Forecasting Figures at:',destdir)\n",
    "\n",
    "        else:\n",
    "            with open('stagedata-Indy500_2013_2019_v9_p0.pickle', 'rb') as f:\n",
    "                stagedata = pickle.load(f, encoding='latin1') \n",
    "                _alldata, rankdata, _acldata, _flagdata = stagedata[_test_event]\n",
    "\n",
    "            #destdir = outputRoot + 'oracle-forecast-figs/'\n",
    "            os.makedirs(destdir, exist_ok=True)\n",
    "\n",
    "            for carno in alldata:\n",
    "                plotoracle(alldata, carno, destdir)\n",
    "\n",
    "            #draw summary result\n",
    "            outputfile = destdir + f'{configname}'\n",
    "            plotallcars(alldata, outputfile, drawid = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotoracle(alldata, 3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputRoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
