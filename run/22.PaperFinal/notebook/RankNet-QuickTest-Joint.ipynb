{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuickTest Stint\n",
    "\n",
    "    makedb laptime\n",
    "    makedb gluonts\n",
    "    train model\n",
    "    evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import random\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.deep_factor import DeepFactorEstimator\n",
    "from gluonts.model.deepstate import DeepStateEstimator\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.model.prophet import ProphetPredictor\n",
    "from gluonts.model.r_forecast import RForecastPredictor\n",
    "from gluonts.dataset.util import to_pandas\n",
    "\n",
    "from gluonts.distribution.neg_binomial import NegativeBinomialOutput\n",
    "from gluonts.distribution.student_t import StudentTOutput\n",
    "from gluonts.distribution.multivariate_gaussian import MultivariateGaussianOutput\n",
    "\n",
    "from indycar.model.NaivePredictor import NaivePredictor\n",
    "from indycar.model.deeparw import DeepARWeightEstimator\n",
    "\n",
    "#import indycar.model.stint_simulator_shortterm_pitmodel as stint\n",
    "import indycar.model.quicktest_simulator as stint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make indy car completed_laps dataset\n",
    "# car_number, completed_laps, rank, elapsed_time, rank_diff, elapsed_time_diff \n",
    "def make_cl_data(dataset):\n",
    "\n",
    "    # pick up data with valid rank\n",
    "    rankdata = dataset.rename_axis('MyIdx').sort_values(by=['elapsed_time','MyIdx'], ascending=True)\n",
    "    rankdata = rankdata.drop_duplicates(subset=['car_number', 'completed_laps'], keep='first')\n",
    "\n",
    "    # resort by car_number, lap\n",
    "    uni_ds = rankdata.sort_values(by=['car_number', 'completed_laps', 'elapsed_time'], ascending=True)    \n",
    "    #uni_ds = uni_ds.drop([\"unique_id\", \"best_lap\", \"current_status\", \"track_status\", \"lap_status\",\n",
    "    #                  \"laps_behind_leade\",\"laps_behind_prec\",\"overall_rank\",\"pit_stop_count\",\n",
    "    #                  \"last_pitted_lap\",\"start_position\",\"laps_led\"], axis=1)\n",
    "    \n",
    "    uni_ds = uni_ds.drop([\"unique_id\", \"best_lap\", \n",
    "                      \"laps_behind_leade\",\"laps_behind_prec\",\"overall_rank\",\"pit_stop_count\",\n",
    "                      \"last_pitted_lap\",\"start_position\",\"laps_led\"], axis=1)\n",
    "        \n",
    "    carnumber = set(uni_ds['car_number'])\n",
    "    print('cars:', carnumber)\n",
    "    print('#cars=', len(carnumber))\n",
    "   \n",
    "    # faster solution , uni_ds already sorted by car_number and lap\n",
    "    uni_ds['rank_diff'] = uni_ds['rank'].diff()\n",
    "    mask = uni_ds.car_number != uni_ds.car_number.shift(1)\n",
    "    uni_ds['rank_diff'][mask] = 0\n",
    "    \n",
    "    uni_ds['time_diff'] = uni_ds['elapsed_time'].diff()\n",
    "    mask = uni_ds.car_number != uni_ds.car_number.shift(1)\n",
    "    uni_ds['time_diff'][mask] = 0\n",
    "    \n",
    "    #df = uni_ds[['car_number','completed_laps','rank','elapsed_time','rank_diff','time_diff']]\n",
    "    #df = uni_ds[['car_number','completed_laps','rank',\n",
    "    #             'rank_diff','time_diff',\"current_status\", \"track_status\", \"lap_status\",'elapsed_time']]\n",
    "    \n",
    "    df = uni_ds[['car_number','completed_laps','time_diff','rank','track_status', 'lap_status','elapsed_time']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def make_lapstatus_data(dataset):\n",
    "    final_lap = max(dataset.completed_laps)\n",
    "    total_laps = final_lap + 1\n",
    "\n",
    "    # get records for the cars that finish the race\n",
    "    completed_car_numbers= dataset[dataset.completed_laps == final_lap].car_number.values\n",
    "    completed_car_count = len(completed_car_numbers)\n",
    "\n",
    "    print('count of completed cars:', completed_car_count)\n",
    "    print('completed cars:', completed_car_numbers)\n",
    "    \n",
    "    #pick up one of them\n",
    "    onecar = dataset[dataset['car_number']==completed_car_numbers[0]]\n",
    "    onecar = onecar.drop_duplicates(subset=['car_number', 'completed_laps'], keep='first')\n",
    "    return onecar[['completed_laps','track_status']]\n",
    "\n",
    "def load_data(event, year=0):\n",
    "    #inputfile = '../data/final/C_'+ event +'-' + year + '-final.csv'\n",
    "    if year>0:\n",
    "        inputfile = '../data/final/C_'+ event +'-' + year + '.csv'\n",
    "    else:\n",
    "        inputfile = '../data/final/C_'+ event +'.csv'\n",
    "    \n",
    "    #outputprefix = year +'-' + event + '-'\n",
    "    dataset = pd.read_csv(inputfile)\n",
    "    #dataset.info(verbose=True)    \n",
    "    \n",
    "    final_lap = max(dataset.completed_laps)\n",
    "    total_laps = final_lap + 1\n",
    "\n",
    "    # get records for the cars that finish the race\n",
    "    completed_car_numbers= dataset[dataset.completed_laps == final_lap].car_number.values\n",
    "    completed_car_count = len(completed_car_numbers)\n",
    "\n",
    "    print('count of completed cars:', completed_car_count)\n",
    "    print('completed cars:', completed_car_numbers)\n",
    "\n",
    "    #make a copy\n",
    "    alldata = dataset.copy()\n",
    "    dataset = dataset[dataset['car_number'].isin(completed_car_numbers)]\n",
    "    rankdata = alldata.rename_axis('MyIdx').sort_values(by=['elapsed_time','MyIdx'], ascending=True)\n",
    "    rankdata = rankdata.drop_duplicates(subset=['car_number', 'completed_laps'], keep='first')\n",
    "    \n",
    "    cldata = make_cl_data(dataset)\n",
    "    flagdata = make_lapstatus_data(dataset)\n",
    "    acldata = make_cl_data(alldata)\n",
    "\n",
    "    return alldata, rankdata, acldata, flagdata\n",
    "\n",
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def get_lap2nextpit(lap_status, maxlap=200):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        lapstatus  ; array of 0/1 indicating pitstops for each lap, nan means incomplete race\n",
    "        maxlap     ; the max lap number of the race\n",
    "    output:\n",
    "        lap2nextpit ; array of the lap gap to the next pit for each lap\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #pitstops = np.where(lap_status==1)[0]\n",
    "                    \n",
    "    pitstops = list(np.where(lap_status==1)[0])\n",
    "    #if not len(lap_status) < maxlap:\n",
    "    nans, x= nan_helper(lap_status)\n",
    "    nan_count = np.sum(nans)      \n",
    "    if nan_count == 0:\n",
    "        #complete cars\n",
    "        # the last stint, to the end\n",
    "        pitstops.append(maxlap)\n",
    "    \n",
    "    lap2nextpit = np.zeros_like(lap_status)\n",
    "    lap2nextpit[:] = np.nan\n",
    "    \n",
    "    #guard\n",
    "    if len(pitstops)==0:\n",
    "        return lap2nextpit\n",
    "    \n",
    "    idx = 0\n",
    "    for lap in range(len(lap_status)):\n",
    "        if lap < pitstops[idx]:\n",
    "            lap2nextpit[lap] = pitstops[idx] - lap\n",
    "        else:\n",
    "            idx += 1\n",
    "            if idx < len(pitstops):\n",
    "                lap2nextpit[lap] = pitstops[idx] - lap\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    return lap2nextpit\n",
    "\n",
    "def get_lapdata(acldata):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        acldata['car_number','completed_laps','time_diff','rank','track_status', 'lap_status','elapsed_time']\n",
    "    \n",
    "        timediff: [car_number, completed_laps] -> elapsed time diff to leader\n",
    "    output:\n",
    "        lapdata = acldata[['car_number','completed_laps',\n",
    "                           'time_diff','rank','track_status', 'lap_status','time_behind']].to_numpy()\n",
    "    \"\"\"\n",
    "    COL_COMPLETED_LAPS = 1\n",
    "    COL_ELAPSED_TIME = 6\n",
    "    \n",
    "    maxlap = np.max(acldata['completed_laps'].values)\n",
    "    #'car_number','completed_laps','time_diff','rank','track_status', 'lap_status','time_behind'\n",
    "    time_behind = []\n",
    "    \n",
    "    for lap in range(1, maxlap+1):\n",
    "        this_lap = acldata[acldata['completed_laps']==lap][\n",
    "            ['car_number','completed_laps','time_diff','rank',\n",
    "             'track_status', 'lap_status','elapsed_time']].values\n",
    "        \n",
    "        min_elapsed_time = np.nanmin(this_lap[:,COL_ELAPSED_TIME].astype(np.float))\n",
    "        #print(f'lap:{lap}, min_elapsed_time:{min_elapsed_time}')\n",
    "        \n",
    "        for row in this_lap:\n",
    "            car_number = int(row[0])\n",
    "            time_diff = row[2]\n",
    "            rank = row[3]\n",
    "            track_status = row[4]\n",
    "            lap_status = row[5]\n",
    "            \n",
    "            timebehind = float(row[COL_ELAPSED_TIME]) - min_elapsed_time\n",
    "            #\n",
    "            time_behind.append([car_number, lap, time_diff,rank,track_status, lap_status,\n",
    "                                timebehind, float(row[COL_ELAPSED_TIME])])\n",
    "    \n",
    "    #return\n",
    "    lapdata = np.array(time_behind)\n",
    "    return lapdata\n",
    "\n",
    "\n",
    "\n",
    "# features: laptime, rank, track_status, lap_status, timediff\n",
    "LAPTIME = 0\n",
    "RANK = 1\n",
    "TRACK_STATUS = 2\n",
    "LAP_STATUS = 3\n",
    "TIME_BEHIND = 4\n",
    "CAUTION_LAPS_INSTINT = 5 \n",
    "LAPS_INSTINT = 6\n",
    "ELAPSED_TIME = 7\n",
    "LAP2NEXTPIT = 8\n",
    "\n",
    "_featureCnt = 9\n",
    "        \n",
    "def get_laptime_dataset(stagedata, inlap_status = 0):\n",
    "    \"\"\"\n",
    "    #add caution_laps_instint, laps_instint\n",
    "    \n",
    "    input: (alldata, rankdata, acldata, flagdata)\n",
    "    output: laptime & rank data\n",
    "    \n",
    "    [(\n",
    "    eventid,\n",
    "    carids : rowid -> carno,\n",
    "    datalist: #car_number x features x #totallaps (padded by Nan)\n",
    "        entry: [[laptime, rank, track_status, lap_status,\n",
    "                caution_laps_instint, laps_instint]]\n",
    "    )]\n",
    "    \"\"\"\n",
    "    laptime_data = []\n",
    "    for event in stagedata.keys():\n",
    "        \n",
    "        print(f'start event: {event}')\n",
    "        \n",
    "        laptime_rec = []\n",
    "        eventid = events_id[event]\n",
    "        \n",
    "        alldata, rankdata, acldata, flagdata = stagedata[event]\n",
    "        carlist = set(acldata['car_number'])\n",
    "        laplist = set(acldata['completed_laps'])\n",
    "        totalcars = len(carlist)\n",
    "        totallaps = len(laplist)\n",
    "        \n",
    "\n",
    "\n",
    "        #carnumber -> carid\n",
    "        carids={key:idx for idx, key in enumerate(carlist)}\n",
    "        decode_carids={idx:key for idx, key in enumerate(carlist)}\n",
    "\n",
    "        #init\n",
    "        lap_instint = {carids[x]:0 for x in carlist}\n",
    "        caution_instint = {carids[x]:0 for x in carlist}        \n",
    "        \n",
    "        #array: car_number x lap\n",
    "        #laptime = np.zeros((totalcars, totallaps-1))\n",
    "        #rank = np.zeros((totalcars, totallaps-1))\n",
    "        laptime = np.empty((totalcars, totallaps-1))\n",
    "        rank = np.empty((totalcars, totallaps-1))\n",
    "        laptime[:] = np.NaN\n",
    "        rank[:] = np.NaN\n",
    "        \n",
    "\n",
    "        datalist = np.empty((totalcars, _featureCnt, totallaps-1))\n",
    "        datalist[:] = np.NaN\n",
    "        \n",
    "        #lapdata = acldata[['car_number','completed_laps',\n",
    "        #                   'time_diff','rank','track_status', 'lap_status','elapsed_time']].to_numpy()\n",
    "        \n",
    "        #'car_number','completed_laps','time_diff','rank','track_status', 'lap_status','time_behind'\n",
    "        lapdata = get_lapdata(acldata)\n",
    "        \n",
    "        \n",
    "        for row in lapdata:\n",
    "            #completed_laps\n",
    "            if int(row[1]) == 0:\n",
    "                continue\n",
    "                \n",
    "            #add to data array\n",
    "            car_number = carids[int(row[0])]\n",
    "            completed_laps = int(row[1])-1\n",
    "            time_diff = float(row[2])\n",
    "            rank = int(row[3])\n",
    "            track_status = 1 if row[4]=='Y' else 0\n",
    "            lap_status = 1 if row[5]=='P' else 0\n",
    "            time_behind = float(row[6])\n",
    "            \n",
    "            datalist[car_number, LAPTIME, completed_laps] = time_diff\n",
    "            datalist[car_number, RANK, completed_laps] = rank\n",
    "            datalist[car_number, TRACK_STATUS, completed_laps] = track_status\n",
    "            datalist[car_number, LAP_STATUS, completed_laps] = lap_status\n",
    "            datalist[car_number, TIME_BEHIND, completed_laps] = time_behind\n",
    "\n",
    "            datalist[car_number, ELAPSED_TIME, completed_laps] = float(row[7])\n",
    "\n",
    "            \n",
    "            #stint status\n",
    "            if track_status == 1:\n",
    "                caution_instint[car_number] += 1\n",
    "            lap_instint[car_number] += 1\n",
    "            if lap_status == 1:\n",
    "                #new stint\n",
    "                lap_instint[car_number] = 0\n",
    "                caution_instint[car_number] = 0\n",
    "                \n",
    "                # add inlap feature into lap_Status\n",
    "                # set the previous lap to inlap status\n",
    "                \n",
    "                # what does it mean?\n",
    "                \n",
    "                if (inlap_status!=0):\n",
    "                    if inlap_status == 1:\n",
    "                        # set the previous lap of 'P'\n",
    "                        if completed_laps > 0:\n",
    "                            #datalist[car_number, LAP_STATUS, completed_laps-1] = INLAP_STATUS\n",
    "                            datalist[car_number, LAP_STATUS, completed_laps-1] = 1\n",
    "                    else:\n",
    "                        # set the next lap of 'P'\n",
    "                        if completed_laps +1 < totallaps:\n",
    "                            #datalist[car_number, LAP_STATUS, completed_laps-1] = INLAP_STATUS\n",
    "                            datalist[car_number, LAP_STATUS, completed_laps + 1] = 1\n",
    "                \n",
    "            \n",
    "            datalist[car_number, LAPS_INSTINT, completed_laps] = lap_instint[car_number]\n",
    "            datalist[car_number, CAUTION_LAPS_INSTINT, completed_laps] = caution_instint[car_number]\n",
    "                \n",
    "\n",
    "                \n",
    "        #update lap2nextpit in datalist\n",
    "        for caridx in range(datalist.shape[0]):\n",
    "            lap_status = datalist[caridx, LAP_STATUS, :]\n",
    "            #pit status\n",
    "            lap2nextpit = get_lap2nextpit(lap_status)\n",
    "            datalist[caridx, LAP2NEXTPIT, :] = lap2nextpit        \n",
    "                \n",
    "        #add one record\n",
    "        laptime_data.append([eventid, decode_carids, datalist])\n",
    "        # push this event into stage dataframe\n",
    "        print('event=%s, records=%s'%(event, datalist.shape))\n",
    "        \n",
    "    \n",
    "    return laptime_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def test_flag(a, bitflag):\n",
    "    return (a & bitflag) ==  bitflag\n",
    "\n",
    "#\n",
    "# remove NaN at the tail\n",
    "# there should be no nans in the middle of the ts\n",
    "COL_LAPTIME=0\n",
    "COL_RANK=1\n",
    "COL_TRACKSTATUS=2\n",
    "COL_LAPSTATUS=3\n",
    "COL_TIMEDIFF=4\n",
    "COL_CAUTION_LAPS_INSTINT=5\n",
    "COL_LAPS_INSTINT= 6\n",
    "COL_ELAPSED_TIME= 7\n",
    "COL_LAP2NEXTPIT = 8\n",
    "#_featureCnt = 9\n",
    "\n",
    "# added new features\n",
    "COL_LEADER_PITCNT = 9\n",
    "COL_TOTAL_PITCNT = 10\n",
    "COL_SHIFT_TRACKSTATUS = 11\n",
    "COL_SHIFT_LAPSTATUS = 12\n",
    "COL_SHIFT_LEADER_PITCNT = 13\n",
    "COL_SHIFT_TOTAL_PITCNT = 14\n",
    "\n",
    "COL_LASTFEATURE = 14\n",
    "# dynamically extended space in simulation\n",
    "COL_TRACKSTATUS_SAVE = COL_LASTFEATURE+1\n",
    "COL_LAPSTATUS_SAVE = COL_LASTFEATURE+2\n",
    "COL_CAUTION_LAPS_INSTINT_SAVE = COL_LASTFEATURE+3\n",
    "COL_LAPS_INSTINT_SAVE= COL_LASTFEATURE+4\n",
    "\n",
    "COL_ENDPOS = COL_LASTFEATURE+5\n",
    "\n",
    "\n",
    "FEATURE_STATUS = 2\n",
    "FEATURE_PITAGE = 4\n",
    "FEATURE_LEADER_PITCNT = 8\n",
    "FEATURE_TOTAL_PITCNT = 16\n",
    "FEATURE_SHIFT_TRACKSTATUS = 32\n",
    "FEATURE_SHIFT_LAPSTATUS = 64\n",
    "FEATURE_SHIFT_LEADER_PITCNT = 128\n",
    "FEATURE_SHIFT_TOTAL_PITCNT  = 256\n",
    "\n",
    "_feature2str= {\n",
    "    FEATURE_STATUS : (\"FEATURE_STATUS\",'S'),\n",
    "    FEATURE_PITAGE : (\"FEATURE_PITAGE\",'A'),\n",
    "    FEATURE_LEADER_PITCNT : (\"FEATURE_LEADER_PITCNT\",'L'),\n",
    "    FEATURE_TOTAL_PITCNT : (\"FEATURE_TOTAL_PITCNT\",'T'),\n",
    "    FEATURE_SHIFT_TRACKSTATUS : (\"FEATURE_SHIFT_TRACKSTATUS\",'Y'),\n",
    "    FEATURE_SHIFT_LAPSTATUS : (\"FEATURE_SHIFT_LAPSTATUS\",'P'),\n",
    "    FEATURE_SHIFT_LEADER_PITCNT : (\"FEATURE_SHIFT_LEADER_PITCNT\",'L'),\n",
    "    FEATURE_SHIFT_TOTAL_PITCNT  : (\"FEATURE_SHIFT_TOTAL_PITCNT\",'T')\n",
    "    }\n",
    "\n",
    "\n",
    "MODE_ORACLE = 0\n",
    "MODE_NOLAP = 1\n",
    "MODE_NOTRACK = 2\n",
    "MODE_TESTZERO = 4\n",
    "MODE_TESTCURTRACK = 8\n",
    "#MODE_STR={MODE_ORACLE:'oracle', MODE_NOLAP:'nolap',MODE_NOTRACK:'notrack',MODE_TEST:'test'}\n",
    "\n",
    "#_feature_mode = FEATURE_STATUS\n",
    "def decode_feature_mode(feature_mode):\n",
    "    \n",
    "    retstr = []\n",
    "    short_ret = []\n",
    "    for feature in _feature2str.keys():\n",
    "        if test_flag(feature_mode, feature):\n",
    "            retstr.append(_feature2str[feature][0])\n",
    "            short_ret.append(_feature2str[feature][1])\n",
    "        else:\n",
    "            short_ret.append('0')\n",
    "\n",
    "    print(' '.join(retstr))\n",
    "    \n",
    "    return ''.join(short_ret)\n",
    "\n",
    "\n",
    "def add_leader_cnt(selmat, rank_col=COL_RANK, pit_col=COL_LAPSTATUS, shift_len = 0, \n",
    "                   dest_col = COL_LEADER_PITCNT,\n",
    "                   verbose = False):\n",
    "    \"\"\"\n",
    "    add a new feature into mat(car, feature, lap)\n",
    "    \n",
    "    shift rank status\n",
    "    \n",
    "    input:\n",
    "        sel_mat : laptime_data array [car, feature, lap]\n",
    "    \n",
    "    \"\"\"\n",
    "    dim1, dim2, dim3 = selmat.shape\n",
    "    \n",
    "    # rerank by the rank_col\n",
    "    idx = np.argsort(selmat[:, rank_col,:], axis=0)\n",
    "    true_rank = np.argsort(idx, axis=0).astype(np.float)\n",
    "\n",
    "    # get leaderCnt by sorted pits\n",
    "    pits = np.zeros((dim1,dim3))\n",
    "    \n",
    "    for lap in range(shift_len, dim3):\n",
    "        col = idx[:, lap-shift_len]\n",
    "        pits[:, lap] = selmat[col, pit_col, lap]\n",
    "    \n",
    "    leaderCnt = np.nancumsum(pits, axis=0) - pits\n",
    "    \n",
    "    if verbose:\n",
    "        print('pits:\\n')\n",
    "        print(pits[:,190:])\n",
    "        print('leaderCnt raw:\\n')\n",
    "        print(leaderCnt[:,190:])\n",
    "    \n",
    "    #remove nans\n",
    "    nanidx = np.isnan(leaderCnt)\n",
    "    leaderCnt[nanidx] = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print('leaderCnt after remove nan:\\n')\n",
    "        print(leaderCnt[:,190:])\n",
    "    \n",
    "    if dest_col == -1:\n",
    "        #create a new data\n",
    "        newmat = np.zeros((dim1,dim2+1,dim3))\n",
    "        dest_col = dim2\n",
    "        newmat[:,:dim2,:] = selmat.copy()\n",
    "    else:\n",
    "        #update mode\n",
    "        newmat = selmat\n",
    "    \n",
    "    for lap in range(dim3):\n",
    "        col = idx[:, lap]\n",
    "        newmat[col, dest_col, lap] = leaderCnt[:, lap]\n",
    "        \n",
    "    # sync length to COL_RANK\n",
    "    for rec in newmat:\n",
    "        nans, x= nan_helper(rec[rank_col,:])\n",
    "        nan_count = np.sum(nans)\n",
    "        if nan_count > 0:\n",
    "            #todo, some invalid nan, remove them\n",
    "            #rec[dim2, np.isnan(rec[dim2,:])] = 0\n",
    "            rec[dest_col, -nan_count:] = np.nan\n",
    "    \n",
    "    return newmat\n",
    "\n",
    "def add_allpit_cnt(selmat, rank_col=COL_RANK, pit_col=COL_LAPSTATUS, \n",
    "                   dest_col = COL_TOTAL_PITCNT,verbose = False):\n",
    "    \"\"\"\n",
    "    add a new feature into mat(car, feature, lap)\n",
    "    \n",
    "    total pits in a lap\n",
    "    \n",
    "    input:\n",
    "        sel_mat : laptime_data array [car, feature, lap]\n",
    "    \n",
    "    \"\"\"\n",
    "    dim1, dim2, dim3 = selmat.shape\n",
    "\n",
    "    #calc totalCnt vector for \n",
    "    totalCnt = np.nansum(selmat[:, pit_col, :], axis=0).reshape((-1))\n",
    "    \n",
    "    if verbose:\n",
    "        print('pits:\\n')\n",
    "        print(pits[:,190:])\n",
    "        print('totalCnt raw:\\n')\n",
    "        print(totalCnt[190:])\n",
    "    \n",
    "    #remove nans\n",
    "    nanidx = np.isnan(totalCnt)\n",
    "    totalCnt[nanidx] = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print('totalCnt after remove nan:\\n')\n",
    "        print(totalCnt[190:])\n",
    "    \n",
    "    if dest_col == -1:\n",
    "        #create a new data\n",
    "        newmat = np.zeros((dim1,dim2+1,dim3))\n",
    "        dest_col = dim2\n",
    "        newmat[:,:dim2,:] = selmat.copy()\n",
    "    else:\n",
    "        #update mode\n",
    "        newmat = selmat\n",
    "\n",
    "    for car in range(dim1):\n",
    "        newmat[car, dest_col, :] = totalCnt\n",
    "        \n",
    "    # sync length to COL_RANK\n",
    "    for rec in newmat:\n",
    "        nans, x= nan_helper(rec[rank_col,:])\n",
    "        nan_count = np.sum(nans)\n",
    "        if nan_count > 0:\n",
    "            #todo, some invalid nan, remove them\n",
    "            #rec[dim2, np.isnan(rec[dim2,:])] = 0\n",
    "            rec[dest_col, -nan_count:] = np.nan\n",
    "    \n",
    "    return newmat\n",
    "\n",
    "def add_shift_feature(selmat, rank_col=COL_RANK, shift_col=COL_LAPSTATUS, shift_len = 2, \n",
    "                      dest_col = -1,verbose = False):\n",
    "    \"\"\"\n",
    "    add a new feature into mat(car, feature, lap)\n",
    "    \n",
    "    shift features left in a lap\n",
    "    \n",
    "    warning: these are oracle features, be careful not to let future rank positions leaking\n",
    "    \n",
    "    input:\n",
    "        sel_mat : laptime_data array [car, feature, lap]\n",
    "    \n",
    "    \"\"\"\n",
    "    dim1, dim2, dim3 = selmat.shape\n",
    "\n",
    "    if dest_col == -1:\n",
    "        #create a new data\n",
    "        newmat = np.zeros((dim1,dim2+1,dim3))\n",
    "        dest_col = dim2\n",
    "        newmat[:,:dim2,:] = selmat.copy()\n",
    "    else:\n",
    "        #update mode\n",
    "        newmat = selmat\n",
    "    \n",
    "    for car in range(dim1):\n",
    "        # set empty status by default\n",
    "        newmat[car, dest_col, :] = np.nan\n",
    "        \n",
    "        # get valid laps\n",
    "        rec = selmat[car]\n",
    "        nans, x= nan_helper(rec[rank_col,:])\n",
    "        nan_count = np.sum(nans)\n",
    "        recnnz = rec[shift_col, ~np.isnan(rec[rank_col,:])]\n",
    "        reclen = len(recnnz)\n",
    "\n",
    "        #shift copy\n",
    "        newmat[car, dest_col, :reclen] = 0\n",
    "        #newmat[car, dim2, :-shift_len] = selmat[car, shift_col, shift_len:]\n",
    "        newmat[car, dest_col, :reclen-shift_len] = recnnz[shift_len:]\n",
    "        \n",
    "    # sync length to COL_RANK\n",
    "    #for rec in newmat:\n",
    "    #    nans, x= nan_helper(rec[rank_col,:])\n",
    "    #    nan_count = np.sum(nans)\n",
    "    #    if nan_count > 0:\n",
    "    #        #todo, some invalid nan, remove them\n",
    "    #        #rec[dim2, np.isnan(rec[dim2,:])] = 0\n",
    "    #        rec[dim2, -nan_count:] = np.nan\n",
    "    \n",
    "    return newmat\n",
    "\n",
    "\n",
    "def prepare_laptimedata(prediction_length, freq, \n",
    "                       test_event = 'Indy500-2018',\n",
    "                       train_ratio=0.8,\n",
    "                       context_ratio = 0.,\n",
    "                       shift_len = -1):\n",
    "    \"\"\"\n",
    "    prepare the laptime data for training\n",
    "    \n",
    "    1. remove short ts\n",
    "    2. rerank the tss\n",
    "    3. create new features\n",
    "    \n",
    "    input: \n",
    "        laptime_data   ; global var\n",
    "    output:\n",
    "        data  ; new representation of laptime_data\n",
    "    \n",
    "    \"\"\"\n",
    "    _laptime_data = laptime_data.copy()\n",
    "    \n",
    "    test_eventid = events_id[test_event]\n",
    "    run_ts = COL_RANK\n",
    "    \n",
    "    # check shift len\n",
    "    if shift_len < 0:\n",
    "        shift_len = prediction_length\n",
    "    print('prepare_laptimedata shift len:', shift_len)\n",
    "    \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    new_data = []\n",
    "    for _data in _laptime_data:\n",
    "        #skip eid > test_eventid\n",
    "        if _data[0] > test_eventid:\n",
    "            print('skip this event:', events[_data[0]])\n",
    "            break\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        else:\n",
    "            test_mode = False        \n",
    "        \n",
    "        #statistics on the ts length\n",
    "        ts_len = [ _entry.shape[1] for _entry in _data[2]]\n",
    "        train_len = int(np.max(ts_len) * train_ratio)\n",
    "        if train_len == 0:\n",
    "            #use global train_len\n",
    "            train_len = _train_len if not test_mode else _test_train_len\n",
    "        \n",
    "        if context_ratio != 0.:\n",
    "            # add this part to train set\n",
    "            context_len = int(np.max(ts_len) * context_ratio)\n",
    "        else:    \n",
    "            context_len = prediction_length*2\n",
    "        if context_len < 10:\n",
    "            context_len = 10\n",
    "        \n",
    "        print(f'before ====event:{events[_data[0]]}, prediction_len={prediction_length},train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)},context_len={context_len}')\n",
    "\n",
    "        #rerank due to short ts removed\n",
    "        #if run_ts == COL_RANK and dorerank == True:\n",
    "        if True:\n",
    "            sel_rows = []\n",
    "            \n",
    "            # use to check the dimension of features\n",
    "            input_feature_cnt = _data[2].shape[1]\n",
    "            if input_feature_cnt < COL_LASTFEATURE + 1:\n",
    "                print('create new features mode, feature_cnt:', input_feature_cnt)\n",
    "            else:\n",
    "                print('update features mode, feature_cnt:', input_feature_cnt)\n",
    "            \n",
    "            for rowid in range(_data[2].shape[0]):\n",
    "                # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "                rec = _data[2][rowid].copy()\n",
    "                #remove nan(only tails)\n",
    "                nans, x= nan_helper(rec[run_ts,:])\n",
    "                nan_count = np.sum(nans)             \n",
    "                rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "                \n",
    "                totallen = rec.shape[1]\n",
    "                if ( totallen < train_len + prediction_length):\n",
    "                    print(f'rerank a short ts: carid={_data[1][rowid]}，len={totallen}')\n",
    "                    continue \n",
    "                else:\n",
    "                    sel_rows.append(rowid)\n",
    "                    \n",
    "            #get selected matrix\n",
    "            sel_idx = np.array(sel_rows)\n",
    "            selmat = _data[2][sel_idx]\n",
    "            \n",
    "            # check the format of _data\n",
    "            #ipdb.set_trace()\n",
    "            \n",
    "            mask = np.isnan(selmat[:,COL_RANK,:])\n",
    "            \n",
    "            idx = np.argsort(selmat[:,COL_RANK,:], axis=0)\n",
    "            true_rank = np.argsort(idx, axis=0).astype(np.float)\n",
    "            true_rank[mask] = np.nan\n",
    "            \n",
    "            if test_mode:\n",
    "                #\n",
    "                # for historical code mismatch, simulation does not run rerank\n",
    "                #\n",
    "                _data[2][sel_idx,COL_RANK,:] = true_rank + 1\n",
    "            else:\n",
    "                _data[2][sel_idx,COL_RANK,:] = true_rank\n",
    "            \n",
    "            # update the carno dict\n",
    "            new_carids = {}\n",
    "            for rowid in range(len(sel_idx)):\n",
    "                carid = sel_idx[rowid]\n",
    "                carno = _data[1][carid]\n",
    "                new_carids[rowid] = carno\n",
    "\n",
    "                \n",
    "            # add new features\n",
    "            # add leaderPitCnt\n",
    "            if _data[0]==0:\n",
    "                verbose = True\n",
    "            else:\n",
    "                verbose = False\n",
    "                \n",
    "\n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_LEADER_PITCNT\n",
    "            data2_intermediate = add_leader_cnt(_data[2][sel_idx], shift_len = shift_len, dest_col=dest_col, verbose = verbose)\n",
    "            \n",
    "            # add totalPit\n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_TOTAL_PITCNT\n",
    "            data2_intermediate = add_allpit_cnt(data2_intermediate, dest_col=dest_col)\n",
    "            \n",
    "            #\n",
    "            # add shift features, a fixed order, see the MACROS \n",
    "            #COL_SHIFT_TRACKSTATUS = 11\n",
    "            #COL_SHIFT_LAPSTATUS = 12\n",
    "            #COL_SHIFT_LEADER_PITCNT = 13\n",
    "            #COL_SHIFT_TOTAL_PITCNT = 14\n",
    "            #\n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_SHIFT_TRACKSTATUS\n",
    "            data2_intermediate = add_shift_feature(data2_intermediate, dest_col=dest_col,\n",
    "                                                   shift_col=COL_TRACKSTATUS, shift_len = shift_len)\n",
    "            \n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_SHIFT_LAPSTATUS\n",
    "            data2_intermediate = add_shift_feature(data2_intermediate, dest_col=dest_col,\n",
    "                                                   shift_col=COL_LAPSTATUS, shift_len = shift_len)\n",
    "            \n",
    "            # leader_pitcnt can not be shift, target leaking, just do not use it\n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_SHIFT_LEADER_PITCNT\n",
    "            data2_intermediate = add_shift_feature(data2_intermediate, dest_col=dest_col,\n",
    "                                                   shift_col=COL_LEADER_PITCNT, shift_len = shift_len)\n",
    "            \n",
    "            dest_col = -1 if input_feature_cnt < COL_LASTFEATURE + 1 else COL_SHIFT_TOTAL_PITCNT\n",
    "            data2_intermediate = add_shift_feature(data2_intermediate, dest_col=dest_col,\n",
    "                                                   shift_col=COL_TOTAL_PITCNT, shift_len = shift_len)\n",
    "            \n",
    "            # final\n",
    "            data2_newfeature = data2_intermediate\n",
    "            \n",
    "        new_data.append([_data[0], new_carids, data2_newfeature])\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "\n",
    "def get_real_features(feature_mode, rec, endpos):\n",
    "    \"\"\"\n",
    "    construct the real value feature vector from feature_mode\n",
    "\n",
    "    legacy code:\n",
    "        real_features = {\n",
    "            FEATURE_STATUS:[rec[COL_TRACKSTATUS,:],rec[COL_LAPSTATUS,:]],\n",
    "            FEATURE_PITAGE:[rec[COL_TRACKSTATUS,:],rec[COL_LAPSTATUS,:],rec[COL_LAPS_INSTINT,:]],\n",
    "            FEATURE_LEADERPITCNT:[rec[COL_TRACKSTATUS,:],rec[COL_LAPSTATUS,:],rec[COL_LEADER_PITCNT,:]],\n",
    "            FEATURE_TOTALPITCNT:[rec[COL_TRACKSTATUS,:],rec[COL_LAPSTATUS,:],rec[COL_TOTAL_PITCNT,:]]\n",
    "        }    \n",
    "    \n",
    "        real_features[feature_mode]\n",
    "        \n",
    "        \n",
    "        COL_LEADER_PITCNT = 9\n",
    "        COL_TOTAL_PITCNT = 10\n",
    "        COL_SHIFT_TRACKSTATUS = 11\n",
    "        COL_SHIFT_LAPSTATUS = 12\n",
    "        COL_SHIFT_LEADER_PITCNT = 13\n",
    "        COL_SHIFT_TOTAL_PITCNT = 14\n",
    "\n",
    "\n",
    "        FEATURE_STATUS = 2\n",
    "        FEATURE_PITAGE = 4\n",
    "        FEATURE_LEADER_PITCNT = 8\n",
    "        FEATURE_TOTAL_PITCNT = 16\n",
    "        FEATURE_SHIFT_TRACKSTATUS = 32\n",
    "        FEATURE_SHIFT_LAPSTATUS = 64\n",
    "        FEATURE_SHIFT_LEADER_PITCNT = 128\n",
    "        FEATURE_SHIFT_TOTAL_PITCNT  = 256        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    #check endpos\n",
    "    if endpos <=0 :\n",
    "        endpos = rec.shape[1]\n",
    "    \n",
    "    if test_flag(feature_mode, FEATURE_STATUS):\n",
    "        features.append(rec[COL_TRACKSTATUS,:endpos])\n",
    "        features.append(rec[COL_LAPSTATUS,:endpos])\n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_PITAGE):\n",
    "        features.append(rec[COL_LAPS_INSTINT,:endpos])\n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_LEADER_PITCNT):\n",
    "        features.append(rec[COL_LEADER_PITCNT,:endpos])\n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_TOTAL_PITCNT):\n",
    "        features.append(rec[COL_TOTAL_PITCNT,:endpos])    \n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_SHIFT_TRACKSTATUS):\n",
    "        features.append(rec[COL_SHIFT_TRACKSTATUS,:endpos])    \n",
    "        \n",
    "    if test_flag(feature_mode, FEATURE_SHIFT_LAPSTATUS):\n",
    "        features.append(rec[COL_SHIFT_LAPSTATUS,:endpos])    \n",
    "\n",
    "    if test_flag(feature_mode, FEATURE_SHIFT_LEADER_PITCNT):\n",
    "        features.append(rec[COL_SHIFT_LEADER_PITCNT,:endpos])    \n",
    "\n",
    "    if test_flag(feature_mode, FEATURE_SHIFT_TOTAL_PITCNT):\n",
    "        features.append(rec[COL_SHIFT_TOTAL_PITCNT,:endpos])    \n",
    "        \n",
    "        \n",
    "    return features\n",
    "\n",
    "def make_dataset_byevent(_laptime_data, prediction_length, freq, \n",
    "                       useeid = False,\n",
    "                       run_ts=COL_LAPTIME, \n",
    "                       test_event = 'Indy500-2018',\n",
    "                       use_global_dict = True,\n",
    "                       oracle_mode = MODE_ORACLE,\n",
    "                       half_moving_win = True,\n",
    "                       train_ratio=0.8,\n",
    "                       log_transform = False,\n",
    "                       context_ratio = 0.,\n",
    "                       dorerank = True,\n",
    "                       test_cars = []  \n",
    "                ):\n",
    "    \"\"\"\n",
    "    split the ts to train and test part by the ratio\n",
    "    \n",
    "    oracle_mode: false to simulate prediction in real by \n",
    "        set the covariates of track and lap status as nan in the testset\n",
    "            \n",
    "    \n",
    "    \"\"\"    \n",
    "    #global setting\n",
    "    feature_mode = _feature_mode\n",
    "    \n",
    "    start = pd.Timestamp(\"01-01-2019\", freq=freq)  # can be different for each time series\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    \n",
    "    totalTSCnt = 0\n",
    "    totalTSLen = 0\n",
    "    test_eventid = events_id[test_event]\n",
    "    \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    for _data in _laptime_data:\n",
    "        _train = []\n",
    "        _test = []\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        else:\n",
    "            test_mode = False\n",
    "            \n",
    "        #statistics on the ts length\n",
    "        ts_len = [ _entry.shape[1] for _entry in _data[2]]\n",
    "        train_len = int(np.max(ts_len) * train_ratio)\n",
    "        if train_len == 0:\n",
    "            #use global train_len\n",
    "            train_len = _train_len if not test_mode else _test_train_len\n",
    "        \n",
    "        if context_ratio != 0.:\n",
    "            # add this part to train set\n",
    "            context_len = int(np.max(ts_len) * context_ratio)\n",
    "        else:    \n",
    "            context_len = prediction_length*2\n",
    "        if context_len < 10:\n",
    "            context_len = 10\n",
    "        \n",
    "        print(f'after ====event:{events[_data[0]]}, prediction_len={prediction_length},train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)},context_len={context_len}')\n",
    "\n",
    "        # process for each ts\n",
    "        for rowid in range(_data[2].shape[0]):\n",
    "            # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "            rec = _data[2][rowid].copy()\n",
    "            \n",
    "            #remove nan(only tails)\n",
    "            nans, x= nan_helper(rec[run_ts,:])\n",
    "            nan_count = np.sum(nans)             \n",
    "            rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "            \n",
    "            # remove short ts\n",
    "            totallen = rec.shape[1]\n",
    "            \n",
    "            totalTSCnt += 1\n",
    "            totalTSLen += totallen\n",
    "            \n",
    "            if ( totallen < train_len + prediction_length):\n",
    "                print(f'a short ts: carid={_data[1][rowid]}，len={totallen}')\n",
    "                continue                \n",
    "            \n",
    "            if use_global_dict:\n",
    "                carno = _data[1][rowid]\n",
    "                carid = global_carids[_data[1][rowid]]\n",
    "            else:\n",
    "                #simulation dataset, todo, fix the carids as decoder\n",
    "                carno = rowid\n",
    "                carid = rowid\n",
    "                \n",
    "            #check carno in test_cars, testmode only\n",
    "            if len(test_cars)>0 and carno not in test_cars:\n",
    "                continue\n",
    "                \n",
    "            if useeid:\n",
    "                static_cat = [carid, _data[0]]    \n",
    "            else:\n",
    "                static_cat = [carid]    \n",
    "                \n",
    "            #first, get target a copy    \n",
    "            # target can be COL_XXSTATUS\n",
    "            if _joint_train:\n",
    "                target_cols = [run_ts, COL_LAPSTATUS]\n",
    "                target_val = rec[target_cols].copy().astype(np.float32)                \n",
    "            else:\n",
    "                target_val = rec[run_ts,:].copy().astype(np.float32)\n",
    "                \n",
    "            if log_transform:\n",
    "                target_val = np.log(target_val + 1.0)\n",
    "            \n",
    "            # selection of features\n",
    "            if test_flag(oracle_mode, MODE_NOTRACK):                \n",
    "                rec[COL_TRACKSTATUS, :] = 0\n",
    "            if test_flag(oracle_mode, MODE_NOLAP):                \n",
    "                rec[COL_LAPSTATUS, :] = 0\n",
    "\n",
    "            test_rec_cnt = 0\n",
    "            if not test_mode:\n",
    "                # all go to train set\n",
    "                real_features = get_real_features(feature_mode, rec, -1)\n",
    "                \n",
    "                _train.append({'target': target_val, \n",
    "                            'start': start, \n",
    "                            'feat_static_cat': static_cat,\n",
    "                            'feat_dynamic_real': real_features\n",
    "                          })\n",
    "                    \n",
    "            else:\n",
    "                # reset train_len\n",
    "                if context_ratio != 0.:\n",
    "                    # all go to train set\n",
    "                    #add [0, context_len] to train set \n",
    "                    # all go to train set\n",
    "                    if _joint_train:\n",
    "                        _train.append({'target': target_val[:,:context_len],  \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': get_real_features(feature_mode, rec, context_len)\n",
    "                              })\n",
    "                    else:\n",
    "                        _train.append({'target': target_val[:context_len],  \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': get_real_features(feature_mode, rec, context_len)\n",
    "                              })\n",
    "                              \n",
    "                # testset\n",
    "                # multiple test ts(rolling window as half of the prediction_length)\n",
    "                #step = -int(prediction_length/2) if half_moving_win else -prediction_length\n",
    "                step = -1\n",
    "                for endpos in range(totallen, context_len+prediction_length, \n",
    "                                    step):\n",
    "\n",
    "                    track_rec = rec[COL_TRACKSTATUS, :endpos].copy()\n",
    "                    lap_rec = rec[COL_LAPSTATUS, :endpos].copy()\n",
    "                    pitage_rec = rec[COL_LAPS_INSTINT, :endpos].copy()\n",
    "\n",
    "                    real_features = get_real_features(feature_mode, rec, endpos)\n",
    "                    \n",
    "                    if _joint_train:                    \n",
    "                        _test.append({'target': target_val[:,:endpos], \n",
    "                            'start': start, \n",
    "                            'feat_static_cat': static_cat,\n",
    "                            'feat_dynamic_real': real_features\n",
    "                             })\n",
    "                        \n",
    "                    else:\n",
    "                        _test.append({'target': target_val[:endpos], \n",
    "                            'start': start, \n",
    "                            'feat_static_cat': static_cat,\n",
    "                            'feat_dynamic_real': real_features\n",
    "                             })\n",
    "                                 \n",
    "                    test_rec_cnt += 1\n",
    "            \n",
    "            #check feature cnt\n",
    "            featureCnt = len(real_features)\n",
    "            \n",
    "            #add one ts\n",
    "            print(f'carno:{carno}, totallen:{totallen}, nancount:{nan_count}, test_reccnt:{test_rec_cnt},featureCnt:{featureCnt}')\n",
    "\n",
    "        train_set.extend(_train)\n",
    "        test_set.extend(_test)\n",
    "\n",
    "    print(f'train len:{len(train_set)}, test len:{len(test_set)}, totsl TsCnt:{totalTSCnt}, total ts len:{totalTSLen}')\n",
    "    \n",
    "    train_ds = ListDataset(train_set, freq=freq,one_dim_target= False if _joint_train else True)\n",
    "    test_ds = ListDataset(test_set, freq=freq,one_dim_target= False if _joint_train else True)    \n",
    "    \n",
    "    return train_ds, test_ds, train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_estimator(model, gpuid, epochs=100, batch_size = 32, \n",
    "        target_dim = 3, distr_output = None, use_feat_static = True):\n",
    "    \n",
    "    if int(gpuid) < 0:\n",
    "        ctx = \"cpu\"\n",
    "    else:\n",
    "        ctx = \"gpu(%s)\"%gpuid\n",
    "\n",
    "    if model == 'deepAR':\n",
    "        if use_feat_static:\n",
    "            estimator = DeepAREstimator(\n",
    "                prediction_length=prediction_length,\n",
    "                context_length= context_length,\n",
    "                use_feat_static_cat=True,\n",
    "                cardinality=cardinality,\n",
    "                use_feat_dynamic_real=False,\n",
    "                distr_output = distr_output,\n",
    "                freq=freq,\n",
    "                trainer=Trainer(ctx=ctx, \n",
    "                                batch_size = batch_size,\n",
    "                                epochs=epochs, \n",
    "                                learning_rate=1e-3, \n",
    "                                num_batches_per_epoch=100\n",
    "                               )\n",
    "            )\n",
    "        else:\n",
    "            estimator = DeepAREstimator(\n",
    "                prediction_length=prediction_length,\n",
    "                context_length= context_length,\n",
    "                use_feat_static_cat=False,\n",
    "                #cardinality=cardinality,\n",
    "                use_feat_dynamic_real=False,\n",
    "                distr_output = distr_output,\n",
    "                freq=freq,\n",
    "                trainer=Trainer(ctx=ctx, \n",
    "                                batch_size = batch_size,\n",
    "                                epochs=epochs, \n",
    "                                learning_rate=1e-3, \n",
    "                                num_batches_per_epoch=100\n",
    "                               )\n",
    "        )\n",
    "        \n",
    "    elif model == 'deepAR-Oracle':\n",
    "\n",
    "        if use_feat_static:\n",
    "            estimator = DeepAREstimator(\n",
    "                prediction_length=prediction_length,\n",
    "                context_length= context_length,\n",
    "                use_feat_static_cat=use_feat_static,\n",
    "                cardinality=cardinality,\n",
    "                use_feat_dynamic_real=True,\n",
    "                distr_output = distr_output,\n",
    "                freq=freq,\n",
    "                trainer=Trainer(ctx=ctx, \n",
    "                                batch_size = batch_size,\n",
    "                                epochs=epochs, \n",
    "                                learning_rate=1e-3, \n",
    "                                num_batches_per_epoch=100\n",
    "                               )\n",
    "                )\n",
    "        else:\n",
    "            estimator = DeepAREstimator(\n",
    "                prediction_length=prediction_length,\n",
    "                context_length= context_length,\n",
    "                use_feat_static_cat=use_feat_static,\n",
    "                #cardinality=cardinality,\n",
    "                use_feat_dynamic_real=True,\n",
    "                distr_output = distr_output,\n",
    "                freq=freq,\n",
    "                trainer=Trainer(ctx=ctx, \n",
    "                                batch_size = batch_size,\n",
    "                                epochs=epochs, \n",
    "                                learning_rate=1e-3, \n",
    "                                num_batches_per_epoch=100\n",
    "                               )\n",
    "                )\n",
    "    elif model == 'deepARW-Oracle':\n",
    "\n",
    "        if use_feat_static:\n",
    "            estimator = DeepARWeightEstimator(\n",
    "                prediction_length=prediction_length,\n",
    "                context_length= context_length,\n",
    "                use_feat_static_cat=use_feat_static,\n",
    "                cardinality=cardinality,\n",
    "                use_feat_dynamic_real=True,\n",
    "                distr_output = distr_output,\n",
    "                freq=freq,\n",
    "                trainer=Trainer(ctx=ctx, \n",
    "                                batch_size = batch_size,\n",
    "                                epochs=epochs, \n",
    "                                learning_rate=1e-3, \n",
    "                                #hybridize=False,\n",
    "                                num_batches_per_epoch=100\n",
    "                               )\n",
    "                )\n",
    "        else:\n",
    "            estimator = DeepARWeightEstimator(\n",
    "                prediction_length=prediction_length,\n",
    "                context_length= context_length,\n",
    "                use_feat_static_cat=use_feat_static,\n",
    "                #cardinality=cardinality,\n",
    "                use_feat_dynamic_real=True,\n",
    "                distr_output = distr_output,\n",
    "                freq=freq,\n",
    "                trainer=Trainer(ctx=ctx, \n",
    "                                batch_size = batch_size,\n",
    "                                epochs=epochs, \n",
    "                                learning_rate=1e-3, \n",
    "                                #hybridize=False,\n",
    "                                num_batches_per_epoch=100\n",
    "                               )\n",
    "                )\n",
    "            \n",
    "    elif model == 'deepAR-multi':\n",
    "        estimator = DeepAREstimator(\n",
    "            prediction_length=prediction_length,\n",
    "            context_length= context_length,\n",
    "            use_feat_static_cat=use_feat_static,\n",
    "            #cardinality=cardinality,\n",
    "            use_feat_dynamic_real=False,\n",
    "            freq=freq,\n",
    "            trainer=Trainer(ctx=ctx, \n",
    "                            batch_size = batch_size,\n",
    "                            epochs=epochs, \n",
    "                            learning_rate=1e-3, \n",
    "                            num_batches_per_epoch=100\n",
    "                           ),\n",
    "            distr_output=MultivariateGaussianOutput(dim=target_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    elif model == 'simpleFF':\n",
    "        estimator = SimpleFeedForwardEstimator(\n",
    "            num_hidden_dimensions=[10],\n",
    "            prediction_length=prediction_length,\n",
    "            context_length= context_length,\n",
    "            freq=freq,\n",
    "            trainer=Trainer(ctx=ctx, \n",
    "                            batch_size = batch_size,\n",
    "                            epochs=epochs,\n",
    "                            learning_rate=1e-3,\n",
    "                            hybridize=False,\n",
    "                            num_batches_per_epoch=100\n",
    "                           )\n",
    "        )\n",
    "    elif model == 'deepFactor':\n",
    "        estimator = DeepFactorEstimator(\n",
    "            prediction_length=prediction_length,\n",
    "            context_length= context_length,\n",
    "            freq=freq,\n",
    "            trainer=Trainer(ctx=ctx, \n",
    "                            batch_size = batch_size,\n",
    "                            epochs=epochs, \n",
    "                            learning_rate=1e-3, \n",
    "                            num_batches_per_epoch=100\n",
    "                           )\n",
    "        )\n",
    "    elif model == 'deepState':\n",
    "        estimator = DeepStateEstimator(\n",
    "            prediction_length=prediction_length,\n",
    "            use_feat_static_cat=True,\n",
    "            cardinality=cardinality,\n",
    "            freq=freq,\n",
    "            trainer=Trainer(ctx=ctx, \n",
    "                            batch_size = batch_size,\n",
    "                            epochs=epochs, \n",
    "                            learning_rate=1e-3, \n",
    "                            num_batches_per_epoch=100\n",
    "                           )\n",
    "        )\n",
    "    elif model == 'ets':\n",
    "        estimator = RForecastPredictor(method_name='ets',freq= freq, prediction_length = prediction_length)\n",
    "    elif model == 'prophet':\n",
    "\n",
    "        estimator = ProphetPredictor(freq= freq, prediction_length = prediction_length)\n",
    "    elif model == 'arima':\n",
    "        estimator = RForecastPredictor(method_name='arima',freq= freq, prediction_length = prediction_length, trunc_length = 200)\n",
    "    elif model == 'naive':\n",
    "        estimator = NaivePredictor(freq= freq, prediction_length = prediction_length)\n",
    "    else:\n",
    "        logger.error('model %s not support yet, quit', model)\n",
    "        sys.exit(-1)\n",
    "\n",
    "\n",
    "    return estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# simulation engine general\n",
    "#\n",
    "def init_simulation(datasetid, testevent, taskid, runts, expid, predictionlen, \n",
    "               featuremode = stint.FEATURE_STATUS,\n",
    "               pitmodel = 0, \n",
    "               inlapmode=0,\n",
    "               train_len = 40,test_train_len=40,\n",
    "               joint_train = False,\n",
    "               pitmodel_bias = 0):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        prepared_laptimedata   ; global\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # configurataion\n",
    "    #\n",
    "    # model path:  <_dataset_id>/<_task_id>-<trainid>/\n",
    "    #_dataset_id = 'indy2013-2018-nocarid'\n",
    "    \n",
    "    stint._inlap_status = inlapmode\n",
    "    \n",
    "    stint.init(pitmodel, pitmodel_bias= pitmodel_bias)\n",
    "    \n",
    "    # todo: add into stint code\n",
    "    #here add new laptime_data with new features\n",
    "    #\n",
    "    stint.set_laptimedata(prepared_laptimedata)\n",
    "    \n",
    "    \n",
    "    stint._dataset_id = datasetid\n",
    "    stint._test_event = testevent\n",
    "    #_test_event = 'Indy500-2019'\n",
    "\n",
    "    stint._feature_mode = featuremode\n",
    "    stint._context_ratio = 0.\n",
    "\n",
    "    stint._task_id = taskid  # rank,laptime, the trained model's task\n",
    "    stint._run_ts = runts   #COL_LAPTIME,COL_RANK\n",
    "    stint._exp_id=expid  #rank, laptime, laptim2rank, timediff2rank... \n",
    "\n",
    "    stint._use_mean = True\n",
    "    \n",
    "    stint._train_len = train_len\n",
    "    stint._test_train_len = test_train_len\n",
    "    \n",
    "    stint._joint_train = joint_train\n",
    "    \n",
    "def simulation(datasetid, testevent, taskid, runts, expid, predictionlen, \n",
    "               datamode, loopcnt, featuremode = stint.FEATURE_STATUS,\n",
    "              pitmodel = 0, model = 'oracle', inlapmode=0, train_len = 40,test_train_len=40,\n",
    "              forecastmode = 'shortterm', joint_train = False, \n",
    "               pitmodel_bias= 0):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        prepared_laptimedata   ; global\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #\n",
    "    # configurataion\n",
    "    #\n",
    "    # model path:  <_dataset_id>/<_task_id>-<trainid>/\n",
    "    #_dataset_id = 'indy2013-2018-nocarid'\n",
    "    \n",
    "    stint._inlap_status = inlapmode\n",
    "    \n",
    "    stint.init(pitmodel, pitmodel_bias= pitmodel_bias)\n",
    "    \n",
    "    # todo: add into stint code\n",
    "    #here add new laptime_data with new features\n",
    "    #\n",
    "    stint.set_laptimedata(prepared_laptimedata)\n",
    "    #stint.set_laptimedata(laptime_data)\n",
    "    \n",
    "    stint._dataset_id = datasetid\n",
    "    stint._test_event = testevent\n",
    "    #_test_event = 'Indy500-2019'\n",
    "\n",
    "    stint._feature_mode = featuremode\n",
    "    stint._context_ratio = 0.\n",
    "\n",
    "    stint._task_id = taskid  # rank,laptime, the trained model's task\n",
    "    stint._run_ts = runts   #COL_LAPTIME,COL_RANK\n",
    "    stint._exp_id=expid  #rank, laptime, laptim2rank, timediff2rank... \n",
    "\n",
    "    stint._use_mean = True\n",
    "    \n",
    "    stint._train_len = train_len\n",
    "    stint._test_train_len = test_train_len\n",
    "    \n",
    "    stint._joint_train = joint_train\n",
    "\n",
    "    if forecastmode == 'stint':\n",
    "        stint._trim = 0\n",
    "        stint._debug_carlist=[]\n",
    "        stint._force_endpit_align = False\n",
    "        stint._include_endpit = True    \n",
    "    \n",
    "    predictor = stint.load_model(predictionlen, model,trainid='indy500',epochs = epochs, exproot='./')\n",
    "\n",
    "    ret2 = {}\n",
    "    for i in range(loopcnt):\n",
    "        #df, full_samples, full_tss\n",
    "        if forecastmode == 'shortterm':\n",
    "            ret2[i] = stint.run_simulation_shortterm(predictor, predictionlen, stint.freq, datamode=datamode)\n",
    "        elif forecastmode == 'stint':\n",
    "            ret2[i] = stint.run_simulation_pred(predictor, predictionlen, stint.freq, datamode=datamode)\n",
    "        else:\n",
    "            print('forecastmode not support:', forecastmode)\n",
    "            break\n",
    "            \n",
    "\n",
    "    acc = []\n",
    "    for i in ret2.keys():\n",
    "        \n",
    "        if forecastmode == 'shortterm':\n",
    "            df = ret2[i][0]\n",
    "            _x = stint.get_evalret_shortterm(df)\n",
    "        elif forecastmode == 'stint':\n",
    "            df = ret2[i]\n",
    "            _x = stint.get_evalret(df)\n",
    "        \n",
    "        acc.append(_x)\n",
    "\n",
    "    b = np.array(acc)\n",
    "    print(np.mean(b, axis=0))\n",
    "    \n",
    "    #save keys\n",
    "    #stint._pitmodel.save_keys('pitmodel-keys.pickle')\n",
    "    \n",
    "    return b, ret2\n",
    "\n",
    "def long_predict(predictor, sampleCnt = 100):\n",
    "    \"\"\"\n",
    "    use the farest samples only\n",
    "    \n",
    "    input:\n",
    "        test_ds    ; global var\n",
    "        predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def get_start(idx):\n",
    "        td = forecasts[idx].start_date - start_time\n",
    "        return td.days*24*60 + td.seconds//60\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=sampleCnt,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    print(f'tss len={len(tss)}, forecasts len={len(forecasts)}')\n",
    "    \n",
    "    start_time, row = next(tss[0].iterrows())\n",
    "\n",
    "    first_start = get_start(-1)\n",
    "    last_start = get_start(0)\n",
    "    print(first_start, last_start)    \n",
    "    \n",
    "    import copy\n",
    "    target = copy.deepcopy(forecasts[-1])\n",
    "\n",
    "    #100, 10\n",
    "    nsample, npredict = target.samples.shape\n",
    "    print('sampel# x predictlen: ', nsample, npredict)\n",
    "    \n",
    "    newsamples = np.zeros((nsample, last_start - first_start + npredict))\n",
    "    newsamples[:,:] = np.nan\n",
    "\n",
    "    for idx in range(len(forecasts)):\n",
    "        #copy samples\n",
    "        start_pos = get_start(idx)\n",
    "\n",
    "        pos = start_pos - first_start\n",
    "        #copy sample to block\n",
    "        #newsamples[:, pos:pos + npredict] = forecasts[idx].samples\n",
    "        newsamples[:, pos + npredict - 1] = forecasts[idx].samples[:,-1]\n",
    "        \n",
    "\n",
    "    target.samples = newsamples\n",
    "\n",
    "    #plot_prob_forecasts_ex([tss[0]],[target],output)\n",
    "    \n",
    "    return target, tss[0]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def get_alldf(dfx, year=2018):\n",
    "    \n",
    "\n",
    "    #dfx = ret[f'{model}-RANK-{year}-inlap-nopitage']\n",
    "    #dfx = ret[f'{model}-TIMEDIFF-{year}-noinlap-nopitage']\n",
    "    \n",
    "    samples = dfx.keys()\n",
    "    retdfs = []\n",
    "    for id in samples:\n",
    "        if _forecast_mode == 'shortterm':\n",
    "            df = dfx[id][0]\n",
    "        else:\n",
    "            df = dfx[id]\n",
    "        retdfs.append(df)\n",
    "        \n",
    "    if len(retdfs) > 1:\n",
    "        dfout = pd.concat(retdfs)\n",
    "    else:\n",
    "        dfout = retdfs[0]\n",
    "        \n",
    "    return dfout\n",
    "    \n",
    "def get_alldf_mode(dfx, year=2018,mode=0):\n",
    "    \"\"\"\n",
    "    mode: \n",
    "        0; mode\n",
    "        1; mean\n",
    "        2; median\n",
    "    \"\"\"\n",
    "    dfall = get_alldf(dfx, year=year)\n",
    "    \n",
    "    cars = set(dfall.carno.values)\n",
    "    startlaps = {}\n",
    "    for car in cars:\n",
    "        startlaps[car] = set(dfall[dfall['carno']==car].startlap.values)\n",
    "        \n",
    "    retdf = []\n",
    "    for car in cars:\n",
    "        for startlap in startlaps[car]:\n",
    "            dfrec = dfall[(dfall['carno']==car) & (dfall['startlap']==startlap)]\n",
    "            \n",
    "            #get mode\n",
    "            if mode == 0:\n",
    "                pred_endrank = stats.mode(dfrec.pred_endrank.values).mode[0]\n",
    "                #pred_endlap =  stats.mode(dfrec.pred_endlap.values).mode[0]\n",
    "            elif mode == 1:\n",
    "                #use mean\n",
    "                pred_endrank = np.mean(dfrec.pred_endrank.values)\n",
    "                #pred_endlap =  np.mean(dfrec.pred_endlap.values)\n",
    "            elif mode == 2:\n",
    "                #use mean\n",
    "                pred_endrank = np.median(dfrec.pred_endrank.values)\n",
    "                #pred_endlap =  np.median(dfrec.pred_endlap.values)\n",
    "            \n",
    "            firstrec = dfrec.to_numpy()[0,:]\n",
    "            firstrec[6] = pred_endrank\n",
    "            firstrec[7] = pred_endrank - firstrec[2]\n",
    "            if firstrec[7] == 0:\n",
    "                firstrec[8] = 0\n",
    "            elif firstrec[7] > 0:\n",
    "                firstrec[8] = 1\n",
    "            else:\n",
    "                firstrec[8] = -1\n",
    "                \n",
    "            #endlap, pred_endlap\n",
    "            \n",
    "        \n",
    "            retdf.append(firstrec)\n",
    "        \n",
    "    #dfout = pd.concat(retdf)\n",
    "    if _forecast_mode == 'shortterm':\n",
    "        dfout = pd.DataFrame(retdf, columns =['carno', 'startlap', 'startrank',    \n",
    "                                         'endrank', 'diff', 'sign',\n",
    "                                         'pred_endrank', 'pred_diff', 'pred_sign',\n",
    "                                         #'endlap','pred_endlap'\n",
    "                                        ])\n",
    "    else:\n",
    "        dfout = pd.DataFrame(retdf, columns =['carno', 'startlap', 'startrank',    \n",
    "                                         'endrank', 'diff', 'sign',\n",
    "                                         'pred_endrank', 'pred_diff', 'pred_sign',\n",
    "                                         'endlap','pred_endlap'\n",
    "                                        ])\n",
    "        \n",
    "    print('df size:', len(dfout))\n",
    "    return dfout\n",
    "\n",
    "def get_allsamples(dfx, year=2018):\n",
    "    \n",
    "    runs = list(dfx.keys())\n",
    "    runcnt = len(runs)\n",
    "    \n",
    "    full_samples = {}\n",
    "    full_tss = dfx[runs[0]][2]\n",
    "    carlist = list(full_tss.keys())\n",
    "    samplecnt, lapcnt = dfx[runs[0]][1][carlist[0]].shape\n",
    "    \n",
    "    print('sacmplecnt:', samplecnt, 'lapcnt:',lapcnt,'runcnt:', runcnt)\n",
    "    \n",
    "    #empty samples\n",
    "    for carid, carno in enumerate(carlist):\n",
    "        full_samples[carno] = np.zeros((runcnt, lapcnt))\n",
    "    \n",
    "    for runid in runs:\n",
    "        #one run\n",
    "        tss = dfx[runid][2]\n",
    "        forecast = dfx[runid][1]\n",
    "        \n",
    "        for carid, carno in enumerate(carlist):\n",
    "            #get mean for this run\n",
    "            forecast_mean = np.nanmean(forecast[carno], axis=0)\n",
    "            full_samples[carno][runid, :] = forecast_mean\n",
    "            \n",
    "            #if carno==3 and runid == 0:\n",
    "            #    print('forecast:',forecast_mean)\n",
    "            \n",
    "    return full_samples, full_tss\n",
    "\n",
    "#straight implementation of prisk\n",
    "def quantile_loss(target, quantile_forecast, q):\n",
    "    return 2.0 * np.nansum(\n",
    "        np.abs(\n",
    "            (quantile_forecast - target)\n",
    "            * ((target <= quantile_forecast) - q)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def abs_target_sum(target): \n",
    "    return np.nansum(np.abs(target)) \n",
    "\n",
    "def prisk(full_samples, full_tss, verbose = False):\n",
    "    carlist = full_tss.keys()\n",
    "    tss = []\n",
    "    forecasts = []\n",
    "    forecasts_mean = []\n",
    "    freq = '1min'\n",
    "    start = pd.Timestamp(\"01-01-2019\", freq=freq) \n",
    "\n",
    "    for car in carlist:\n",
    "        testcar = car\n",
    "        fc = SampleForecast(samples = full_samples[testcar][:, 12:], freq=freq, start_date=start + 12)\n",
    "\n",
    "        samples = np.mean(full_samples[testcar][:, 12:], axis =0, keepdims=True)\n",
    "        fc_mean = SampleForecast(samples = samples, freq=freq, start_date=start + 12)\n",
    "\n",
    "        index = pd.date_range(start='2019-01-01 00:00:00', freq = 'T', periods = len(full_tss[testcar]))\n",
    "        ts = pd.DataFrame(index = index, data = full_tss[testcar])    \n",
    "\n",
    "        tss.append(ts)\n",
    "        forecasts.append(fc)\n",
    "        forecasts_mean.append(fc_mean)\n",
    "\n",
    "    evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9]) \n",
    "    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(tss))\n",
    "    if verbose:\n",
    "        print(json.dumps(agg_metrics, indent=4))  \n",
    "    \n",
    "    print(agg_metrics[\"wQuantileLoss[0.1]\"], agg_metrics[\"wQuantileLoss[0.5]\"],agg_metrics[\"wQuantileLoss[0.9]\"])\n",
    "    \n",
    "    return agg_metrics\n",
    "\n",
    "\n",
    "def prisk_direct_bysamples2(full_samples, full_tss, quantiles=[0.1,0.5,0.9], startid = 12, verbose=False):\n",
    "    \"\"\"\n",
    "    target: endrank\n",
    "    forecast: pred_endrank\n",
    "    item_id: <carno, startlap>\n",
    "    \"\"\"\n",
    "    \n",
    "    carlist = full_tss.keys()\n",
    "    \n",
    "    prisk = np.zeros((len(carlist), len(quantiles)))\n",
    "    target_sum = np.zeros((len(carlist)))\n",
    "    aggrisk = np.zeros((len(quantiles)))\n",
    "    \n",
    "    for carid, carno in enumerate(carlist):\n",
    "\n",
    "        # for this car\n",
    "        forecast = full_samples[carno]\n",
    "        target = full_tss[carno]\n",
    "        \n",
    "        #calc quantiles\n",
    "        # len(quantiles) x 1\n",
    "        quantile_forecasts = np.quantile(forecast, quantiles, axis=0)\n",
    "        \n",
    "        for idx, q in enumerate(quantiles):\n",
    "            q_forecast = quantile_forecasts[idx]\n",
    "            prisk[carid, idx] = quantile_loss(target[startid:], q_forecast[startid:], q)\n",
    "            target_sum[carid] = abs_target_sum(target[startid:])\n",
    "            \n",
    "        if verbose==True and carno==3:\n",
    "            print('target:', target[startid:])\n",
    "            print('forecast:', q_forecast[startid:])\n",
    "            print('target_sum:', target_sum[carid])\n",
    "            \n",
    "            print('quantile_forecasts:', quantile_forecasts[:,startid:])\n",
    "        \n",
    "    #agg\n",
    "    #aggrisk = np.mean(prisk, axis=0)\n",
    "    prisk_sum = np.nansum(prisk, axis=0)\n",
    "    if verbose==True:\n",
    "        print('prisk:',prisk)\n",
    "        print('prisk_sum:',prisk_sum)\n",
    "        print('target_sum:',target_sum)\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        aggrisk[idx] = np.divide(prisk_sum[idx], np.sum(target_sum))\n",
    "    \n",
    "    agg_metrics = {}\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        agg_metrics[f'wQuantileLoss[{q}]'] = aggrisk[idx]\n",
    "        \n",
    "    print(agg_metrics.values())\n",
    "    \n",
    "    return agg_metrics, aggrisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prisk_direct_bysamples(full_samples, full_tss, quantiles=[0.1,0.5,0.9], startid = 12, verbose=False):\n",
    "    \"\"\"\n",
    "    calculate prisk by <samples, tss> directly (equal to gluonts implementation)\n",
    "    \n",
    "    target: endrank\n",
    "    forecast: pred_endrank\n",
    "    item_id: <carno, startlap>\n",
    "    \"\"\"\n",
    "    \n",
    "    carlist = full_tss.keys()\n",
    "    \n",
    "    prisk = np.zeros((len(carlist), len(quantiles)))\n",
    "    target_sum = np.zeros((len(carlist)))\n",
    "    aggrisk = np.zeros((len(quantiles)))\n",
    "    \n",
    "    for carid, carno in enumerate(carlist):\n",
    "\n",
    "        # for this car\n",
    "        forecast = full_samples[carno]\n",
    "        target = full_tss[carno]\n",
    "        \n",
    "        #calc quantiles\n",
    "        # len(quantiles) x 1\n",
    "        quantile_forecasts = np.quantile(forecast, quantiles, axis=0)\n",
    "        \n",
    "        for idx, q in enumerate(quantiles):\n",
    "            q_forecast = quantile_forecasts[idx]\n",
    "            prisk[carid, idx] = quantile_loss(target[startid:], q_forecast[startid:], q)\n",
    "            target_sum[carid] = abs_target_sum(target[startid:])\n",
    "            \n",
    "        if verbose==True and carno==3:\n",
    "            print('target:', target[startid:])\n",
    "            print('forecast:', q_forecast[startid:])\n",
    "            print('target_sum:', target_sum[carid])\n",
    "            \n",
    "            print('quantile_forecasts:', quantile_forecasts[:,startid:])\n",
    "        \n",
    "    #agg\n",
    "    #aggrisk = np.mean(prisk, axis=0)\n",
    "    prisk_sum = np.nansum(prisk, axis=0)\n",
    "    if verbose==True:\n",
    "        print('prisk:',prisk)\n",
    "        print('prisk_sum:',prisk_sum)\n",
    "        print('target_sum:',target_sum)\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        aggrisk[idx] = np.divide(prisk_sum[idx], np.sum(target_sum))\n",
    "    \n",
    "    agg_metrics = {}\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        agg_metrics[f'wQuantileLoss[{q}]'] = aggrisk[idx]\n",
    "        \n",
    "    print(agg_metrics.values())\n",
    "    \n",
    "    return agg_metrics, aggrisk\n",
    "\n",
    "def clear_samples(full_samples, full_tss, clearidx):\n",
    "    \"\"\"\n",
    "    clear the laps in clearidx\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    ret_samples = copy.deepcopy(full_samples)\n",
    "    ret_tss = copy.deepcopy(full_tss)\n",
    "    \n",
    "    \n",
    "    carlist = full_tss.keys()\n",
    "    \n",
    "    for carid, carno in enumerate(carlist):\n",
    "        forecast = ret_samples[carno]\n",
    "        target = ret_tss[carno]\n",
    "        \n",
    "        forecast[:, clearidx] = np.nan\n",
    "        target[clearidx] = np.nan\n",
    "        \n",
    "        ret_samples[carno] = forecast\n",
    "        ret_tss[carno] = target\n",
    "        \n",
    "    return ret_samples, ret_tss\n",
    "\n",
    "def do_rerank(dfout, short=True):\n",
    "    \"\"\"\n",
    "    carno','startlap','startrank','endrank','diff','sign','pred_endrank','pred_diff','pred_sign','endlap','pred_endlap\n",
    "    \n",
    "    output of prediction of target can be float\n",
    "    \n",
    "    resort the endrank globally\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cols=['carno','startlap','startrank','endrank','diff','sign','pred_endrank','pred_diff','pred_sign','endlap','pred_endlap']\n",
    "    colid={x:id for id,x in enumerate(cols)}\n",
    "    \n",
    "    #df = dfout.sort_values(by=['startlap','carno'])\n",
    "    print('rerank...')\n",
    "    laps = set(dfout.startlap.values)\n",
    "    \n",
    "    dfs = []\n",
    "    for lap in laps:\n",
    "        df = dfout[dfout['startlap']==lap].to_numpy()\n",
    "        \n",
    "        #print('in',df)\n",
    "        \n",
    "        idx = np.argsort(df[:,colid['pred_endrank']], axis=0)\n",
    "        true_rank = np.argsort(idx, axis=0)\n",
    "    \n",
    "        df[:,colid['pred_endrank']] = true_rank\n",
    "        \n",
    "        #reset preds \n",
    "        df[:,colid['pred_diff']] = df[:,colid['pred_endrank']] - df[:,colid['endrank']]\n",
    "\n",
    "        for rec in df:\n",
    "            if rec[colid['pred_diff']] == 0:\n",
    "                rec[colid['pred_sign']] = 0\n",
    "            elif rec[colid['pred_diff']] > 0:\n",
    "                rec[colid['pred_sign']] = 1\n",
    "            else:\n",
    "                rec[colid['pred_sign']] = -1        \n",
    "        \n",
    "        #print('out',df)\n",
    "        if len(dfs) == 0:\n",
    "            dfs = df\n",
    "        else:\n",
    "            dfs = np.vstack((dfs, df))\n",
    "        #dfs.append(df)\n",
    "        #np.vstack(df)\n",
    "        \n",
    "    #dfret = pd.concat(dfs)\n",
    "    #data = np.array(dfs)\n",
    "    if short:\n",
    "        dfret = pd.DataFrame(dfs.astype(int), columns = cols[:-2])\n",
    "    else:\n",
    "        dfret = pd.DataFrame(dfs.astype(int), columns = cols)\n",
    "    return dfret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_predict_bymloutput_multirun(output, dfin, sampleCnt=100):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        test_ds\n",
    "        predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_start(idx):\n",
    "        td = forecasts[idx].start_date - start_time\n",
    "        return td.days*24*60 + td.seconds//60\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor=_predictor,  # predictor\n",
    "        num_samples=sampleCnt,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    print(f'tss len={len(tss)}, forecasts len={len(forecasts)}')\n",
    "    \n",
    "    start_time, row = next(tss[0].iterrows())\n",
    "\n",
    "    first_start = get_start(-1)\n",
    "    last_start = get_start(0)\n",
    "    print('first start:', first_start, 'last start:', last_start)    \n",
    "    \n",
    "    import copy\n",
    "    target = copy.deepcopy(forecasts[-1])\n",
    "\n",
    "    #100, 10\n",
    "    nsample, npredict = target.samples.shape\n",
    "    print('sampel# x predictlen: ', nsample, npredict)\n",
    "    \n",
    "    newsamples = np.zeros((nsample, last_start - first_start + npredict))\n",
    "    newsamples[:,:] = np.nan\n",
    "    \n",
    "    for idx in range(len(forecasts)):\n",
    "        #copy samples\n",
    "        start_pos = get_start(idx)\n",
    "\n",
    "        pos = start_pos - first_start + npredict - 1\n",
    "        #copy sample to block\n",
    "        #newsamples[:, pos:pos + npredict] = forecasts[idx].samples\n",
    "        #newsamples[:, pos + npredict - 1] = forecasts[idx].samples[:,-1]\n",
    "        \n",
    "        # get prediction from ml output\n",
    "        # pos = laps\n",
    "        # 1 ... 10 | 11 <- start pos in forecasts\n",
    "        # 0 ...  9 | 10 <- 9 is the startlap\n",
    "        #\n",
    "        startlap = start_pos  - 2\n",
    "        #print('start pos:', start_pos, 'pos:',pos, 'startlap:', startlap)\n",
    "        \n",
    "        _rec = dfin[dfin['startlap']== startlap]\n",
    "        if len(_rec) > 0:\n",
    "            # rank start from 1 for visualization\n",
    "            pred_val = _rec.pred_endrank.values\n",
    "            \n",
    "            #pred_val = _rec.pred_endrank.values\n",
    "            #make sure shape match, 100 samples\n",
    "            \n",
    "            #newsamples[:, pos + npredict - 1] = pred_val + 1\n",
    "            newsamples[:, pos] = pred_val + 1\n",
    "            #print('startlap:', startlap, 'predrank:', pred_val)\n",
    "\n",
    "    target.samples = newsamples\n",
    "    \n",
    "    print('multirun target samples:', target.samples.shape)\n",
    "\n",
    "    #plot_prob_forecasts_ex([tss[0]],[target],output)\n",
    "    \n",
    "    return target,tss[0]\n",
    "\n",
    "def long_predict_bymloutput(output, dfin):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        test_ds\n",
    "        predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_start(idx):\n",
    "        td = forecasts[idx].start_date - start_time\n",
    "        return td.days*24*60 + td.seconds//60\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor=_predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    print(f'tss len={len(tss)}, forecasts len={len(forecasts)}')\n",
    "    \n",
    "    start_time, row = next(tss[0].iterrows())\n",
    "\n",
    "    first_start = get_start(-1)\n",
    "    last_start = get_start(0)\n",
    "    print('first start:', first_start, 'last start:', last_start)    \n",
    "    \n",
    "    import copy\n",
    "    target = copy.deepcopy(forecasts[-1])\n",
    "\n",
    "    #100, 10\n",
    "    nsample, npredict = target.samples.shape\n",
    "    print('sampel# x predictlen: ', nsample, npredict)\n",
    "    \n",
    "    newsamples = np.zeros((nsample, last_start - first_start + npredict))\n",
    "    newsamples[:,:] = np.nan\n",
    "    \n",
    "    for idx in range(len(forecasts)):\n",
    "        #copy samples\n",
    "        start_pos = get_start(idx)\n",
    "\n",
    "        pos = start_pos - first_start + npredict - 1\n",
    "        #copy sample to block\n",
    "        #newsamples[:, pos:pos + npredict] = forecasts[idx].samples\n",
    "        #newsamples[:, pos + npredict - 1] = forecasts[idx].samples[:,-1]\n",
    "        \n",
    "        # get prediction from ml output\n",
    "        # pos = laps\n",
    "        # 1 ... 10 | 11 <- start pos in forecasts\n",
    "        # 0 ...  9 | 10 <- 9 is the startlap\n",
    "        #\n",
    "        startlap = start_pos  - 2\n",
    "        #print('start pos:', start_pos, 'pos:',pos, 'startlap:', startlap)\n",
    "        \n",
    "        _rec = dfin[dfin['startlap']== startlap]\n",
    "        if len(_rec) > 0:\n",
    "            # rank start from 1 for visualization\n",
    "            pred_val = _rec.pred_endrank.values[0]\n",
    "            \n",
    "            #pred_val = _rec.pred_endrank.values\n",
    "            #make sure shape match, 100 samples\n",
    "            \n",
    "            #newsamples[:, pos + npredict - 1] = pred_val + 1\n",
    "            newsamples[:, pos] = pred_val + 1\n",
    "            #print('startlap:', startlap, 'predrank:', pred_val)\n",
    "\n",
    "    target.samples = newsamples\n",
    "    \n",
    "    print('target samples:', target.samples.shape)\n",
    "\n",
    "    #plot_prob_forecasts_ex([tss[0]],[target],output)\n",
    "    \n",
    "    return target,tss[0]\n",
    "\n",
    "def long_predict_bysamples(output, samples, tss):\n",
    "    \"\"\"\n",
    "    use the farest samples only\n",
    "    \n",
    "    input:\n",
    "        samples\n",
    "        tss\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def get_start(idx):\n",
    "        td = forecasts[idx].start_date - start_time\n",
    "        return td.days*24*60 + td.seconds//60\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor=_predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    print(f'tss len={len(tss)}, forecasts len={len(forecasts)}')\n",
    "    \n",
    "    start_time, row = next(tss[0].iterrows())\n",
    "\n",
    "    first_start = get_start(-1)\n",
    "    last_start = get_start(0)\n",
    "    print(first_start, last_start)    \n",
    "    \n",
    "    import copy\n",
    "    target = copy.deepcopy(forecasts[-1])\n",
    "\n",
    "    #100, 10\n",
    "    nsample, npredict = target.samples.shape\n",
    "    print('sampel# x predictlen: ', nsample, npredict)\n",
    "    \n",
    "    #sample array size: last_start - first_start + npredict\n",
    "    arraysize = last_start - first_start + npredict\n",
    "    \n",
    "    #error here\n",
    "    #target.samples = samples[:,-len(forecasts)-1:] + 1\n",
    "    #target.samples = samples[:, 10 + npredict:] + 1\n",
    "    target.samples = samples[:, first_start:first_start + arraysize] + 1\n",
    "\n",
    "    print('long_predict_bysamples==>target samples shape:', target.samples.shape)\n",
    "    #plot_prob_forecasts_ex([tss[0]],[target],output)\n",
    "    \n",
    "    return target, tss[0]\n",
    "\n",
    "#\n",
    "# different idx format to bymloutput\n",
    "#\n",
    "def long_predict_bydf(output, dfin):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        test_ds\n",
    "        predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_start(idx):\n",
    "        td = forecasts[idx].start_date - start_time\n",
    "        return td.days*24*60 + td.seconds//60\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor= _predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    print(f'tss len={len(tss)}, forecasts len={len(forecasts)}')\n",
    "    \n",
    "    start_time, row = next(tss[0].iterrows())\n",
    "\n",
    "    first_start = get_start(-1)\n",
    "    last_start = get_start(0)\n",
    "    print('first start:', first_start, 'last start:', last_start)    \n",
    "    \n",
    "    import copy\n",
    "    target = copy.deepcopy(forecasts[-1])\n",
    "\n",
    "    #100, 10\n",
    "    nsample, npredict = target.samples.shape\n",
    "    print('sampel# x predictlen: ', nsample, npredict)\n",
    "    \n",
    "    newsamples = np.zeros((nsample, last_start - first_start + npredict))\n",
    "    newsamples[:,:] = np.nan\n",
    "    \n",
    "    for idx in range(len(forecasts)):\n",
    "        #copy samples\n",
    "        start_pos = get_start(idx)\n",
    "\n",
    "        pos = start_pos - first_start + npredict - 1\n",
    "        #copy sample to block\n",
    "        #newsamples[:, pos:pos + npredict] = forecasts[idx].samples\n",
    "        #newsamples[:, pos + npredict - 1] = forecasts[idx].samples[:,-1]\n",
    "        \n",
    "        # get prediction from ml output\n",
    "        # pos = laps\n",
    "        # 1 ... 10 | 11 <- start pos in forecasts\n",
    "        # 0 ...  9 | 10 <- 9 is the startlap\n",
    "        #\n",
    "        startlap = start_pos  - 1\n",
    "        #print('start pos:', start_pos, 'pos:',pos, 'startlap:', startlap)\n",
    "        \n",
    "        _rec = dfin[dfin['startlap']== startlap]\n",
    "        if len(_rec) > 0:\n",
    "            # rank start from 1 for visualization\n",
    "            pred_val = _rec.pred_endrank.values[0]\n",
    "            \n",
    "            #pred_val = _rec.pred_endrank.values\n",
    "            #make sure shape match, 100 samples\n",
    "            \n",
    "            #newsamples[:, pos + npredict - 1] = pred_val + 1\n",
    "            newsamples[:, pos] = pred_val + 1\n",
    "            #print('startlap:', startlap, 'predrank:', pred_val)\n",
    "\n",
    "    target.samples = newsamples\n",
    "    \n",
    "    print('target samples:', target.samples.shape)\n",
    "\n",
    "    #plot_prob_forecasts_ex([tss[0]],[target],output)\n",
    "    \n",
    "    return target,tss[0]\n",
    "\n",
    "def get_ranknet_multirun(retdata, testcar, sampleCnt=100):\n",
    "    dfs = []\n",
    "    #for id in range(samplecnt):\n",
    "    for id in retdata.keys():\n",
    "        #ret['pitmodel-RANK-2018-inlap-nopitage']\n",
    "        df = retdata[id][0]\n",
    "        df = df[df['carno']==testcar]\n",
    "        dfs.append(df)\n",
    "\n",
    "    dfin_ranknet = pd.concat(dfs)\n",
    "\n",
    "    print('dfin_ranknet size:', len(dfin_ranknet))\n",
    "    \n",
    "    #modify to fit to ml model format\n",
    "    dfin_ranknet['startlap'] = dfin_ranknet['startlap'] - 1\n",
    "    dfin_ranknet['startrank'] = dfin_ranknet['startrank'] - 1\n",
    "    dfin_ranknet['endrank'] = dfin_ranknet['endrank'] - 1\n",
    "                \n",
    "    target_ranknet, tss_ranknet = long_predict_bymloutput_multirun('ranknet-rank', dfin_ranknet, sampleCnt=sampleCnt)                \n",
    "                \n",
    "    return target_ranknet, tss_ranknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ploth(ts_entry, forecast_entry, pits,caution, pitstop,outputfile,\n",
    "                   colors = ['r','g','m'],\n",
    "                   plabels= ['observed','svr','arima','ranknet'],\n",
    "                   ylabel = 'RANK'):\n",
    "\n",
    "    #plot_length = int(forecast_entry[0].samples.shape[1] *1.2) \n",
    "    #plot_length = forecast_entry[0].samples.shape[1] + 10 \n",
    "    \n",
    "    #prediction_intervals = (50.0, 90.0)\n",
    "    prediction_intervals = [90.0]\n",
    "    \n",
    "    #legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals]\n",
    "\n",
    "    figcnt = len(forecast_entry)\n",
    "    \n",
    "    #fig, axs = plt.subplots(figcnt,1, figsize=(8,6))\n",
    "    fig, axs = plt.subplots(1, figcnt, figsize=(12,3*figcnt))\n",
    "\n",
    "    #colors = ['r','g','m']\n",
    "    #plabels = ['observed','svr','arima','ranknet']\n",
    "    \n",
    "    for idx in range(figcnt):\n",
    "        ax = plt.subplot(figcnt, 1, idx+1)\n",
    "        #ax = plt.subplot(1, figcnt, idx+1)\n",
    "        #ts_entry.iloc[-plot_length:,0].plot(ax=axs, linewidth=1)  # plot the time series\n",
    "        #ts_entry.iloc[-plot_length:,0].plot(ax=axs[idx], linewidth=1)  # plot the time series\n",
    "        #plot_length = int(forecast_entry[idx].samples.shape[1] *1.2) \n",
    "        ts_entry[idx].iloc[:,0].plot(linewidth=1, color='b',\n",
    "                                            marker='*', alpha=0.7, zorder=-1, label=plabels[0]) \n",
    "\n",
    "\n",
    "        # currank\n",
    "        sv = ts_entry[idx].iloc[:,0].to_numpy()\n",
    "        start = pd.Timestamp(\"01-01-2019\", freq='1min') + 2\n",
    "        date_index = pd.date_range(start, periods = len(sv)-2, freq='1min')\n",
    "        df2 = pd.DataFrame(sv[:-2], index=date_index)        \n",
    "        df2.iloc[:,0].plot(linewidth=0.5, color='k',\n",
    "                                            marker='+', alpha=0.7, zorder=-1, label='CurRank') \n",
    "        \n",
    "        \n",
    "    #for idx in range(len(forecast_entry)):\n",
    "    #    forecast_entry[idx].copy_dim(0).plot(prediction_intervals=prediction_intervals, color='g')\n",
    "    \n",
    "        forecast_entry[idx].copy_dim(0).plot(prediction_intervals=prediction_intervals, \n",
    "                                             color=colors[idx],label=plabels[idx+1], zorder=10)\n",
    "        #forecast_entry[1].copy_dim(0).plot(prediction_intervals=prediction_intervals, color='b')\n",
    "        #forecast_entry[2].copy_dim(0).plot(prediction_intervals=prediction_intervals, color='r')\n",
    "        \n",
    "        #add mean line, compare with median\n",
    "        #if forecast_entry[idx].samples.shape[0] > 1:\n",
    "        if idx>3:\n",
    "            mean_forecast = copy.deepcopy(forecast_entry[idx])\n",
    "            mean_forecast.samples = np.mean(mean_forecast.samples, axis=0).reshape((1,-1))\n",
    "            mean_forecast.copy_dim(0).plot(prediction_intervals=prediction_intervals, \n",
    "                                                 color='g',label='use-mean', zorder=10)\n",
    "        \n",
    "        \n",
    "        if idx == figcnt-1:\n",
    "            ax.set_xlabel('Lap')\n",
    "        #if idx==0:\n",
    "        ax.set_ylabel(ylabel)\n",
    "        if idx==0:\n",
    "            plt.title(outputfile)        \n",
    "    \n",
    "        locs, labels = plt.xticks() \n",
    "        #plt.xticks(locs, range(len(locs)))\n",
    "        start_loc = locs[0]        \n",
    "        offset = range(0, 200, 5)\n",
    "        #new_locs = range(start_loc , start_loc+200, 10)\n",
    "        new_locs = [start_loc + x for x in offset]\n",
    "        #new_labels = [str(x-start_loc + 1) for x in new_locs]\n",
    "        new_labels = [str(x+1) for x in offset]\n",
    "        plt.xticks(new_locs, new_labels)\n",
    "\n",
    "        if figcnt==1 or idx < figcnt -1:\n",
    "            print('xlim:', plt.xlim())\n",
    "            xl, xr = plt.xlim()\n",
    "            xlim_h = len(ts_entry[idx])\n",
    "            \n",
    "            #xlim_h = 100\n",
    "            ax.set_xlim((xl+0,xl+xlim_h))\n",
    "        elif idx == figcnt - 1:\n",
    "            xlim_h = len(ts_entry[idx])\n",
    "            \n",
    "            #xlim_h = 100\n",
    "            ax.set_xlim((xl+0,xl+xlim_h))\n",
    "        \n",
    "        if ylabel=='RANK':\n",
    "            ax.set_ylim((-5,+40))\n",
    "        else:\n",
    "            ax.set_ylim((25,175))\n",
    "            \n",
    "        #ax.set_xlim((80,110))\n",
    "        ax.set_zorder(-1)\n",
    "        plt.grid(which=\"both\", zorder=-1)\n",
    "        ax.set_axisbelow(True)\n",
    "        \n",
    "        l=plt.legend(prop={'size': 10},loc='upper left')\n",
    "        l.set_zorder(0.6)\n",
    "        \n",
    "        #add racestatus\n",
    "        if ylabel=='RANK':\n",
    "            ax.plot(xl+pits[:,0]-1,pits[:,1],'^',color='r', label='PitStop', linewidth=2,alpha=0.7, zorder=-1)\n",
    "            add_status(ax,xl, caution, pitstop)\n",
    "        else:\n",
    "            ax.plot(xl+pits[:,0]-1,pits[:,2],'^',color='r', label='PitStop', linewidth=2,alpha=0.7, zorder=-1)\n",
    "            add_status(ax,xl, caution, pitstop,y=27, height=3)\n",
    "        \n",
    "        \n",
    "    \n",
    "    plt.show()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outputfile + '.pdf')    \n",
    "    \n",
    "def plotcar(carno):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        alldata, rankdata; global data\n",
    "    \"\"\"\n",
    "    #target_svr, target_rf,target_arima, target_oracle, target_ranknet_1run = savedata[carno]\n",
    "    #target_oracle(by longpredict), tss_oracle_multirun,tss_ranknet_multirun\n",
    "    tsss, targets = alldata[carno]\n",
    "    \n",
    "    pits, cautions, caution, pitstop,ranks,laptimes = get_racestatus(carno, rankdata)\n",
    "    print(np.where(pitstop==1))\n",
    "    \n",
    "    ploth(tsss[:5], targets[:5], pits, caution, pitstop,\n",
    "               'ranknet-rf-rank-forecast-%d'%carno,\n",
    "                   colors = ['y','c','g','m','r'],\n",
    "                   plabels= ['observed','SVR','RF','Arima','RrankNet-Oracle','RrankNet-MLP'])\n",
    "    \n",
    "def plotcar_laptime(carno):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        alldata, rankdata; global data\n",
    "    \"\"\"\n",
    "    #target_svr, target_rf,target_arima, target_oracle, target_ranknet_1run = savedata[carno]\n",
    "    #target_oracle(by longpredict), tss_oracle_multirun,tss_ranknet_multirun\n",
    "    tsss, targets = alldata[carno]\n",
    "    \n",
    "    pits, cautions, caution, pitstop,ranks,laptimes = get_racestatus(carno, rankdata)\n",
    "    print(np.where(pitstop==1))\n",
    "    \n",
    "    ploth(tsss, targets, pits, caution, pitstop,\n",
    "               'ranknet-oracle-laptime-forecast-%d'%carno,\n",
    "                   colors = ['m','r'],\n",
    "                   plabels= ['observed','RrankNet-Oracle','RrankNet-MLP'],\n",
    "                ylabel='LapTime')\n",
    "    \n",
    "    \n",
    "def plotrank(outputfile, mode='RANK' ):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        alldata, rankdata; global data\n",
    "    \"\"\"\n",
    "    \n",
    "    figcnt = len(alldata)\n",
    "    fig, axs = plt.subplots(1, figcnt, figsize=(12,3*figcnt))\n",
    "\n",
    "    carlist = list(alldata.keys())\n",
    "    \n",
    "    for idx, carno in enumerate(carlist):\n",
    "        #target_svr, target_rf,target_arima, target_oracle, target_ranknet_1run = savedata[carno]\n",
    "        #target_oracle(by longpredict), tss_oracle_multirun,tss_ranknet_multirun\n",
    "        tsss, targets = alldata[carno]\n",
    "\n",
    "        pits, cautions, caution, pitstop,ranks,laptimes = get_racestatus(carno, rankdata)\n",
    "        print(np.where(pitstop==1))\n",
    "    \n",
    "        ax = plt.subplot(figcnt, 1, idx+1)\n",
    "        \n",
    "        if mode == 'RANK':\n",
    "            ax.plot(ranks, linewidth=1, color='b',marker='*', alpha=0.7, zorder=-1, label='Rank') \n",
    "            ax.set_ylim((-5,+35))\n",
    "            ax.plot(pits[:,0]-1,pits[:,1],'^',color='r', label='PitStop', linewidth=2,alpha=0.7, zorder=-1)\n",
    "            #add racestatus\n",
    "            add_status(ax,0, caution, pitstop)\n",
    "\n",
    "        else:\n",
    "            ax.plot(laptimes, linewidth=1, color='b',marker='*', alpha=0.7, zorder=-1, label='LapTime') \n",
    "            ax.set_ylim((30,140))\n",
    "            ax.plot(pits[:,0]-1,pits[:,2],'^',color='r', label='PitStop', linewidth=2,alpha=0.7, zorder=-1)\n",
    "            #add racestatus\n",
    "            add_status(ax,0, caution, pitstop,y=32, height=5)\n",
    "        \n",
    "        ax.set_xlim((0,200))\n",
    "        \n",
    "        ax.set_ylabel('car-%d'%carno)\n",
    "        \n",
    "        \n",
    "    plt.show()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outputfile + '.pdf')    \n",
    "    \n",
    "def plotcarx(carno):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        alldata, rankdata; global data\n",
    "    \"\"\"\n",
    "    #target_svr, target_rf,target_arima, target_oracle, target_ranknet_1run = savedata[carno]\n",
    "    #target_oracle(by longpredict), tss_oracle_multirun,tss_ranknet_multirun\n",
    "    tsss, targets = alldata[carno]\n",
    "    \n",
    "    oracle_tss, oracle_targets = oracledata[carno]\n",
    "    \n",
    "    tsss[2] = oracle_tss[1]\n",
    "    targets[2] = oracle_targets[1]\n",
    "    \n",
    "    pits, cautions, caution, pitstop,ranks,laptimes = get_racestatus(carno, rankdata)\n",
    "    print(np.where(pitstop==1))\n",
    "    \n",
    "    ploth(tsss[:5], targets[:5], pits, caution, pitstop,\n",
    "               'ranknet-rf-rank-forecast-%d'%carno,\n",
    "                   colors = ['y','c','g','m','r'],\n",
    "                   plabels= ['observed','SVR','RF','Weighted-Oracle','RrankNet-Oracle','RrankNet-MLP'])\n",
    "    \n",
    "    \n",
    "def plotoracle(alldata, carno, destdir):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        alldata, rankdata; global data\n",
    "    \"\"\"\n",
    "    \n",
    "    outputfile = destdir + 'ranknet-oracle-forecast-%d'%carno\n",
    "    \n",
    "    #target_svr, target_rf,target_arima, target_oracle, target_ranknet_1run = savedata[carno]\n",
    "    #target_oracle(by longpredict), tss_oracle_multirun,tss_ranknet_multirun\n",
    "    tsss, targets = alldata[carno]\n",
    "    \n",
    "    pits, cautions, caution, pitstop,ranks,laptimes = get_racestatus(carno, rankdata)\n",
    "    print(np.where(pitstop==1))\n",
    "    \n",
    "    ploth(tsss, targets, pits, caution, pitstop,\n",
    "               outputfile,\n",
    "               colors = ['y','c','g','m','r'],\n",
    "               plabels= ['observed','1run-samples','1run-df','multimean','norerank-multimean','mrun-samples'])    \n",
    "    \n",
    "\n",
    "def plotallcars(alldata, outputfile, drawid = 0, \n",
    "               colors = ['g','c','m','r','y'],\n",
    "               plabels= ['observed','1run-samples','1run-df','multimean','norerank-multimean','mrun-samples'],\n",
    "               ylabel='RANK'):\n",
    "    \"\"\"\n",
    "    plot a single fig for all cars\n",
    "    \n",
    "    input:\n",
    "        prediction_length,freq   ; global var\n",
    "        alldata, rankdata; global data\n",
    "        drawid : long prediction result index in alldata[carno] to draw\n",
    "    \"\"\"\n",
    "    figcnt = len(alldata)\n",
    "    fig, axs = plt.subplots(1, figcnt, figsize=(12,3*figcnt))\n",
    "    prediction_intervals = [90.0]\n",
    "    #legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals][::-1]\n",
    "    legend = [\"observations\", \"median prediction\"] + [f\"{k}% prediction interval\" for k in prediction_intervals]\n",
    "\n",
    "    font = {'family': 'serif',\n",
    "        'color':  'darkred',\n",
    "        'weight': 'normal',\n",
    "        'size': 12,\n",
    "        }    \n",
    "    \n",
    "    carlist = list(alldata.keys())\n",
    "    \n",
    "    for idx, carno in enumerate(carlist):\n",
    "        #target_svr, target_rf,target_arima, target_oracle, target_ranknet_1run = savedata[carno]\n",
    "        #target_oracle(by longpredict), tss_oracle_multirun,tss_ranknet_multirun\n",
    "        ts_entry, forecast_entry = alldata[carno]\n",
    "\n",
    "        pits, cautions, caution, pitstop,ranks,laptimes = get_racestatus(carno, rankdata)\n",
    "        print(np.where(pitstop==1))\n",
    "    \n",
    "        ax = plt.subplot(figcnt, 1, idx+1)\n",
    "    \n",
    "        # observed\n",
    "        ts_entry[drawid].iloc[:,0].plot(linewidth=1, color='b',\n",
    "                                            marker='*', alpha=0.7, zorder=-1, label=plabels[0]) \n",
    "\n",
    "\n",
    "        # currank\n",
    "        sv = ts_entry[drawid].iloc[:,0].to_numpy()\n",
    "        start = pd.Timestamp(\"01-01-2019\", freq=freq) + prediction_length\n",
    "        date_index = pd.date_range(start, periods = len(sv)-prediction_length, freq=freq)\n",
    "        df2 = pd.DataFrame(sv[:-prediction_length], index=date_index)        \n",
    "        df2.iloc[:,0].plot(linewidth=0.5, color='k',\n",
    "                            marker='+', alpha=0.7, zorder=-1, label='CurRank') \n",
    "        \n",
    "        #forecast\n",
    "        forecast_entry[drawid].copy_dim(0).plot(prediction_intervals=prediction_intervals, \n",
    "                                             color=colors[drawid],label=plabels[drawid+1], zorder=10)\n",
    "        \n",
    "        if idx == figcnt-1:\n",
    "            ax.set_xlabel('Lap')\n",
    "        ax.set_ylabel(ylabel)\n",
    "        \n",
    "    \n",
    "        locs, labels = plt.xticks() \n",
    "        #plt.xticks(locs, range(len(locs)))\n",
    "        start_loc = locs[0]        \n",
    "        offset = range(0, 200, 5)\n",
    "        #new_locs = range(start_loc , start_loc+200, 10)\n",
    "        new_locs = [start_loc + x for x in offset]\n",
    "        #new_labels = [str(x-start_loc + 1) for x in new_locs]\n",
    "        new_labels = [str(x+1) for x in offset]\n",
    "        plt.xticks(new_locs, new_labels)\n",
    "\n",
    "        if figcnt==1 or idx < figcnt -1:\n",
    "            print('xlim:', plt.xlim())\n",
    "            xl, xr = plt.xlim()\n",
    "            xlim_h = len(ts_entry[drawid])\n",
    "            \n",
    "            #xlim_h = 100\n",
    "            ax.set_xlim((xl+0,xl+xlim_h))\n",
    "        elif idx == figcnt - 1:\n",
    "            xlim_h = len(ts_entry[drawid])\n",
    "            \n",
    "            #xlim_h = 100\n",
    "            ax.set_xlim((xl+0,xl+xlim_h))\n",
    "            \n",
    "        #plt.title(outputfile)        \n",
    "        plt.text(xl + xlim_h - 15, 35, f'car-{carno}',fontdict=font)\n",
    "            \n",
    "        \n",
    "        if ylabel=='RANK':\n",
    "            ax.set_ylim((-5,+40))\n",
    "        else:\n",
    "            ax.set_ylim((25,175))\n",
    "            \n",
    "        #ax.set_xlim((80,110))\n",
    "        ax.set_zorder(-1)\n",
    "        plt.grid(which=\"both\", zorder=-1)\n",
    "        ax.set_axisbelow(True)\n",
    "        \n",
    "        l=plt.legend(prop={'size': 10},loc='upper left')\n",
    "        l.set_zorder(0.6)\n",
    "        \n",
    "        #add racestatus\n",
    "        if ylabel=='RANK':\n",
    "            ax.plot(xl+pits[:,0]-1,pits[:,1],'^',color='r', label='PitStop', linewidth=2,alpha=0.7, zorder=-1)\n",
    "            add_status(ax,xl, caution, pitstop)\n",
    "        else:\n",
    "            ax.plot(xl+pits[:,0]-1,pits[:,2],'^',color='r', label='PitStop', linewidth=2,alpha=0.7, zorder=-1)\n",
    "            add_status(ax,xl, caution, pitstop,y=27, height=3)\n",
    "        \n",
    "        \n",
    "    \n",
    "    plt.show()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outputfile + '.pdf')      \n",
    "    \n",
    "    \n",
    "def get_racestatus(carno, rankdata):\n",
    "    df12 = rankdata[rankdata['car_number']==carno]\n",
    "    #\n",
    "    # completed_laps start from 0\n",
    "    # in array mode completed_laps=1 should indexed by 0\n",
    "    #\n",
    "    data = df12[['completed_laps','rank','last_laptime','time_behind_leader']].values\n",
    "    pitstop = df12[['lap_status']].values\n",
    "    caution = df12[['track_status']].values\n",
    "    pitstop = np.array([1 if x=='P' else 0 for x in pitstop])\n",
    "    caution = np.array([1 if x=='Y' else 0 for x in caution])\n",
    "    pitidx = np.where(pitstop == 1)\n",
    "    pits = data[pitidx]\n",
    "    yidx = np.where(caution == 1)\n",
    "    cautions = data[yidx]\n",
    "    \n",
    "    ranks = df12[['rank']].values\n",
    "    laptimes = df12[['last_laptime']].values\n",
    "\n",
    "    #return pits, cautions, caution, pitstop\n",
    "    return pits, cautions, caution[1:], pitstop[1:], ranks[1:],laptimes[1:]\n",
    "\n",
    "\n",
    "#red = '#ff8080'\n",
    "red = 'red'\n",
    "#yellow = '#8080ff'\n",
    "yellow = 'yellow'\n",
    "#green = '#80ff80'\n",
    "green = 'green'\n",
    "\n",
    "def add_status(axs,xl, caution, pitstop, maxlap= 200, y=-4, height=2):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        caution, pitstop : race status\n",
    "    \"\"\"\n",
    "    maxlap = min(len(caution), len(pitstop))\n",
    "    for lap in range(maxlap):\n",
    "        fc = green\n",
    "        if caution[lap] == 1:\n",
    "            fc = yellow\n",
    "        if pitstop[lap] == 1:\n",
    "            fc = red\n",
    "        ec = fc\n",
    "        rectangle = plt.Rectangle((lap+xl-0.5,y), 1, height, fc=fc,ec=ec)\n",
    "        #plt.gca().add_patch(rectangle)\n",
    "        axs.add_patch(rectangle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# stint evaluation\n",
    "#\n",
    "def eval_bydf(testdf, bydf, forcematch=True, force2int=False):\n",
    "    #collect only records in bydf <carno and startlap>\n",
    "    cars = set(bydf.carno.values)\n",
    "    startlaps = {}\n",
    "    for car in cars:\n",
    "        startlaps[car] = set(bydf[bydf['carno']==car].startlap.values)\n",
    "        \n",
    "    retdf = []\n",
    "    for car in cars:\n",
    "        for startlap in startlaps[car]:    \n",
    "            dfrec = testdf[(testdf['carno']==car) & (testdf['startlap']==startlap)]\n",
    "            \n",
    "            #check match\n",
    "            if forcematch:\n",
    "                a = testdf[(testdf['carno']==car) & (testdf['startlap']==startlap)].to_numpy().astype(int)\n",
    "                b = bydf[(bydf['carno']==car) & (bydf['startlap']==startlap)].to_numpy().astype(int)\n",
    "\n",
    "                if len(a)!=0 and len(b)!=0:\n",
    "                    #compare \n",
    "                    #startrank, endrank\n",
    "                    if not ((a[0][2] == b[0][2]) and (a[0][3] == b[0][3])):\n",
    "                        #print('mismatch:', a, b)            \n",
    "                        continue\n",
    "            \n",
    "            retdf.append(dfrec)\n",
    "        \n",
    "    dfout = pd.concat(retdf)\n",
    "    \n",
    "    if force2int:\n",
    "        dfdata = dfout.to_numpy().astype(int)\n",
    "        dfout = pd.DataFrame(dfdata, columns =['carno', 'startlap', 'startrank',    \n",
    "                                         'endrank', 'diff', 'sign',\n",
    "                                         'pred_endrank', 'pred_diff', 'pred_sign',\n",
    "                                         'endlap','pred_endlap'\n",
    "                                        ])\n",
    "    \n",
    "    dfout = dfout.sort_values(by=['carno','startlap'])\n",
    "    \n",
    "    print('df size:', len(dfout))\n",
    "    #return acc\n",
    "    accret = stint.get_evalret(dfout)[0]\n",
    "    \n",
    "    return dfout  , accret\n",
    "\n",
    "\n",
    "def eval_sync(testdf, errlist, force2int=False):\n",
    "    \"\"\"\n",
    "    eval df result by sync with the errlist detected\n",
    "    remove the records in errlist\n",
    "    \n",
    "    \"\"\"\n",
    "    #collect only records in bydf <carno and startlap>\n",
    "    cars = set(testdf.carno.values)\n",
    "    startlaps = {}\n",
    "    for car in cars:\n",
    "        startlaps[car] = set(testdf[testdf['carno']==car].startlap.values)\n",
    "        \n",
    "    retdf = []\n",
    "    for car in cars:\n",
    "        for startlap in startlaps[car]:    \n",
    "            dfrec = testdf[(testdf['carno']==car) & (testdf['startlap']==startlap)]\n",
    "            \n",
    "            #check match\n",
    "            this_rec = [car, startlap]\n",
    "            if this_rec in errlist:\n",
    "                continue\n",
    "            \n",
    "            retdf.append(dfrec)\n",
    "        \n",
    "    dfout = pd.concat(retdf)\n",
    "    \n",
    "    if force2int:\n",
    "        dfdata = dfout.to_numpy().astype(int)\n",
    "        dfout = pd.DataFrame(dfdata, columns =['carno', 'startlap', 'startrank',    \n",
    "                                         'endrank', 'diff', 'sign',\n",
    "                                         'pred_endrank', 'pred_diff', 'pred_sign',\n",
    "                                         'endlap','pred_endlap'\n",
    "                                        ])\n",
    "    \n",
    "    dfout = dfout.sort_values(by=['carno','startlap'])\n",
    "    \n",
    "    print('df size:', len(dfout))\n",
    "    #return acc\n",
    "    accret = stint.get_evalret(dfout)[0]\n",
    "    \n",
    "    return dfout  , accret\n",
    "\n",
    "def cmp_df(testdf, bydf):\n",
    "    \"\"\"\n",
    "    df can be different, minor difference for the rank when RankNet removes short ts\n",
    "    \"\"\"\n",
    "    #collect only records in bydf <carno and startlap>\n",
    "    cars = set(bydf.carno.values)\n",
    "    startlaps = {}\n",
    "    for car in cars:\n",
    "        startlaps[car] = set(bydf[bydf['carno']==car].startlap.values)\n",
    "        \n",
    "    err_list = []\n",
    "    retdf = []\n",
    "    errcnt = 0\n",
    "    for car in cars:\n",
    "        for startlap in startlaps[car]:    \n",
    "            a = testdf[(testdf['carno']==car) & (testdf['startlap']==startlap)].to_numpy().astype(int)\n",
    "            b = bydf[(bydf['carno']==car) & (bydf['startlap']==startlap)].to_numpy().astype(int)\n",
    "            \n",
    "            if len(a)!=0 and len(b)!=0:\n",
    "                #compare \n",
    "                #startrank, endrank\n",
    "                if not ((a[0][2] == b[0][2]) and (a[0][3] == b[0][3])):\n",
    "                    print('mismatch:', a, b)\n",
    "                    errcnt += 1\n",
    "                    err_list.append([car, startlap])\n",
    "            else:\n",
    "                errcnt += 1\n",
    "                print('mismatch empty:', a, b)\n",
    "                err_list.append([car, startlap])\n",
    "                \n",
    "                \n",
    "    print('errcnt:', errcnt)\n",
    "    return errcnt, err_list\n",
    "\n",
    "def df2samples(dfall, prediction_len=2, samplecnt=1):\n",
    "    \"\"\"\n",
    "    convert a df into <samples, tss> format\n",
    "    \n",
    "    this version works for the output of ml modles which contains only 1 sample\n",
    "    \"\"\"\n",
    "    carlist = set(dfall.carno.values)\n",
    "    full_samples = {}\n",
    "    full_tss = {}\n",
    "\n",
    "    startlaps = {}\n",
    "    for car in carlist:\n",
    "        startlaps[car] = set(dfall[dfall['carno']==car].startlap.values)\n",
    "        \n",
    "    #empty samples\n",
    "    for carid, carno in enumerate(carlist):\n",
    "        full_tss[carno] = np.zeros((200))\n",
    "        full_tss[carno][:] = np.nan\n",
    "        full_samples[carno] = np.zeros((samplecnt,200))\n",
    "        full_samples[carno][:] = np.nan\n",
    "        \n",
    "        for startlap in startlaps[carno]:\n",
    "            dfrec = dfall[(dfall['carno']==carno) & (dfall['startlap']==startlap)]\n",
    "            \n",
    "            curlap = int(dfrec.startlap.values[0] + prediction_len)\n",
    "            target = dfrec.endrank.values[0]\n",
    "            forecast = dfrec.pred_endrank.values[0]\n",
    "            \n",
    "            for idx in range(samplecnt):\n",
    "                full_samples[carno][idx,curlap] = forecast\n",
    "                \n",
    "            full_tss[carno][curlap] = target\n",
    "    \n",
    "    return full_samples, full_tss\n",
    "\n",
    "def df2samples_ex(dfall, samplecnt=100,errlist=[]):\n",
    "    \"\"\"\n",
    "    for stint results only\n",
    "    \n",
    "    get samples from the runs\n",
    "    \n",
    "    input:\n",
    "        runret  ; list of result df <carno,startlap,startrank,endrank,diff,sign,pred_endrank,pred_diff,pred_sign,endlap,pred_endlap>\n",
    "        errlist ; <car, startlap> list\n",
    "    return:\n",
    "        samples, tss\n",
    "    \"\"\"\n",
    "    #samplecnt = len(runret)\n",
    "    full_samples = {}\n",
    "    full_tss = {}\n",
    "    \n",
    "    carlist = set(dfall.carno.values)\n",
    "    \n",
    "    startlaps = {}\n",
    "    for car in carlist:\n",
    "        startlaps[car] = set(dfall[dfall['carno']==car].startlap.values)\n",
    "        \n",
    "    #empty samples\n",
    "    for carid, carno in enumerate(carlist):\n",
    "        full_tss[carno] = np.zeros((200))\n",
    "        full_tss[carno][:] = np.nan\n",
    "        full_samples[carno] = np.zeros((samplecnt,200))\n",
    "        full_samples[carno][:] = np.nan\n",
    "        \n",
    "        for startlap in startlaps[carno]:\n",
    "            \n",
    "            thisrec = [carno,startlap]\n",
    "            if thisrec in errlist:\n",
    "                continue\n",
    "            \n",
    "            dfrec = dfall[(dfall['carno']==carno) & (dfall['startlap']==startlap)]\n",
    "            \n",
    "            curlap = int(dfrec.startlap.values[0])\n",
    "            target = dfrec.endrank.values[0]\n",
    "            forecast = dfrec.pred_endrank.to_numpy()\n",
    "            \n",
    "            #if carno==12:\n",
    "            #    print('forecast.shape', forecast.shape)\n",
    "            \n",
    "            full_samples[carno][:,curlap] = forecast\n",
    "                \n",
    "            full_tss[carno][curlap] = target\n",
    "    \n",
    "    return full_samples, full_tss \n",
    "\n",
    "def runs2samples(runret, errlist):\n",
    "    \"\"\"\n",
    "    for stint results only\n",
    "    \n",
    "    get samples from the runs\n",
    "    \n",
    "    input:\n",
    "        runret  ; list of result df <carno,startlap,startrank,endrank,diff,sign,pred_endrank,pred_diff,pred_sign,endlap,pred_endlap>\n",
    "        errlist ; <car, startlap> list\n",
    "    return:\n",
    "        samples, tss\n",
    "    \"\"\"\n",
    "    samplecnt = len(runret)\n",
    "    carlist = set(runret[0].carno.values)\n",
    "    full_samples = {}\n",
    "    full_tss = {}\n",
    "    \n",
    "    #concat all dfs\n",
    "    dfall = pd.concat(runret)\n",
    "    \n",
    "    \n",
    "    startlaps = {}\n",
    "    for car in carlist:\n",
    "        startlaps[car] = set(dfall[dfall['carno']==car].startlap.values)\n",
    "        \n",
    "    #empty samples\n",
    "    for carid, carno in enumerate(carlist):\n",
    "        full_tss[carno] = np.zeros((200))\n",
    "        full_tss[carno][:] = np.nan\n",
    "        full_samples[carno] = np.zeros((samplecnt,200))\n",
    "        full_samples[carno][:] = np.nan\n",
    "        \n",
    "        for startlap in startlaps[carno]:\n",
    "            \n",
    "            thisrec = [carno,startlap]\n",
    "            if thisrec in errlist:\n",
    "                continue\n",
    "            \n",
    "            dfrec = dfall[(dfall['carno']==carno) & (dfall['startlap']==startlap)]\n",
    "            \n",
    "            curlap = int(dfrec.startlap.values[0])\n",
    "            target = dfrec.endrank.values[0]\n",
    "            forecast = dfrec.pred_endrank.to_numpy()\n",
    "            \n",
    "            #if carno==12:\n",
    "            #    print('forecast.shape', forecast.shape)\n",
    "            \n",
    "            full_samples[carno][:,curlap] = forecast\n",
    "                \n",
    "            full_tss[carno][curlap] = target\n",
    "    \n",
    "    return full_samples, full_tss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = [\n",
    "        _savedata,\n",
    "        _skip_overwrite,\n",
    "        _inlap_status,\n",
    "        _feature_mode,\n",
    "        _featureCnt,\n",
    "        freq ,\n",
    "        _train_len,\n",
    "        prediction_length,\n",
    "        context_ratio,\n",
    "        context_length,\n",
    "        contextlen,\n",
    "        dataset,\n",
    "        epochs,\n",
    "        gpuid,\n",
    "        _use_weighted_model,\n",
    "        trainmodel,\n",
    "        _use_cate_feature,\n",
    "        use_feat_static,\n",
    "        distroutput,\n",
    "        batch_size,\n",
    "        loopcnt,\n",
    "        _test_event,\n",
    "        testmodel,\n",
    "        pitmodel,\n",
    "        year\n",
    "    ]\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "WorkRootDir = 'QuickTestOutput'\n",
    "#reference\n",
    "#configname = 'weighted-noinlap-nopitage-nocate-c60-drank'\n",
    "#configname = 'weighted-noinlap-S0LTYP0T-nocate-c60-drank-pitmodel'\n",
    "configname = 'weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle'\n",
    "configfile = f'{configname}.ini'\n",
    "\n",
    "if configfile != '':\n",
    "    config = configparser.RawConfigParser()\n",
    "    config.read(WorkRootDir + '/' + configfile)\n",
    "\n",
    "    #set them back\n",
    "    section = \"RankNet-QuickTest\"\n",
    "    \n",
    "    _savedata = config.getboolean(section, \"_savedata\")\n",
    "    _skip_overwrite = config.getboolean(section, \"_skip_overwrite\")\n",
    "    _inlap_status = config.getint(section, \"_inlap_status\") #0\n",
    "    _feature_mode = config.getint(section, \"_feature_mode\") #FEATURE_STATUS\n",
    "    _featureCnt = config.getint(section, \"_featureCnt\") #9\n",
    "    freq = config.get(section, \"freq\") #\"1min\"\n",
    "    _train_len = config.getint(section, \"_train_len\") #40\n",
    "    prediction_length = config.getint(section, \"prediction_length\") #2\n",
    "    context_ratio = config.getfloat(section, \"context_ratio\") #0.\n",
    "    context_length =  config.getint(section, \"context_length\") #40\n",
    "    \n",
    "    dataset= config.get(section, \"dataset\") #'rank'\n",
    "    epochs = config.getint(section, \"epochs\") #1000\n",
    "    gpuid = config.getint(section, \"gpuid\") #5\n",
    "    _use_weighted_model = config.getboolean(section, \"_use_weighted_model\")\n",
    "    trainmodel = config.get(section, \"trainmodel\") #'deepARW-Oracle' if _use_weighted_model else 'deepAR-Oracle'\n",
    "    \n",
    "    _use_cate_feature = config.getboolean(section, \"_use_cate_feature\")\n",
    "    \n",
    "    distroutput = config.get(section, \"distroutput\") #'student'\n",
    "    batch_size = config.getint(section, \"batch_size\") #32\n",
    "    loopcnt = config.getint(section, \"loopcnt\") #2\n",
    "    _test_event = config.get(section, \"_test_event\") #'Indy500-2018'\n",
    "    testmodel = config.get(section, \"testmodel\") #'oracle'\n",
    "    pitmodel = config.get(section, \"pitmodel\") #'oracle'\n",
    "    year = config.get(section, \"year\") #'2018'\n",
    "    \n",
    "    contextlen = context_length\n",
    "    use_feat_static = _use_cate_feature \n",
    "\n",
    "    #config1 = get_config()\n",
    "    \n",
    "else:\n",
    "    print('Warning, please use config file')\n",
    "    sys.exit(0)\n",
    "    \n",
    "    #\n",
    "    # global settings\n",
    "    #\n",
    "    #_savedata = False\n",
    "    _savedata = True\n",
    "    _skip_overwrite = True\n",
    "\n",
    "    #inlap status = \n",
    "    # 0 , no inlap\n",
    "    # 1 , set previous lap\n",
    "    # 2 , set the next lap\n",
    "    _inlap_status = 0\n",
    "\n",
    "    #\n",
    "    # featuremode in [FEATURE_STATUS, FEATURE_PITAGE]:\n",
    "    #\n",
    "    _feature_mode = FEATURE_LEADERPITCNT\n",
    "    _featureCnt = 9\n",
    "\n",
    "    #\n",
    "    # training parameters\n",
    "    #\n",
    "    freq = \"1min\"\n",
    "    _train_len = 60\n",
    "    prediction_length = 2\n",
    "\n",
    "    context_ratio = 0.\n",
    "    context_length =  60\n",
    "    contextlen = context_length\n",
    "\n",
    "    dataset='rank'\n",
    "    epochs = 1000\n",
    "    #epochs = 10\n",
    "    gpuid = 5\n",
    "\n",
    "    #'deepAR-Oracle','deepARW-Oracle'\n",
    "    _use_weighted_model = True\n",
    "    trainmodel = 'deepARW-Oracle' if _use_weighted_model else 'deepAR-Oracle'\n",
    "\n",
    "    _use_cate_feature = False\n",
    "    use_feat_static = _use_cate_feature \n",
    "\n",
    "    distroutput = 'student'\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "    #\n",
    "    # test parameters\n",
    "    #\n",
    "    loopcnt = 2\n",
    "    _test_event = 'Indy500-2018'\n",
    "    testmodel = 'oracle'\n",
    "    pitmodel = 'oracle'\n",
    "    year = '2018'\n",
    "    \n",
    "    #config2 = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current configfile: weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle.ini\n",
      "FEATURE_STATUS FEATURE_LEADER_PITCNT FEATURE_TOTAL_PITCNT FEATURE_SHIFT_TRACKSTATUS FEATURE_SHIFT_LAPSTATUS FEATURE_SHIFT_TOTAL_PITCNT\n",
      "feature_mode: 378 S0LTYP0T\n",
      "testmodel: standard\n",
      "pitmodel: oracle\n",
      "year: 2019\n",
      "test_event: Indy500-2019\n"
     ]
    }
   ],
   "source": [
    "# new added parameters\n",
    "_test_train_len = 40\n",
    "_joint_train = False\n",
    "_pitmodel_bias = 0\n",
    "\n",
    "_test_event = 'Indy500-2019'\n",
    "year = '2019'\n",
    "\n",
    "#shortterm, stint\n",
    "#_forecast_mode = 'stint'\n",
    "_forecast_mode = 'shortterm'\n",
    "\n",
    "# bias of the pitmodel\n",
    "#_pitmodel_bias = 4\n",
    "\n",
    "#train model: [deepARW-Oracle, deepAR]\n",
    "\n",
    "# test the standard deepAR model training and testing\n",
    "\n",
    "# DeepAR\n",
    "trainmodel = 'deepAR'\n",
    "testmodel = 'standard'\n",
    "\n",
    "# Joint \n",
    "#trainmodel = 'deepAR-multi'\n",
    "#testmodel = 'joint'\n",
    "#_joint_train = True\n",
    "#loopcnt = 2\n",
    "\n",
    "if testmodel == 'pitmodel':\n",
    "    testmodel = 'pitmodel%s'%(_pitmodel_bias if _pitmodel_bias!=0 else '')\n",
    "\n",
    "loopcnt = 2    \n",
    "    \n",
    "#featurestr = {FEATURE_STATUS:'nopitage',FEATURE_PITAGE:'pitage',FEATURE_LEADERPITCNT:'leaderpitcnt'}\n",
    "#cur_featurestr = featurestr[_feature_mode]\n",
    "print('current configfile:', configfile)\n",
    "cur_featurestr = decode_feature_mode(_feature_mode)\n",
    "print('feature_mode:', _feature_mode, cur_featurestr)\n",
    "print('testmodel:', testmodel)\n",
    "print('pitmodel:', pitmodel)\n",
    "print('year:', year)\n",
    "print('test_event:', _test_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# string map\n",
    "#\n",
    "inlapstr = {0:'noinlap',1:'inlap',2:'outlap'}\n",
    "weightstr = {True:'weighted',False:'noweighted'}\n",
    "catestr = {True:'cate',False:'nocate'}\n",
    "\n",
    "#\n",
    "# input data parameters\n",
    "#\n",
    "years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "events = [f'Indy500-{x}' for x in years]\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "dbid = f'Indy500_{years[0]}_{years[-1]}_v{_featureCnt}_p{_inlap_status}'\n",
    "_dataset_id = '%s-%s'%(inlapstr[_inlap_status], cur_featurestr)\n",
    "\n",
    "\n",
    "#\n",
    "# internal parameters\n",
    "#\n",
    "distr_outputs ={'student':StudentTOutput(),\n",
    "                'negbin':NegativeBinomialOutput()\n",
    "                }\n",
    "distr_output = distr_outputs[distroutput]\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "experimentid = f'{weightstr[_use_weighted_model]}-{inlapstr[_inlap_status]}-{cur_featurestr}-{catestr[_use_cate_feature]}-c{context_length}'\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "outputRoot = f\"{WorkRootDir}/{experimentid}/\"\n",
    "\n",
    "\n",
    "# standard output file names\n",
    "LAPTIME_DATASET = f'{outputRoot}/laptime_rank_timediff_pit-oracle-{dbid}.pickle' \n",
    "STAGE_DATASET = f'{outputRoot}/stagedata-{dbid}.pickle' \n",
    "# year related\n",
    "SIMULATION_OUTFILE = f'{outputRoot}/{_test_event}/{_forecast_mode}-dfout-{trainmodel}-indy500-{dataset}-{inlapstr[_inlap_status]}-{cur_featurestr}-{testmodel}-l{loopcnt}-alldata.pickle'\n",
    "EVALUATION_RESULT_DF = f'{outputRoot}/{_test_event}/{_forecast_mode}-evaluation_result_d{dataset}_m{testmodel}.csv'\n",
    "LONG_FORECASTING_DFS = f'{outputRoot}/{_test_event}/{_forecast_mode}-long_forecasting_dfs_d{dataset}_m{testmodel}.pickle'\n",
    "FORECAST_FIGS_DIR = f'{outputRoot}/{_test_event}/{_forecast_mode}-forecast-figs-d{dataset}_m{testmodel}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. make laptime dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load laptime and stage dataset: QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60//laptime_rank_timediff_pit-oracle-Indy500_2013_2019_v9_p0.pickle QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60//stagedata-Indy500_2013_2019_v9_p0.pickle\n"
     ]
    }
   ],
   "source": [
    "stagedata = {}\n",
    "global_carids = {}\n",
    "os.makedirs(outputRoot, exist_ok=True)\n",
    "os.makedirs(f'{outputRoot}/{_test_event}', exist_ok=True)\n",
    "\n",
    "#check the dest files first\n",
    "if _skip_overwrite and os.path.exists(LAPTIME_DATASET) and os.path.exists(STAGE_DATASET):\n",
    "        #\n",
    "        # load data\n",
    "        #\n",
    "        print('Load laptime and stage dataset:',LAPTIME_DATASET, STAGE_DATASET)\n",
    "        with open(LAPTIME_DATASET, 'rb') as f:\n",
    "            global_carids, laptime_data = pickle.load(f, encoding='latin1') \n",
    "        with open(STAGE_DATASET, 'rb') as f:\n",
    "            stagedata = pickle.load(f, encoding='latin1') \n",
    "    \n",
    "else:    \n",
    "    cur_carid = 0\n",
    "    for event in events:\n",
    "        #dataid = f'{event}-{year}'\n",
    "        #alldata, rankdata, acldata, flagdata\n",
    "        stagedata[event] = load_data(event)\n",
    "\n",
    "        alldata, rankdata, acldata, flagdata = stagedata[event]\n",
    "        carlist = set(acldata['car_number'])\n",
    "        laplist = set(acldata['completed_laps'])\n",
    "        print('%s: carno=%d, lapnum=%d'%(event, len(carlist), len(laplist)))\n",
    "\n",
    "        #build the carid map\n",
    "        for car in carlist:\n",
    "            if car not in global_carids:\n",
    "                global_carids[car] = cur_carid\n",
    "                cur_carid += 1\n",
    "\n",
    "    laptime_data = get_laptime_dataset(stagedata,inlap_status = _inlap_status)\n",
    "\n",
    "    if _savedata:\n",
    "        import pickle\n",
    "        #stintdf.to_csv('laptime-%s.csv'%year)\n",
    "        #savefile = outputRoot + f'laptime_rank_timediff_pit-oracle-{dbid}.pickle' \n",
    "        savefile = LAPTIME_DATASET\n",
    "        print(savefile)\n",
    "        with open(savefile, 'wb') as f:\n",
    "            #pack [global_carids, laptime_data]\n",
    "            savedata = [global_carids, laptime_data]\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        #savefile = outputRoot + f'stagedata-{dbid}.pickle' \n",
    "        savefile = STAGE_DATASET\n",
    "        print(savefile)\n",
    "        with open(savefile, 'wb') as f:\n",
    "            #pack [global_carids, laptime_data]\n",
    "            savedata = stagedata\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. make gluonts db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Gluonts Dataset: QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60/noinlap-S0LTYP0T/rank-indy500/gluontsdb-rank-oracle-noip-noeid-all-all-f1min-t2-rIndy500-2019-indy-2019.pickle\n",
      ".......loaded data, freq= 1min prediction_length= 2\n",
      "Load New Laptime Dataset: QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60/noinlap-S0LTYP0T/rank-indy500/gluontsdb-rank-oracle-noip-noeid-all-all-f1min-t2-rIndy500-2019-indy-2019-newlaptimedata.pickle\n"
     ]
    }
   ],
   "source": [
    "outdir = outputRoot + _dataset_id\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "if dataset == 'laptime':\n",
    "    subdir = 'laptime-indy500'\n",
    "    os.makedirs(f'{outdir}/{subdir}', exist_ok=True)\n",
    "    _run_ts = COL_LAPTIME\n",
    "elif dataset == 'timediff':\n",
    "    subdir = 'timediff-indy500'\n",
    "    os.makedirs(f'{outdir}/{subdir}', exist_ok=True)\n",
    "    _run_ts = COL_TIMEDIFF\n",
    "elif dataset == 'rank':\n",
    "    subdir = 'rank-indy500'\n",
    "    os.makedirs(f'{outdir}/{subdir}', exist_ok=True)\n",
    "    _run_ts = COL_RANK\n",
    "else:\n",
    "    print('error, dataset not support: ', dataset)\n",
    "    \n",
    "_task_dir = f'{outdir}/{subdir}/'\n",
    "\n",
    "#\n",
    "#dbname, train_ds, test_ds = makedbs()   \n",
    "#\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "jointstr = '-joint' if _joint_train else ''\n",
    "\n",
    "dbname = _task_dir + f'gluontsdb-{dataset}-oracle-{ipstr}-all-all-f{freq}-t{prediction_length}-r{_test_event}-indy-{year}{jointstr}.pickle'\n",
    "laptimedb = _task_dir + f'gluontsdb-{dataset}-oracle-{ipstr}-all-all-f{freq}-t{prediction_length}-r{_test_event}-indy-{year}-newlaptimedata.pickle'\n",
    "\n",
    "#check the dest files first\n",
    "if _skip_overwrite and os.path.exists(dbname) and os.path.exists(laptimedb):\n",
    "        print('Load Gluonts Dataset:',dbname)\n",
    "        with open(dbname, 'rb') as f:\n",
    "            freq, prediction_length, cardinality, train_ds, test_ds = pickle.load(f, encoding='latin1') \n",
    "        print('.......loaded data, freq=', freq, 'prediction_length=', prediction_length)\n",
    "        print('Load New Laptime Dataset:',laptimedb)\n",
    "        with open(laptimedb, 'rb') as f:\n",
    "            prepared_laptimedata = pickle.load(f, encoding='latin1') \n",
    "        \n",
    "else:\n",
    "    if useeid:\n",
    "        cardinality = [len(global_carids), len(laptime_data)]\n",
    "    else:\n",
    "        cardinality = [len(global_carids)]\n",
    "\n",
    "    prepared_laptimedata = prepare_laptimedata(prediction_length, freq, test_event = _test_event,\n",
    "                           train_ratio=0, context_ratio = 0.,shift_len = prediction_length)\n",
    "\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(prepared_laptimedata, prediction_length,freq,\n",
    "                                         useeid=useeid, run_ts=_run_ts,\n",
    "                                        test_event=_test_event, log_transform =False,\n",
    "                                        context_ratio=0, train_ratio = 0)    \n",
    "\n",
    "\n",
    "    if _savedata:\n",
    "        print('Save Gluonts Dataset:',dbname)\n",
    "        with open(dbname, 'wb') as f:\n",
    "            savedata = [freq, prediction_length, cardinality, train_ds, test_ds]\n",
    "            pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Save preprocessed laptime Dataset:',laptimedb)\n",
    "        with open(laptimedb, 'wb') as f:\n",
    "            pickle.dump(prepared_laptimedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint found at: QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60/noinlap-S0LTYP0T/rank-indy500/deepAR-rank-all-indy-f1min-t2-e1000-r1_oracle_t2\n"
     ]
    }
   ],
   "source": [
    "id='oracle'\n",
    "run=1\n",
    "runid=f'{trainmodel}-{dataset}-all-indy-f1min-t{prediction_length}-e{epochs}-r{run}_{id}_t{prediction_length}'\n",
    "modelfile = _task_dir + runid\n",
    "\n",
    "if _skip_overwrite and os.path.exists(modelfile):\n",
    "    print('Model checkpoint found at:',modelfile)\n",
    "\n",
    "else:\n",
    "    #get target dim\n",
    "    entry = next(iter(train_ds))\n",
    "    target_dim = entry['target'].shape\n",
    "    target_dim = target_dim[0] if len(target_dim) > 1 else 1\n",
    "    print('target_dim:%s', target_dim)\n",
    "\n",
    "    estimator = init_estimator(trainmodel, gpuid, \n",
    "            epochs, batch_size,target_dim, distr_output = distr_output,use_feat_static = use_feat_static)\n",
    "\n",
    "    predictor = estimator.train(train_ds)\n",
    "\n",
    "    if _savedata:\n",
    "        os.makedirs(modelfile, exist_ok=True)\n",
    "\n",
    "        print('Start to save the model to %s', modelfile)\n",
    "        predictor.serialize(Path(modelfile))\n",
    "        print('End of saving the model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Simulation Results: QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60//Indy500-2019/shortterm-dfout-deepAR-indy500-rank-noinlap-S0LTYP0T-standard-l2-alldata.pickle\n",
      ".......loaded data, ret keys= dict_keys(['standard-rank-2019-noinlap-S0LTYP0T'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_simulator.py:673: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  uni_ds['rank_diff'][mask] = 0\n",
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_simulator.py:677: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  uni_ds['time_diff'][mask] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: load dataset laptime_rank_timediff_pit-oracle-Indy500_2013_2019_v9_p0.pickle with 7 races, 58 cars\n",
      "Set a new global laptime_data, shape= 7 (30, 15, 200)\n"
     ]
    }
   ],
   "source": [
    "lapmode = _inlap_status\n",
    "fmode = _feature_mode\n",
    "runts = dataset\n",
    "mid = f'{testmodel}-%s-%s-%s-%s'%(runts, year, inlapstr[lapmode], cur_featurestr)\n",
    "datasetid = outputRoot + _dataset_id\n",
    "\n",
    "if _skip_overwrite and os.path.exists(SIMULATION_OUTFILE):\n",
    "    print('Load Simulation Results:',SIMULATION_OUTFILE)\n",
    "    with open(SIMULATION_OUTFILE, 'rb') as f:\n",
    "        dfs,acc,ret,pret = pickle.load(f, encoding='latin1') \n",
    "    print('.......loaded data, ret keys=', ret.keys())\n",
    "    \n",
    "    \n",
    "    # init the stint module\n",
    "    #\n",
    "    # in test mode, set all train_len = 40 to unify the evaluation results\n",
    "    #\n",
    "    init_simulation(datasetid, _test_event, 'rank',stint.COL_RANK,'rank',prediction_length, \n",
    "                    pitmodel=pitmodel, inlapmode=lapmode,featuremode =fmode,\n",
    "                    train_len = _test_train_len, pitmodel_bias= _pitmodel_bias)    \n",
    "\n",
    "else:\n",
    "    #run simulation\n",
    "    acc, ret, pret = {}, {}, {}\n",
    "\n",
    "    #lapmode = _inlap_status\n",
    "    #fmode = _feature_mode\n",
    "    #runts = dataset\n",
    "    #mid = f'{testmodel}-%s-%s-%s-%s'%(runts, year, inlapstr[lapmode], featurestr[fmode])\n",
    "\n",
    "    if runts == 'rank':\n",
    "        acc[mid], ret[mid] = simulation(datasetid, _test_event, \n",
    "                    'rank',stint.COL_RANK,'rank',\n",
    "                   prediction_length, stint.MODE_ORACLE,loopcnt, \n",
    "                      pitmodel=pitmodel, model=testmodel, inlapmode=lapmode,featuremode =fmode,\n",
    "                    train_len = _test_train_len, forecastmode = _forecast_mode, joint_train = _joint_train,\n",
    "                    pitmodel_bias= _pitmodel_bias)\n",
    "    else:\n",
    "        acc[mid], ret[mid] = simulation(datasetid, _test_event, \n",
    "                    'timediff',stint.COL_TIMEDIFF,'timediff2rank',\n",
    "                   prediction_length, stint.MODE_ORACLE,loopcnt, \n",
    "                      pitmodel=pitmodel, model=testmodel, inlapmode=lapmode,featuremode =fmode,\n",
    "                    train_len = _test_train_len, forecastmode = _forecast_mode, joint_train = _joint_train,\n",
    "                    pitmodel_bias= _pitmodel_bias)\n",
    "\n",
    "    if _forecast_mode == 'shortterm':\n",
    "        allsamples, alltss = get_allsamples(ret[mid], year=year)\n",
    "        _, pret[mid]= prisk_direct_bysamples(allsamples, alltss)\n",
    "        print(pret[mid])\n",
    "    \n",
    "\n",
    "    dfs={}\n",
    "\n",
    "    mode=1\n",
    "    df = get_alldf_mode(ret[mid], year=year,mode=mode)\n",
    "    name = '%s_%s'%(testmodel, 'mean' if mode==1 else ('mode' if mode==0 else 'median'))\n",
    "    if year not in dfs:\n",
    "        dfs[year] = {}\n",
    "    dfs[year][name] = df\n",
    "\n",
    "    _trim = 0\n",
    "    _include_final = True\n",
    "    _include_stintlen = True\n",
    "    include_str = '1' if _include_final else '0'\n",
    "    stint_str = '1' if _include_stintlen else ''            \n",
    "    #simulation_outfile=outputRoot + f'shortterm-dfout-oracle-indy500-{dataset}-{inlapstr[_inlap_status]}-{featurestr[_feature_mode]}-2018-oracle-l{loopcnt}-alldata-weighted.pickle'\n",
    "\n",
    "    with open(SIMULATION_OUTFILE, 'wb') as f:\n",
    "        savedata = [dfs,acc,ret,pret]\n",
    "        pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "#alias\n",
    "ranknetdf = dfs   \n",
    "ranknet_ret = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Evaluation Results: QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60//Indy500-2019/shortterm-evaluation_result_drank_mstandard.csv\n"
     ]
    }
   ],
   "source": [
    "if _skip_overwrite and os.path.exists(EVALUATION_RESULT_DF):\n",
    "    print('Load Evaluation Results:',EVALUATION_RESULT_DF)\n",
    "    oracle_eval_result = pd.read_csv(EVALUATION_RESULT_DF)\n",
    "\n",
    "else:    \n",
    "    ##-------------------------------------------------------------------------------\n",
    "    if _forecast_mode == 'shortterm':\n",
    "\n",
    "        # get pit laps, pit-covered-laps\n",
    "        # pitdata[year] = [pitlaps, pitcoveredlaps]\n",
    "        with open('pitcoveredlaps-g1.pickle', 'rb') as f:\n",
    "            # The protocol version used is detected automatically, so we do not\n",
    "            # have to specify it.\n",
    "            pitdata = pickle.load(f, encoding='latin1') \n",
    "\n",
    "        #\n",
    "        # Model,SignAcc,MAE,50-Risk,90-Risk\n",
    "        # \n",
    "        cols = ['Year','Model','ExpID','laptype','Top1Acc','MAE','50-Risk','90-Risk']\n",
    "        plen = prediction_length\n",
    "        usemeanstr='mean'\n",
    "\n",
    "        #load data\n",
    "        # dfs,acc,ret,pret\n",
    "\n",
    "        retdata = []\n",
    "\n",
    "        #oracle\n",
    "        dfx = ret[mid]\n",
    "        allsamples, alltss = get_allsamples(dfx, year=year)\n",
    "        #_, pret[mid]= prisk_direct_bysamples(ret[mid][0][1], ret[mid][0][2])\n",
    "        _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)\n",
    "\n",
    "        dfout = do_rerank(ranknetdf[year][f'{testmodel}_mean'])\n",
    "        accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "        #fsamples, ftss = runs2samples_ex(ranknet_ret[f'oracle-RANK-{year}-inlap-nopitage'],[])\n",
    "        #_, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        retdata.append([year,f'{testmodel}',configname,'all', accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "        for laptype in ['normal','pit']:\n",
    "            # select the set\n",
    "            pitcoveredlaps = pitdata[year][1]\n",
    "            normallaps = set([x for x in range(1,201)]) - pitcoveredlaps\n",
    "\n",
    "            if laptype == 'normal':\n",
    "                sellaps = normallaps\n",
    "                clearlaps = pitcoveredlaps\n",
    "            else:\n",
    "                sellaps = pitcoveredlaps\n",
    "                clearlaps = normallaps\n",
    "\n",
    "\n",
    "            # pitcoveredlaps start idx = 1\n",
    "            startlaps = [x-plen-1 for x in sellaps]\n",
    "            #sellapidx = np.array([x-1 for x in sellaps])\n",
    "            clearidx = np.array([x-1 for x in clearlaps])\n",
    "            print('sellaps:', len(sellaps), 'clearlaps:',len(clearlaps))\n",
    "\n",
    "            #oracle\n",
    "            #outfile=f'shortterm-dfout-ranknet-indy500-rank-inlap-nopitage-20182019-oracle-l10-alldata-weighted.pickle'\n",
    "            #_all = load_dfout_all(outfile)[0]\n",
    "            #ranknetdf, acc, ret, pret = _all[0],_all[1],_all[2],_all[3]\n",
    "\n",
    "            dfout = do_rerank(ranknetdf[year][f'{testmodel}_mean'])\n",
    "\n",
    "            allsamples, alltss = get_allsamples(dfx, year=year)\n",
    "\n",
    "\n",
    "            allsamples, alltss = clear_samples(allsamples, alltss,clearidx)\n",
    "\n",
    "            _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)\n",
    "\n",
    "            dfout = dfout[dfout['startlap'].isin(startlaps)]\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "            print(year, laptype,f'RankNet-{testmodel}',accret[0], accret[1], prisk_vals[1], prisk_vals[2])\n",
    "            retdata.append([year, f'{testmodel}',configname,laptype, accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "            \n",
    "    ##-------------------------------------------------------------------------------\n",
    "    elif _forecast_mode == 'stint':\n",
    "        if testmodel == 'oracle':\n",
    "            datafile=f'stint-dfout-mlmodels-indy500-tr2013_2017-te2018_2019-end1-oracle-t0-tuned.pickle'\n",
    "        else:\n",
    "            datafile=f'stint-dfout-mlmodels-indy500-tr2013_2017-te2018_2019-end1-normal-t0-tuned.pickle'\n",
    "        #preddf = load_dfout(outfile)\n",
    "        with open(datafile, 'rb') as f:\n",
    "            preddf = pickle.load(f, encoding='latin1')[0] \n",
    "        #preddf_oracle = load_dfout(outfile)\n",
    "        ranknet_ret = ret \n",
    "\n",
    "        errlist = {}\n",
    "        errcnt, errlist[year] = cmp_df(ranknetdf[year][f'{testmodel}_mean'], preddf[year]['lasso'])\n",
    "        \n",
    "        retdata = []\n",
    "        #\n",
    "        # Model,SignAcc,MAE,50-Risk,90-Risk\n",
    "        # \n",
    "        cols = ['Year','Model','ExpID','laptype','SignAcc','MAE','50-Risk','90-Risk']\n",
    "        models = {'currank':'CurRank','rf':'RandomForest','svr_lin':'SVM','xgb':'XGBoost'}\n",
    "\n",
    "        for clf in ['currank','rf','svr_lin','xgb']:\n",
    "            print('year:',year,'clf:',clf)\n",
    "            dfout, accret = eval_sync(preddf[year][clf],errlist[year])\n",
    "            fsamples, ftss = df2samples_ex(dfout)\n",
    "            _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "\n",
    "            retdata.append([year,models[clf],configname,'all', accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "            \n",
    "        #ml models -oracle\n",
    "        #for clf in ['rf','svr_lin','xgb']:\n",
    "        #    print('year:',year,'clf:',clf)\n",
    "        #    dfout, accret = eval_sync(preddf_oracle[year][clf],errlist[year])\n",
    "        #    fsamples, ftss = df2samples(dfout)\n",
    "        #    _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        #    retdata.append([year,models[clf]+'-Oracle',configname,'all',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "        dfout, accret = eval_sync(ranknetdf[year][f'{testmodel}_mean'], errlist[year],force2int=True)\n",
    "        #fsamples, ftss = df2samples(dfout)\n",
    "        fsamples, ftss = runs2samples(ranknet_ret[mid],errlist[f'{year}'])\n",
    "        _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        retdata.append([year,f'{testmodel}',configname,'all',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "        #dfout, accret = eval_sync(ranknetdf[year]['oracle_mean'], errlist[year],force2int=True)\n",
    "        ##fsamples, ftss = df2samples(dfout)\n",
    "        #fsamples, ftss = runs2samples(ranknet_ret[f'oracle-TIMEDIFF-{year}-noinlap-nopitage'],errlist[f'{year}'])\n",
    "        #_, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        #retdata.append([year,'RankNet-Oracle',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "    oracle_eval_result = pd.DataFrame(data=retdata, columns=cols)\n",
    "    if _savedata:\n",
    "        oracle_eval_result.to_csv(EVALUATION_RESULT_DF)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Draw forecasting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Long Forecasting Data: QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60//Indy500-2019/shortterm-long_forecasting_dfs_drank_mstandard.pickle\n",
      ".......loaded data, alldata keys= dict_keys([2, 3, 4, 5, 7, 9, 10, 12, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 33, 39, 42, 48, 60, 63, 77, 81, 98])\n"
     ]
    }
   ],
   "source": [
    "if _forecast_mode == 'shortterm' and _joint_train == False:\n",
    "    if _skip_overwrite and os.path.exists(LONG_FORECASTING_DFS):\n",
    "        fname = LONG_FORECASTING_DFS\n",
    "        print('Load Long Forecasting Data:',fname)\n",
    "        with open(fname, 'rb') as f:\n",
    "            alldata = pickle.load(f, encoding='latin1') \n",
    "        print('.......loaded data, alldata keys=', alldata.keys())\n",
    "\n",
    "    else:    \n",
    "\n",
    "        oracle_ret = ret    \n",
    "        mid = f'{testmodel}-%s-%s-%s-%s'%(runts, year, inlapstr[lapmode], cur_featurestr)\n",
    "        print('eval mid:', mid, f'{testmodel}_ret keys:', ret.keys())\n",
    "\n",
    "        ## init predictor\n",
    "        _predictor =  NaivePredictor(freq= freq, prediction_length = prediction_length)\n",
    "\n",
    "        oracle_dfout = do_rerank(dfs[year][f'{testmodel}_mean'])\n",
    "        carlist = set(list(oracle_dfout.carno.values))\n",
    "        carlist = [int(x) for x in carlist]\n",
    "        print('carlist:', carlist,'len:',len(carlist))\n",
    "\n",
    "        #carlist = [13, 7, 3, 12]\n",
    "        #carlist = [13]    \n",
    "\n",
    "        retdata = {}\n",
    "        for carno in carlist:\n",
    "            print(\"*\"*40)\n",
    "            print('Run models for carno=', carno)\n",
    "            # create the test_ds first\n",
    "            test_cars = [carno]\n",
    "\n",
    "            #train_ds, test_ds, trainset, testset = stint.make_dataset_byevent(events_id[_test_event], \n",
    "            #                                 prediction_length,freq, \n",
    "            #                                 oracle_mode=stint.MODE_ORACLE,\n",
    "            #                                 run_ts = _run_ts,\n",
    "            #                                 test_event = _test_event,\n",
    "            #                                 test_cars=test_cars,\n",
    "            #                                 half_moving_win = 0,\n",
    "            #                                 train_ratio = 0.01)\n",
    "\n",
    "            train_ds, test_ds, trainset, testset = make_dataset_byevent(prepared_laptimedata, prediction_length,freq,\n",
    "                                             useeid=useeid, run_ts=_run_ts,\n",
    "                                            test_event=_test_event, log_transform =False,\n",
    "                                            context_ratio=0, train_ratio = 0,\n",
    "                                            test_cars = test_cars)    \n",
    "\n",
    "\n",
    "            if (len(testset) <= 10 + prediction_length):\n",
    "                print('ts too short, skip ', len(testset))\n",
    "                continue\n",
    "\n",
    "            #by first run samples\n",
    "            samples = oracle_ret[mid][0][1][test_cars[0]]\n",
    "            tss  = oracle_ret[mid][0][2][test_cars[0]]\n",
    "            target_oracle1, tss_oracle1 = long_predict_bysamples('1run-samples', samples, tss)\n",
    "\n",
    "            #by first run output df(_use_mean = true, already reranked)\n",
    "            df = oracle_ret[mid][0][0]\n",
    "            dfin_oracle = df[df['carno']==test_cars[0]]\n",
    "            target_oracle2, tss_oracle2 = long_predict_bydf(f'{testmodel}-1run-dfout', dfin_oracle)        \n",
    "\n",
    "\n",
    "            #by multi-run mean at oracle_dfout\n",
    "            df = oracle_dfout\n",
    "            dfin_oracle = df[df['carno']==test_cars[0]]\n",
    "            target_oracle3, tss_oracle3 = long_predict_bydf(f'{testmodel}-multimean', dfin_oracle)        \n",
    "\n",
    "\n",
    "            #no rerank\n",
    "            df = ranknetdf[year][f'{testmodel}_mean']\n",
    "            dfin_oracle = df[df['carno']==test_cars[0]]\n",
    "            target_oracle4, tss_oracle4 = long_predict_bydf(f'{testmodel}-norerank-multimean', dfin_oracle)        \n",
    "\n",
    "\n",
    "            #by multiple runs\n",
    "            target_oracle_multirun, tss_oracle_multirun = get_ranknet_multirun(\n",
    "                                    oracle_ret[mid], \n",
    "                                    test_cars[0],sampleCnt=loopcnt)\n",
    "\n",
    "            retdata[carno] = [[tss_oracle1,tss_oracle2,tss_oracle3,tss_oracle4,tss_oracle_multirun],\n",
    "                               [target_oracle1,target_oracle2,target_oracle3,target_oracle4,target_oracle_multirun]]\n",
    "\n",
    "        alldata = retdata    \n",
    "\n",
    "        if _savedata:\n",
    "            with open(LONG_FORECASTING_DFS, 'wb') as f:\n",
    "                pickle.dump(alldata, f, pickle.HIGHEST_PROTOCOL)  \n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    if _forecast_mode == 'shortterm' and _joint_train == False:\n",
    "        destdir = FORECAST_FIGS_DIR\n",
    "\n",
    "        if _skip_overwrite and os.path.exists(destdir):\n",
    "            print('Long Forecasting Figures at:',destdir)\n",
    "\n",
    "        else:\n",
    "            with open('stagedata-Indy500_2013_2019_v9_p0.pickle', 'rb') as f:\n",
    "                stagedata = pickle.load(f, encoding='latin1') \n",
    "                _alldata, rankdata, _acldata, _flagdata = stagedata[_test_event]\n",
    "\n",
    "            #destdir = outputRoot + 'oracle-forecast-figs/'\n",
    "            os.makedirs(destdir, exist_ok=True)\n",
    "\n",
    "            for carno in alldata:\n",
    "                plotoracle(alldata, carno, destdir)\n",
    "\n",
    "            #draw summary result\n",
    "            outputfile = destdir + f'{configname}'\n",
    "            plotallcars(alldata, outputfile, drawid = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotoracle(alldata, 3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QuickTestOutput/weighted-noinlap-S0LTYP0T-nocate-c60/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputRoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Model</th>\n",
       "      <th>ExpID</th>\n",
       "      <th>laptype</th>\n",
       "      <th>Top1Acc</th>\n",
       "      <th>MAE</th>\n",
       "      <th>50-Risk</th>\n",
       "      <th>90-Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>standard</td>\n",
       "      <td>weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle</td>\n",
       "      <td>all</td>\n",
       "      <td>0.728723</td>\n",
       "      <td>1.221176</td>\n",
       "      <td>0.086195</td>\n",
       "      <td>0.084938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>standard</td>\n",
       "      <td>weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.932584</td>\n",
       "      <td>0.212509</td>\n",
       "      <td>0.018352</td>\n",
       "      <td>0.017463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>standard</td>\n",
       "      <td>weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle</td>\n",
       "      <td>pit</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>2.121008</td>\n",
       "      <td>0.146530</td>\n",
       "      <td>0.144946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Year     Model  \\\n",
       "0           0  2019  standard   \n",
       "1           1  2019  standard   \n",
       "2           2  2019  standard   \n",
       "\n",
       "                                               ExpID laptype   Top1Acc  \\\n",
       "0  weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle     all  0.728723   \n",
       "1  weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle  normal  0.932584   \n",
       "2  weighted-noinlap-S0LTYP0T-nocate-c60-drank-oracle     pit  0.545455   \n",
       "\n",
       "        MAE   50-Risk   90-Risk  \n",
       "0  1.221176  0.086195  0.084938  \n",
       "1  0.212509  0.018352  0.017463  \n",
       "2  2.121008  0.146530  0.144946  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
