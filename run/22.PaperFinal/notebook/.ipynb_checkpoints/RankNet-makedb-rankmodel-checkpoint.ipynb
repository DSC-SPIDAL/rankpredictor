{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### laptime & rank &timediff dataset - fulltest-pitage\n",
    "\n",
    "make gluonts dataset for rankmodel training\n",
    "\n",
    "base: 18./laptime & rank &timediff dataset - fulltest-pitage\n",
    "\n",
    "+ add inlap into lapstatus(the lap before 'P')\n",
    "+ remove extracting laptime data code, load from file directly\n",
    "\n",
    "base: 14./laptime_rank_timediff_dataset-oracle-fulltest\n",
    "\n",
    "+ add lap_instint to dynamic features(pitage)\n",
    "+ add support for historical data, indy500 for different years\n",
    "+ add track and lap status as target, preparing for StatusModel with deepAR\n",
    "\n",
    "this notebook focuses on create the training set, while laptime2rank-evaluate will have testset codes specified for rank calculation\n",
    "\n",
    "+ add new features, caution_laps_instint,lap_instint\n",
    "\n",
    "Build a time series dataset across all the oval races, including laptime, rank. When this dataset aims to be used in forecasting, covariates of the racing status can not be included, such as track_status and lap_status. However, they can be used in oracle test to tell the upper bound of performance of the predictor.\n",
    "\n",
    "Change to a new dataset format that following telemetry dataset.\n",
    "\n",
    "raw:\n",
    "+  [(eventid, carids: carno -> rowid, datalist)]\n",
    "\n",
    "datalist := [datalist_entry] in shape of #car_number\n",
    "\n",
    "datalist_entry := [[laptime, rank, track_status, lap_status]], in shape of #totallaps x #featureCnt (padded by nan)\n",
    "\n",
    "gluonts:\n",
    "\n",
    "+ Rank2LaptimeDifference\n",
    "\n",
    "rank prediction by elapsed time difference in a race\n",
    "\n",
    "Since use laptime2rank is not very successful, and predicting rank directly is not as well, to combine the real value regression into rank prediction task might be a solution.\n",
    "\n",
    "And, differences of the elapsed time among the cars takes correlations into input, that brings more chances to success.\n",
    "\n",
    "time_behind_leader is accurate when the car runs in the same lap as the leader does, otherwise, the time is truncted. To get accurate differences, using elapsed time is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import random\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch_hdd/hpda/indycar/notebook/19.RankNet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def test_flag(a, bitflag):\n",
    "    return (a & bitflag) ==  bitflag\n",
    "\n",
    "#\n",
    "# remove NaN at the tail\n",
    "# there should be no nans in the middle of the ts\n",
    "COL_LAPTIME=0\n",
    "COL_RANK=1\n",
    "COL_TRACKSTATUS=2\n",
    "COL_LAPSTATUS=3\n",
    "COL_TIMEDIFF=4\n",
    "COL_CAUTION_LAPS_INSTINT=5\n",
    "COL_LAPS_INSTINT= 6\n",
    "COL_ELAPSED_TIME= 7\n",
    "\n",
    "FEATURE_STATUS = 2\n",
    "FEATURE_PITAGE = 4\n",
    "\n",
    "MODE_ORACLE = 0\n",
    "MODE_NOLAP = 1\n",
    "MODE_NOTRACK = 2\n",
    "MODE_TESTZERO = 4\n",
    "MODE_TESTCURTRACK = 8\n",
    "#MODE_STR={MODE_ORACLE:'oracle', MODE_NOLAP:'nolap',MODE_NOTRACK:'notrack',MODE_TEST:'test'}\n",
    "\n",
    "#_feature_mode = FEATURE_STATUS\n",
    "\n",
    "def make_dataset_byevent(runs, prediction_length, freq, \n",
    "                       useeid = False,\n",
    "                       run_ts=COL_LAPTIME, \n",
    "                       test_event = 'Indy500-2018',\n",
    "                       use_global_dict = True,\n",
    "                       oracle_mode = MODE_ORACLE,\n",
    "                       half_moving_win = True,\n",
    "                       train_ratio=0.8,\n",
    "                       log_transform = False,\n",
    "                       context_ratio = 0.,\n",
    "                       dorerank = True\n",
    "                ):\n",
    "    \"\"\"\n",
    "    split the ts to train and test part by the ratio\n",
    "    \n",
    "    oracle_mode: false to simulate prediction in real by \n",
    "        set the covariates of track and lap status as nan in the testset\n",
    "            \n",
    "    \n",
    "    \"\"\"    \n",
    "    #global setting\n",
    "    feature_mode = _feature_mode\n",
    "    \n",
    "    \n",
    "    start = pd.Timestamp(\"01-01-2019\", freq=freq)  # can be different for each time series\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    #select run\n",
    "    if runs>=0:\n",
    "        _laptime_data = [laptime_data[runs].copy()]\n",
    "    else:\n",
    "        _laptime_data = laptime_data.copy()\n",
    "    \n",
    "   \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    for _data in _laptime_data:\n",
    "        _train = []\n",
    "        _test = []\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        \n",
    "        else:\n",
    "            test_mode = False\n",
    "            \n",
    "            \n",
    "        #statistics on the ts length\n",
    "        ts_len = [ _entry.shape[1] for _entry in _data[2]]\n",
    "        train_len = int(np.max(ts_len) * train_ratio)\n",
    "        \n",
    "        if context_ratio != 0.:\n",
    "            # add this part to train set\n",
    "            context_len = int(np.max(ts_len) * context_ratio)\n",
    "        else:    \n",
    "            context_len = prediction_length*2\n",
    "        if context_len < 10:\n",
    "            context_len = 10\n",
    "        \n",
    "        print(f'====event:{events[_data[0]]}, prediction_len={prediction_length},train_len={train_len}, max_len={np.max(ts_len)}, min_len={np.min(ts_len)},context_len={context_len}')\n",
    "\n",
    "        #rerank due to short ts removed\n",
    "        if run_ts == COL_RANK and dorerank == True:\n",
    "            sel_rows = []\n",
    "            for rowid in range(_data[2].shape[0]):\n",
    "                # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "                rec = _data[2][rowid].copy()\n",
    "                #remove nan(only tails)\n",
    "                nans, x= nan_helper(rec[run_ts,:])\n",
    "                nan_count = np.sum(nans)             \n",
    "                rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "                \n",
    "                totallen = rec.shape[1]\n",
    "                if ( totallen < train_len + prediction_length):\n",
    "                    print(f'rerank a short ts: carid={_data[1][rowid]}，len={totallen}')\n",
    "                    continue \n",
    "                else:\n",
    "                    sel_rows.append(rowid)\n",
    "                    \n",
    "            #get selected matrix\n",
    "            sel_idx = np.array(sel_rows)\n",
    "            selmat = _data[2][sel_idx]\n",
    "            \n",
    "            mask = np.isnan(selmat[:,COL_RANK,:])\n",
    "            \n",
    "            idx = np.argsort(selmat[:,COL_RANK,:], axis=0)\n",
    "            true_rank = np.argsort(idx, axis=0).astype(np.float)\n",
    "            true_rank[mask] = np.nan\n",
    "            \n",
    "            #set it back\n",
    "            #if _data[0]==0:\n",
    "            #    print('raw:')\n",
    "            #    print(_data[2][:,COL_RANK,0])\n",
    "            #    print('true_rank:')\n",
    "            #    print(true_rank[:,0])\n",
    "            #_data[2][sel_idx][:,COL_RANK,:] = true_rank       \n",
    "            _data[2][sel_idx,COL_RANK,:] = true_rank       \n",
    "            #if _data[0]==0:\n",
    "            #    _view = _data[2][sel_idx]\n",
    "            #    _view[:,COL_RANK,:] = true_rank\n",
    "            #    print('view:')\n",
    "            #    print(_data[2][:,COL_RANK,0])\n",
    "            #    print(_view[:,COL_RANK,0])\n",
    "            #    print('rerank:')\n",
    "            #    print(_data[2][sel_idx][:,COL_RANK,0])\n",
    "        \n",
    "        \n",
    "        # process for each ts\n",
    "        for rowid in range(_data[2].shape[0]):\n",
    "            # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "            rec = _data[2][rowid].copy()\n",
    "            \n",
    "            #remove nan(only tails)\n",
    "            nans, x= nan_helper(rec[run_ts,:])\n",
    "            nan_count = np.sum(nans)             \n",
    "            rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "            \n",
    "            # remove short ts\n",
    "            totallen = rec.shape[1]\n",
    "            if ( totallen < train_len + prediction_length):\n",
    "                print(f'a short ts: carid={_data[1][rowid]}，len={totallen}')\n",
    "                continue                \n",
    "            \n",
    "            if use_global_dict:\n",
    "                carno = _data[1][rowid]\n",
    "                carid = global_carids[_data[1][rowid]]\n",
    "            else:\n",
    "                #simulation dataset, todo, fix the carids as decoder\n",
    "                carno = rowid\n",
    "                carid = rowid\n",
    "                \n",
    "            \n",
    "            if useeid:\n",
    "                static_cat = [carid, _data[0]]    \n",
    "            else:\n",
    "                static_cat = [carid]    \n",
    "                \n",
    "            #first, get target a copy    \n",
    "            # target can be COL_XXSTATUS\n",
    "            target_val = rec[run_ts,:].copy().astype(np.float32)\n",
    "            if log_transform:\n",
    "                target_val = np.log(target_val + 1.0)\n",
    "            \n",
    "            # selection of features\n",
    "            if test_flag(oracle_mode, MODE_NOTRACK):                \n",
    "                rec[COL_TRACKSTATUS, :] = 0\n",
    "            if test_flag(oracle_mode, MODE_NOLAP):                \n",
    "                rec[COL_LAPSTATUS, :] = 0\n",
    "\n",
    "            test_rec_cnt = 0\n",
    "            if not test_mode:\n",
    "                if feature_mode == FEATURE_PITAGE:  \n",
    "                    # all go to train set\n",
    "                    _train.append({'target': target_val, \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [rec[COL_TRACKSTATUS,:],\n",
    "                                       rec[COL_LAPSTATUS,:],\n",
    "                                       rec[COL_LAPS_INSTINT,:]]\n",
    "                              }\n",
    "                              )\n",
    "                else:\n",
    "                    # all go to train set\n",
    "                    _train.append({'target': target_val, \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [rec[COL_TRACKSTATUS,:],\n",
    "                                       rec[COL_LAPSTATUS,:]]\n",
    "                              }\n",
    "                              )\n",
    "                    \n",
    "            else:\n",
    "                # reset train_len\n",
    "                if context_ratio != 0.:\n",
    "                    # all go to train set\n",
    "                    #add [0, context_len] to train set \n",
    "                    if feature_mode == FEATURE_PITAGE:  \n",
    "                        _train.append({'target': target_val[:context_len], \n",
    "                                    'start': start, \n",
    "                                    'feat_static_cat': static_cat,\n",
    "                                    'feat_dynamic_real': [rec[COL_TRACKSTATUS,:context_len],\n",
    "                                           rec[COL_LAPSTATUS,:context_len],\n",
    "                                           rec[COL_LAPS_INSTINT,:context_len]               \n",
    "                                                         ]\n",
    "                                  }\n",
    "                                  )                    \n",
    "                    else:\n",
    "                        _train.append({'target': target_val[:context_len], \n",
    "                                    'start': start, \n",
    "                                    'feat_static_cat': static_cat,\n",
    "                                    'feat_dynamic_real': [rec[COL_TRACKSTATUS,:context_len],\n",
    "                                           rec[COL_LAPSTATUS,:context_len]               \n",
    "                                                         ]\n",
    "                                  }\n",
    "                                  )                    \n",
    "                \n",
    "                # testset\n",
    "                # multiple test ts(rolling window as half of the prediction_length)\n",
    "\n",
    "                step = -int(prediction_length/2) if half_moving_win else -prediction_length\n",
    "                for endpos in range(totallen, context_len+prediction_length, \n",
    "                                    step):\n",
    "\n",
    "                    track_rec = rec[COL_TRACKSTATUS, :endpos].copy()\n",
    "                    lap_rec = rec[COL_LAPSTATUS, :endpos].copy()\n",
    "                    pitage_rec = rec[COL_LAPS_INSTINT, :endpos].copy()\n",
    "\n",
    "                    # test mode\n",
    "                    if test_flag(oracle_mode, MODE_TESTCURTRACK):\n",
    "                        # since nan does not work, use cur-val instead\n",
    "                        track_rec[-prediction_length:] = track_rec[-prediction_length - 1]\n",
    "                        #track_rec[-prediction_length:] = random.randint(0,1)\n",
    "                        #lap_rec[-prediction_length:] = lap_rec[-prediction_length - 1]\n",
    "                        lap_rec[-prediction_length:] = 0\n",
    "\n",
    "                        #for pitage, just assume there is no pit\n",
    "                        start_pitage = pitage_rec[-prediction_length - 1]\n",
    "                        pitage_rec[-prediction_length:] = np.array([x+start_pitage+1 for x in range(prediction_length)])\n",
    "\n",
    "                    elif test_flag(oracle_mode, MODE_TESTZERO):\n",
    "                        #set prediction part as nan\n",
    "                        #track_rec[-prediction_length:] = np.nan\n",
    "                        #lap_rec[-prediction_length:] = np.nan\n",
    "                        track_rec[-prediction_length:] = 0\n",
    "                        lap_rec[-prediction_length:] = 0                    \n",
    "\n",
    "                        #for pitage, just assume there is no pit\n",
    "                        start_pitage = pitage_rec[-prediction_length - 1]\n",
    "                        pitage_rec[-prediction_length:] = np.array([x+start_pitage+1 for x in range(prediction_length)])\n",
    "\n",
    "                    if feature_mode == FEATURE_PITAGE:                          \n",
    "                        _test.append({'target': rec[run_ts,:endpos].astype(np.float32), \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [track_rec,lap_rec,pitage_rec]\n",
    "                                #'feat_dynamic_real': [rec[COL_TRACKSTATUS,:endpos],\n",
    "                                #       rec[COL_LAPSTATUS,:endpos]] \n",
    "                                 }\n",
    "                              )                     \n",
    "                    else:\n",
    "                        _test.append({'target': rec[run_ts,:endpos].astype(np.float32), \n",
    "                                'start': start, \n",
    "                                'feat_static_cat': static_cat,\n",
    "                                'feat_dynamic_real': [track_rec,lap_rec]\n",
    "                                #'feat_dynamic_real': [rec[COL_TRACKSTATUS,:endpos],\n",
    "                                #       rec[COL_LAPSTATUS,:endpos]] \n",
    "                                 }\n",
    "                              )                     \n",
    "                        \n",
    "   \n",
    "                    test_rec_cnt += 1\n",
    "            \n",
    "            #add one ts\n",
    "            print(f'carno:{carno}, totallen:{totallen}, nancount:{nan_count}, test_reccnt:{test_rec_cnt}')\n",
    "\n",
    "        train_set.extend(_train)\n",
    "        test_set.extend(_test)\n",
    "\n",
    "    print(f'train len:{len(train_set)}, test len:{len(test_set)}')\n",
    "    \n",
    "    train_ds = ListDataset(train_set, freq=freq)\n",
    "    test_ds = ListDataset(test_set, freq=freq)    \n",
    "    \n",
    "    return train_ds, test_ds, train_set, test_set\n",
    "\n",
    "def save_dataset(datafile,freq, prediction_length, cardinality, train_ds, test_ds):\n",
    "    with open(datafile, 'wb') as f:\n",
    "        #pack [global_carids, laptime_data]\n",
    "        savedata = [freq, prediction_length, cardinality, train_ds, test_ds]\n",
    "        #savedata = [freq, train_set, test_set]\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bulid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gluontsdb_byevent(run_ts, dbname, test_event='Indy500-2018', \n",
    "                           log_transform = False,context_ratio=0.,dorerank = True):\n",
    "    #run on indy dataset\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq,\n",
    "                                         useeid=useeid, run_ts=run_ts,\n",
    "                                        test_event=test_event,log_transform =log_transform,\n",
    "                                        context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-all-all-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds)\n",
    "    \n",
    "    if useeid:\n",
    "        #special for classic predictor only\n",
    "        return\n",
    "    \n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq, \n",
    "                                         oracle_mode=MODE_TESTZERO,useeid=useeid, \n",
    "                                         run_ts=run_ts,test_event=test_event,\n",
    "                                         log_transform =log_transform,\n",
    "                                         context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-all-zero-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds)\n",
    "    \n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq,\n",
    "                                         oracle_mode=MODE_TESTCURTRACK,useeid=useeid, \n",
    "                                         run_ts=run_ts,test_event=test_event,\n",
    "                                        log_transform =log_transform,\n",
    "                                        context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-all-curtrack-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds)    \n",
    "    \n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq, \n",
    "                                         oracle_mode=MODE_NOTRACK,useeid=useeid, \n",
    "                                         run_ts=run_ts,test_event=test_event,\n",
    "                                        log_transform =log_transform,\n",
    "                                        context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-notrack-all-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds)\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq, \n",
    "                                         oracle_mode=MODE_NOTRACK+MODE_TESTZERO,\n",
    "                                         useeid=useeid, run_ts=run_ts,\n",
    "                                        test_event=test_event,log_transform =log_transform,\n",
    "                                        context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-notrack-zero-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds)\n",
    "    \n",
    "    # nolap+all -> track + pitage\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq, \n",
    "                                         oracle_mode=MODE_NOLAP,useeid=useeid, \n",
    "                                         run_ts=run_ts,test_event=test_event,\n",
    "                                        log_transform =log_transform,\n",
    "                                        context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-nolap-all-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds)\n",
    "    \n",
    "    \n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq, \n",
    "                                         oracle_mode=MODE_NOLAP+MODE_TESTZERO,useeid=useeid, \n",
    "                                         run_ts=run_ts,test_event=test_event,\n",
    "                                        log_transform =log_transform,\n",
    "                                        context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-nolap-zero-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds)\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq, \n",
    "                                         oracle_mode=MODE_NOLAP+MODE_TESTCURTRACK,\n",
    "                                         useeid=useeid, run_ts=run_ts,\n",
    "                                        test_event=test_event,log_transform =log_transform,\n",
    "                                        context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-nolap-curtrack-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds) \n",
    "    \n",
    "    #pitage only\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, prediction_length,freq, \n",
    "                                         oracle_mode=MODE_NOTRACK+MODE_NOLAP,useeid=useeid, \n",
    "                                         run_ts=run_ts,test_event=test_event,\n",
    "                                         log_transform =log_transform,\n",
    "                                         context_ratio=context_ratio,dorerank =dorerank)\n",
    "    save_dataset(f'{dbname}-oracle-{ipstr}-zero-zero-f{freq}-t{prediction_length}-r{test_event}-gluonts-indy-2018.pickle'\n",
    "                 , freq, prediction_length, cardinality,train_ds, test_ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "useeid = False\n",
    "interpolate = False\n",
    "ipstr =''\n",
    "cardinality=[]\n",
    "prediction_length=0\n",
    "def makedbs(fullmode=False):\n",
    "    global useeid,interpolate,ipstr,cardinality,prediction_length\n",
    "    \n",
    "    useeid = False\n",
    "    interpolate = False\n",
    "    #ipstr = '-ip' if interpolate else '-noip'\n",
    "    ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "    if useeid:\n",
    "        cardinality = [len(global_carids), len(laptime_data)]\n",
    "    else:\n",
    "        cardinality = [len(global_carids)]\n",
    "\n",
    "    #plens=[2,5,10,20,30]\n",
    "    plens=[2,4,6, 8]\n",
    "    for prediction_length in plens:\n",
    "        make_gluontsdb_byevent(_run_ts,_task_id,context_ratio=context_ratio)\n",
    "\n",
    "    if fullmode == True:\n",
    "        #special\n",
    "        interpolate = True\n",
    "        useeid = False\n",
    "        ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "        for prediction_length in plens:\n",
    "            make_gluontsdb_byevent(_run_ts,_task_id,context_ratio=context_ratio)\n",
    "\n",
    "        useeid = True\n",
    "        interpolate = False\n",
    "        #ipstr = '-ip' if interpolate else '-noip'\n",
    "        ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "        if useeid:\n",
    "            cardinality = [len(global_carids), len(laptime_data)]\n",
    "        else:\n",
    "            cardinality = [len(global_carids)]\n",
    "\n",
    "        for prediction_length in plens:\n",
    "            make_gluontsdb_byevent(_run_ts,_task_id,context_ratio=context_ratio)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedir(outdir):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to gluonts dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "years = ['2013','2014','2015','2016','2017','2018']\n",
    "#events = ['Indy500']\n",
    "events = [f'Indy500-{x}' for x in years]\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "inlapstr = {0:'noinlap',1:'inlap',2:'outlap'}\n",
    "featurestr = {FEATURE_STATUS:'nopitage',FEATURE_PITAGE:'pitage'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inlapstat in [0,1,2]:\n",
    "    for featuremode in [FEATURE_STATUS, FEATURE_PITAGE]:\n",
    "        _inlap_status = inlapstat\n",
    "        _feature_mode = featuremode\n",
    "        \n",
    "\n",
    "        dbid = f'Indy500_{years[0]}_{years[-1]}_v9_p{_inlap_status}'\n",
    "        with open(f'laptime_rank_timediff_pit-oracle-{dbid}.pickle', 'rb') as f:\n",
    "            global_carids, laptime_data = pickle.load(f, encoding='latin1') \n",
    "            \n",
    "            \n",
    "        freq = \"1min\"\n",
    "        context_ratio=0.\n",
    "        #context_ratio=0.2\n",
    "        #context_ratio=0.4\n",
    "\n",
    "        #outdir = 'outlap-nopitage'\n",
    "        outdir = '%s-%s'%(inlapstr[_inlap_status], featurestr[_feature_mode])\n",
    "        makedir(outdir)\n",
    "\n",
    "        subdir = 'laptime-indy500'\n",
    "        makedir(f'{outdir}/{subdir}' )\n",
    "        _run_ts = COL_LAPTIME\n",
    "        _task_id = f'{outdir}/{subdir}/laptime'\n",
    "        makedbs()\n",
    "\n",
    "        subdir = 'timediff-indy500'\n",
    "        makedir(f'{outdir}/{subdir}' )\n",
    "        _run_ts = COL_TIMEDIFF\n",
    "        _task_id = f'{outdir}/{subdir}/timediff'\n",
    "        makedbs()\n",
    "\n",
    "        subdir = 'rank-indy500'\n",
    "        makedir(f'{outdir}/{subdir}' )\n",
    "        _run_ts = COL_RANK\n",
    "        _task_id = f'{outdir}/{subdir}/rank'\n",
    "        makedbs()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split by event -- laptime\n",
    "\n",
    "freq = \"1min\"\n",
    "context_ratio=0.\n",
    "#context_ratio=0.2\n",
    "#context_ratio=0.4\n",
    "\n",
    "#_inlap_status = 1\n",
    "\n",
    "outdir = 'outlap-nopitage'\n",
    "makedir(outdir)\n",
    "\n",
    "subdir = 'laptime-indy500'\n",
    "makedir(f'{outdir}/{subdir}' )\n",
    "_run_ts = COL_LAPTIME\n",
    "_task_id = f'{outdir}/{subdir}/laptime'\n",
    "makedbs()\n",
    "\n",
    "subdir = 'timediff-indy500'\n",
    "makedir(f'{outdir}/{subdir}' )\n",
    "_run_ts = COL_TIMEDIFF\n",
    "_task_id = f'{outdir}/{subdir}/timediff'\n",
    "makedbs()\n",
    "\n",
    "subdir = 'rank-indy500'\n",
    "makedir(f'{outdir}/{subdir}' )\n",
    "_run_ts = COL_RANK\n",
    "_task_id = f'{outdir}/{subdir}/rank'\n",
    "makedbs()\n",
    "\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split by event -- laptime\n",
    "\n",
    "freq = \"1min\"\n",
    "context_ratio=0.\n",
    "#context_ratio=0.2\n",
    "#context_ratio=0.4\n",
    "\n",
    "outdir = 'pitage'\n",
    "if not os.path.exist(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "_run_ts = COL_LAPTIME\n",
    "_task_id = 'laptime'\n",
    "\n",
    "#_run_ts = COL_TIMEDIFF\n",
    "#_task_id = 'timediff'\n",
    "\n",
    "#_run_ts = COL_LAPSTATUS\n",
    "#_task_id = 'lapstatus'\n",
    "\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(_run_ts,_task_id,context_ratio=context_ratio)\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(_run_ts,_task_id,context_ratio=context_ratio)\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(_run_ts,_task_id,context_ratio=context_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split by event -- rank\n",
    "\n",
    "freq = \"1min\"\n",
    "context_ratio=0.\n",
    "#context_ratio=0.2\n",
    "#context_ratio=0.4\n",
    "\n",
    "dorerank = False\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_RANK,'rank',context_ratio=context_ratio,dorerank =dorerank)\n",
    "\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_RANK,'rank',context_ratio=context_ratio,dorerank =dorerank)\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_RANK,'rank',context_ratio=context_ratio,dorerank =dorerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### laptime dataset\n",
    "\n",
    "freq = \"1min\"\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_LAPTIME,'laptime')\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_LAPTIME,'laptime')\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_LAPTIME,'laptime')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rank dataset\n",
    "\n",
    "freq = \"1min\"\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_RANK,'rank')\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_RANK,'rank')\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_RANK,'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rank dataset\n",
    "\n",
    "freq = \"1min\"\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_TIMEDIFF,'timediff')\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_TIMEDIFF,'timediff')\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_TIMEDIFF,'timediff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio=0.5\n",
    "\n",
    "freq = \"1min\"\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_TIMEDIFF,'timediff',train_ratio=0.5)\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_TIMEDIFF,'timediff',train_ratio=0.5)\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_TIMEDIFF,'timediff',train_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split by event\n",
    "\n",
    "freq = \"1min\"\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_TIMEDIFF,'timediff')\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_TIMEDIFF,'timediff')\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_TIMEDIFF,'timediff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## log trans\n",
    "log_transform = True\n",
    "\n",
    "\n",
    "freq = \"1min\"\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_TIMEDIFF,'timediff',log_transform = True)\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_TIMEDIFF,'timediff',log_transform = True)\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb_byevent(COL_TIMEDIFF,'timediff',log_transform = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split by ratio=0.5 -- laptime\n",
    "\n",
    "freq = \"1min\"\n",
    "tratio = 0.7\n",
    "tratio = 0.8\n",
    "\n",
    "useeid = False\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "plens=[2,5,10,20,30]\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_LAPTIME,'laptime',train_ratio=tratio)\n",
    "    \n",
    "#special\n",
    "interpolate = True\n",
    "useeid = False\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_LAPTIME,'laptime',train_ratio=tratio)\n",
    "\n",
    "useeid = True\n",
    "interpolate = False\n",
    "#ipstr = '-ip' if interpolate else '-noip'\n",
    "ipstr = '%s-%s'%('ip' if interpolate else 'noip', 'eid' if useeid else 'noeid')\n",
    "if useeid:\n",
    "    cardinality = [len(global_carids), len(laptime_data)]\n",
    "else:\n",
    "    cardinality = [len(global_carids)]\n",
    "\n",
    "for prediction_length in plens:\n",
    "    make_gluontsdb(COL_LAPTIME,'laptime',train_ratio=tratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_a(a, bitflag):\n",
    "    return (a & bitflag) ==  bitflag\n",
    "\n",
    "test_a(5, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.random.randint(5, size=(10,2, 5)).astype(np.float)\n",
    "mat[2,1,0] = np.nan\n",
    "_idx = np.array([0,1,2,8])\n",
    "selmat = mat[_idx]\n",
    "selmat[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isnan(selmat[:,1,:])\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(selmat[:,1,:], axis=0)\n",
    "true_rank = np.argsort(idx, axis=0).astype(np.float)\n",
    "true_rank[mask] = np.nan\n",
    "true_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selmat[:,1,:] = true_rank\n",
    "selmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[_idx] = selmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq, prediction_length, cardinality = 0,0,0\n",
    "\n",
    "def load_dataset(inputfile):\n",
    "    global freq, prediction_length, cardinality\n",
    "\n",
    "    with open(inputfile, 'rb') as f:\n",
    "        # have to specify it.\n",
    "        freq, prediction_length, cardinality,train_ds, test_ds = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    print(f\"number of cars: {cardinality}\")\n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "train_ds, test_ds = load_dataset('rank-oracle-noip-eid-all-all-f1min-t30-rIndy500-gluonts-indy-2018.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_iter =  iter(test_ds)\n",
    "rec = next(ds_iter)\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_get_ranklist(train_ds):\n",
    "    ds_iter =  iter(train_ds)\n",
    "    ranklist = []\n",
    "    for idx in range(len(train_ds)):\n",
    "        rec = next(ds_iter)\n",
    "\n",
    "        if rec['feat_static_cat'][1] == 0:\n",
    "            ranklist.append(rec['target'][0])\n",
    "\n",
    "    return np.array(sorted(ranklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ranklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testds(dorerank):\n",
    "    train_ds, test_ds,_,_ = make_dataset_byevent(-1, 2,freq,\n",
    "                                        useeid=True, run_ts=COL_RANK,\n",
    "                                        test_event='Indy500',log_transform =False,\n",
    "                                        context_ratio=0,\n",
    "                                                dorerank=dorerank)\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_train, rank_ds = get_testds(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawrank_train, rawrank_ds = get_testds(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_get_ranklist(rank_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_get_ranklist(rawrank_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_get_ranklist(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #alldata, rankdata, acldata, flagdata = stagedata['Indy500-2011']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(acldata.car_number.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acldata[acldata['completed_laps']==187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(','.join(list(sorted(rankdata[rankdata['completed_laps']==186].car_number.values))))\n",
    "print(','.join(list(sorted(rankdata[rankdata['completed_laps']==187].car_number.values))))\n",
    "print(','.join(list(sorted(rankdata[rankdata['completed_laps']==188].car_number.values))))\n",
    "print(','.join(list(sorted(rankdata[rankdata['completed_laps']==189].car_number.values))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata, rankdata, acldata, flagdata = stagedata['Indy500-2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kannan = rankdata[rankdata['car_number']==10]\n",
    "kannan[(kannan['completed_laps'] > 60) & (kannan['completed_laps']<70)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata, rankdata, acldata, flagdata = stagedata['Indy500-2013']\n",
    "rankdata[(rankdata['car_number']==16) & (rankdata['completed_laps']<10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdata[(rankdata['car_number']==16) & (rankdata['completed_laps']>88)& (rankdata['completed_laps']<97)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car16 = acldata[acldata['car_number']==16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(car16)):\n",
    "    if(car16.iloc[idx].lap_status == 'P'):\n",
    "        print(car16.iloc[idx].completed_laps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(car16)):\n",
    "    if(car16.iloc[idx].track_status == 'Y'):\n",
    "        print(car16.iloc[idx].completed_laps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata, rankdata, acldata, flagdata = stagedata['Indy500-2014']\n",
    "car16 = acldata[acldata['car_number']==14]\n",
    "car16[car16['lap_status']=='P']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the laptime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indy500-2018 car 12 -> eventid = 5(-2), carid = 7\n",
    "#LAP_STATUS = 3\n",
    "lapstatus = laptime_data[-2][2][7][3].astype(np.int)\n",
    "lapstatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(lapstatus==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAPS_INSTINT = 6,\n",
    "pitage = laptime_data[-2][2][7][6].astype(np.int)\n",
    "pitage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pitage==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAUTION_LAPS_INSTINT = 5\n",
    "trackstatus = laptime_data[-2][2][7][2].astype(np.int)\n",
    "trackstatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(trackstatus==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAUTION_LAPS_INSTINT = 5\n",
    "yfage = laptime_data[-2][2][7][5].astype(np.int)\n",
    "yfage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(yfage!=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'pitage'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(inputfile):\n",
    "    global freq, prediction_length, cardinality\n",
    "\n",
    "    with open(inputfile, 'rb') as f:\n",
    "        # have to specify it.\n",
    "        freq, prediction_length, cardinality,train_ds, test_ds = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    print(f\"number of cars: {cardinality}\")\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = load_dataset('pitage/laptime-indy500/laptime-oracle-ip-noeid-nolap-zero-f1min-t2-rIndy500-2018-gluonts-indy-2018.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_iter =  iter(train_ds)\n",
    "#ranklist = []\n",
    "#for idx in range(len(train_ds)):\n",
    "#    rec = next(ds_iter)\n",
    "rec = next(ds_iter)\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_iter =  iter(test_ds)\n",
    "#ranklist = []\n",
    "#for idx in range(len(train_ds)):\n",
    "#    rec = next(ds_iter)\n",
    "rec = next(ds_iter)\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stagedata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-762f2e0cfb65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### top cars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstagedata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Indy500-2018'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtop10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrankdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'completed_laps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtop10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stagedata' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
