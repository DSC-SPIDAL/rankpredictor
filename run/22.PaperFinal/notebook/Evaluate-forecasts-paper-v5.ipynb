{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating the output forecastings\n",
    "\n",
    "output of forecastings format:\n",
    "\n",
    "    carno\tstartlap\tstartrank\tendrank\tdiff\tsign\tpred_endrank\tpred_diff\tpred_sign\tendlap\tpred_endlap\n",
    "      11\t12\t31\t3.0\t5.0\t2.0\t1\t1.0\t-2.0\t-1\t49\t58\n",
    "    \n",
    "refer to:\n",
    "    19.RankNet/stage_model_regressor.ipynb\n",
    "    19.RankNet/RankForecasting-stint-paper-1kpitmodel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import inspect\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pathlib import Path\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "#from indycar.model.stint_predictor_fastrun import *\n",
    "#import indycar.model.stint_simulator as stint\n",
    "#import indycar.model.stint_simulator_shortterm_pitmodel as stint\n",
    "#import indycar.model.stint_simulator_paper as stint\n",
    "\n",
    "import indycar.model.quicktest_simulator as stint\n",
    "\n",
    "# import all functions \n",
    "#from indycar.model.global_variables import _hi\n",
    "import indycar.model.global_variables as gvar\n",
    "from indycar.model.quicktest_modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfout(datafile, basedir = '../result/22.PaperFinal/'):\n",
    "    datafile = basedir + datafile  \n",
    "    #with open('laptime_rank_timediff_fulltest-oracle-%s.pickle'%year, 'rb') as f:\n",
    "    with open(datafile, 'rb') as f:\n",
    "        # The protocol version used is detected automatically, so we do not\n",
    "        # have to specify it.\n",
    "        dfout = pickle.load(f, encoding='latin1') \n",
    "        \n",
    "        return dfout[0]\n",
    "    \n",
    "def load_dfout_all(datafile):\n",
    "    #with open('laptime_rank_timediff_fulltest-oracle-%s.pickle'%year, 'rb') as f:\n",
    "    datafile = '../result/22.PaperFinal/' + datafile  \n",
    "    with open(datafile, 'rb') as f:\n",
    "        # The protocol version used is detected automatically, so we do not\n",
    "        # have to specify it.\n",
    "        dfout = pickle.load(f, encoding='latin1') \n",
    "        \n",
    "        return dfout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. evalute stint restuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_info = {\n",
    "    'Phoenix':(256, 1.022),'Indy500':(500,2.5),'Texas':(372,1.5),\n",
    "    'Iowa':(268,0.894),'Pocono':(500,2.5),'Gateway':(310,1.25)\n",
    "}\n",
    "\n",
    "years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "events = [f'Indy500-{x}' for x in years]\n",
    "\n",
    "events.extend(['Phoenix-2018','Texas-2018','Texas-2019','Pocono-2018','Pocono-2019','Iowa-2018','Iowa-2019',\n",
    "              'Gateway-2018','Gateway-2019'])\n",
    "\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "\n",
    "dataroot = 'test/'\n",
    "\n",
    "_train_events = [events_id[x] for x in [f'Indy500-{x}' for x in ['2013','2014','2015','2016','2017']]]\n",
    "_test_event = 'Gateway-2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvar.events = events\n",
    "gvar.events_id  = events_id\n",
    "gvar.events_info = events_info\n",
    "\n",
    "\n",
    "gvar.maxlap = get_event_info(_test_event)[0]\n",
    "#gvar.LAPTIME_DATASET = LAPTIME_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load ml models\n",
    "_trim = 0\n",
    "plen = 2\n",
    "_context_len = 60\n",
    "\n",
    "_include_final = True\n",
    "_include_stintlen = True\n",
    "#_include_stintlen = False\n",
    "include_str = '1' if _include_final else '0'\n",
    "stint_str = '1' if _include_stintlen else ''\n",
    "#outfile=f'stint-dfout-mlmodels-indy500-tr2013_2017-te2018_2019-end{include_str}-normal-t{_trim}.pickle'\n",
    "#preddf = load_dfout(outfile)\n",
    "#outfile=f'stint-dfout-mlmodels-indy500-tr2013_2017-te2018_2019-end{include_str}-oracle-t{_trim}.pickle'\n",
    "#preddf_oracle = load_dfout(outfile)\n",
    "version = f'IndyCar-d{len(events)}'\n",
    "outfile=f'{dataroot}/shortterm-dfout-mlmodels-{version}-end{include_str}-rerank-t{plen}-c{_context_len}.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfile=f'stint-dfout-mlmodels-indy500-tr2013_2017-te2018_2019-end{include_str}-normal-t{_trim}-newtry-tuned.pickle'\n",
    "#preddf = load_dfout(outfile)\n",
    "#outfile=f'stint-dfout-mlmodels-indy500-tr2013_2017-te2018_2019-end{include_str}-oracle-t{_trim}-newtry-tuned.pickle'\n",
    "#preddf_oracle = load_dfout(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=f'{dataroot}/stint-dfout-mlmodels-{version}-end{include_str}-normal-t{_trim}-tuned.pickle'\n",
    "preddf = load_dfout(outfile)\n",
    "outfile=f'{dataroot}/stint-dfout-mlmodels-{version}-end{include_str}-oracle-t{_trim}-tuned.pickle'\n",
    "preddf_oracle = load_dfout(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = preddf['2018']\n",
    "for clf in dfout:\n",
    "    df = dfout[clf]\n",
    "    print('model:', clf)\n",
    "    stint.get_evalret(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preddf['2018']['svr_lin']\n",
    "df[df['carno']==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = preddf['2019']\n",
    "for clf in dfout:\n",
    "    df = dfout[clf]\n",
    "    print('model:', clf)\n",
    "    stint.get_evalret(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ranknet results\n",
    "# outfile=f'stint-dfout-ranknet-indy500-tr2013_2017-te2018_2019-end{include_str}-normal-t{_trim}.pickle'\n",
    "outfile = 'stint-dfout-ranknet-indy500-timediff-noinlap-nopitage-20182019-alldata.pickle'\n",
    "ranknetdf, ranknet_acc, ranknet_ret  = load_dfout(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranknetdf['2018'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ranknet_ret['oracle-TIMEDIFF-2018-noinlap-nopitage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranknet_ret['oracle-TIMEDIFF-2018-noinlap-nopitage'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = ranknetdf['2018']\n",
    "for clf in dfout:\n",
    "    df = dfout[clf]\n",
    "    print('model:', clf)\n",
    "    stint.get_evalret(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = ranknetdf['2019']\n",
    "for clf in dfout:\n",
    "    df = dfout[clf]\n",
    "    print('model:', clf)\n",
    "    stint.get_evalret(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranknetdf['2018']['pitmodel_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stint result, just cannot do rerank, \n",
    "### instead use int()\n",
    "df = do_rerank(ranknetdf['2018']['pitmodel_mean'],short=False)\n",
    "stint.get_evalret(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sync with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bydf(testdf, bydf, forcematch=True, force2int=False):\n",
    "    #collect only records in bydf <carno and startlap>\n",
    "    cars = set(bydf.carno.values)\n",
    "    startlaps = {}\n",
    "    for car in cars:\n",
    "        startlaps[car] = set(bydf[bydf['carno']==car].startlap.values)\n",
    "        \n",
    "    retdf = []\n",
    "    for car in cars:\n",
    "        for startlap in startlaps[car]:    \n",
    "            dfrec = testdf[(testdf['carno']==car) & (testdf['startlap']==startlap)]\n",
    "            \n",
    "            #check match\n",
    "            if forcematch:\n",
    "                a = testdf[(testdf['carno']==car) & (testdf['startlap']==startlap)].to_numpy().astype(int)\n",
    "                b = bydf[(bydf['carno']==car) & (bydf['startlap']==startlap)].to_numpy().astype(int)\n",
    "\n",
    "                if len(a)!=0 and len(b)!=0:\n",
    "                    #compare \n",
    "                    #startrank, endrank\n",
    "                    if not ((a[0][2] == b[0][2]) and (a[0][3] == b[0][3])):\n",
    "                        #print('mismatch:', a, b)            \n",
    "                        continue\n",
    "            \n",
    "            retdf.append(dfrec)\n",
    "        \n",
    "    dfout = pd.concat(retdf)\n",
    "    \n",
    "    if force2int:\n",
    "        dfdata = dfout.to_numpy().astype(int)\n",
    "        dfout = pd.DataFrame(dfdata, columns =['carno', 'startlap', 'startrank',    \n",
    "                                         'endrank', 'diff', 'sign',\n",
    "                                         'pred_endrank', 'pred_diff', 'pred_sign',\n",
    "                                         'endlap','pred_endlap'\n",
    "                                        ])\n",
    "    \n",
    "    dfout = dfout.sort_values(by=['carno','startlap'])\n",
    "    \n",
    "    print('df size:', len(dfout))\n",
    "    #return acc\n",
    "    accret = stint.get_evalret(dfout)[0]\n",
    "    \n",
    "    return dfout  , accret\n",
    "\n",
    "\n",
    "def eval_sync(testdf, errlist, force2int=False):\n",
    "    \"\"\"\n",
    "    eval df result by sync with the errlist detected\n",
    "    remove the records in errlist\n",
    "    \n",
    "    \"\"\"\n",
    "    #collect only records in bydf <carno and startlap>\n",
    "    cars = set(testdf.carno.values)\n",
    "    startlaps = {}\n",
    "    for car in cars:\n",
    "        startlaps[car] = set(testdf[testdf['carno']==car].startlap.values)\n",
    "        \n",
    "    retdf = []\n",
    "    for car in cars:\n",
    "        for startlap in startlaps[car]:    \n",
    "            dfrec = testdf[(testdf['carno']==car) & (testdf['startlap']==startlap)]\n",
    "            \n",
    "            #check match\n",
    "            this_rec = [car, startlap]\n",
    "            if this_rec in errlist:\n",
    "                continue\n",
    "            \n",
    "            retdf.append(dfrec)\n",
    "        \n",
    "    dfout = pd.concat(retdf)\n",
    "    \n",
    "    if force2int:\n",
    "        dfdata = dfout.to_numpy().astype(int)\n",
    "        dfout = pd.DataFrame(dfdata, columns =['carno', 'startlap', 'startrank',    \n",
    "                                         'endrank', 'diff', 'sign',\n",
    "                                         'pred_endrank', 'pred_diff', 'pred_sign',\n",
    "                                         'endlap','pred_endlap'\n",
    "                                        ])\n",
    "    \n",
    "    dfout = dfout.sort_values(by=['carno','startlap'])\n",
    "    \n",
    "    print('df size:', len(dfout))\n",
    "    #return acc\n",
    "    accret = stint.get_evalret(dfout)[0]\n",
    "    \n",
    "    return dfout  , accret\n",
    "\n",
    "def cmp_df(testdf, bydf):\n",
    "    \"\"\"\n",
    "    df can be different, minor difference for the rank when RankNet removes short ts\n",
    "    \"\"\"\n",
    "    #collect only records in bydf <carno and startlap>\n",
    "    cars = set(bydf.carno.values)\n",
    "    startlaps = {}\n",
    "    for car in cars:\n",
    "        startlaps[car] = set(bydf[bydf['carno']==car].startlap.values)\n",
    "        \n",
    "    err_list = []\n",
    "    retdf = []\n",
    "    errcnt = 0\n",
    "    for car in cars:\n",
    "        for startlap in startlaps[car]:    \n",
    "            a = testdf[(testdf['carno']==car) & (testdf['startlap']==startlap)].to_numpy().astype(int)\n",
    "            b = bydf[(bydf['carno']==car) & (bydf['startlap']==startlap)].to_numpy().astype(int)\n",
    "            \n",
    "            if len(a)!=0 and len(b)!=0:\n",
    "                #compare \n",
    "                #startrank, endrank\n",
    "                if not ((a[0][2] == b[0][2]) and (a[0][3] == b[0][3])):\n",
    "                    print('mismatch:', a, b)\n",
    "                    errcnt += 1\n",
    "                    err_list.append([car, startlap])\n",
    "            else:\n",
    "                errcnt += 1\n",
    "                print('mismatch empty:', a, b)\n",
    "                err_list.append([car, startlap])\n",
    "                \n",
    "                \n",
    "    print('errcnt:', errcnt)\n",
    "    return errcnt, err_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### problem of df mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errlist = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errcnt, errlist['2019'] = cmp_df(ranknetdf['2019']['pitmodel_mean'], preddf['2019']['lasso'])\n",
    "errcnt, errlist['2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errcnt, errlist['2018'] = cmp_df(ranknetdf['2018']['pitmodel_mean'], preddf['2018']['lasso'])\n",
    "errcnt, errlist['2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model,SignAcc,MAE,50-Risk,90-Risk\n",
    "# \n",
    "cols = ['Year','Model','SignAcc','MAE','50-Risk','90-Risk']\n",
    "models = {'currank':'CurRank','rf':'RandomForest','svr_lin':'SVM','xgb':'XGBoost'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retdata = []\n",
    "for year in ['2018','2019']:\n",
    "    for clf in ['currank','rf','svr_lin','xgb']:\n",
    "        print('year:',year,'clf:',clf)\n",
    "        dfout, accret = eval_sync(preddf[year][clf],errlist[year])\n",
    "        fsamples, ftss = df2samples(dfout)\n",
    "        _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        \n",
    "        retdata.append([year,models[clf],accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "        \n",
    "    #ml models -oracle\n",
    "    for clf in ['rf','svr_lin','xgb']:\n",
    "        print('year:',year,'clf:',clf)\n",
    "        dfout, accret = eval_sync(preddf_oracle[year][clf],errlist[year])\n",
    "        fsamples, ftss = df2samples(dfout)\n",
    "        _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "        \n",
    "        retdata.append([year,models[clf]+'-Oracle',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "        \n",
    "    dfout, accret = eval_sync(ranknetdf[year]['pitmodel_mean'], errlist[year],force2int=True)\n",
    "    #fsamples, ftss = df2samples(dfout)\n",
    "    fsamples, ftss = runs2samples(ranknet_ret[f'pitmodel-TIMEDIFF-{year}-noinlap-nopitage'],errlist[f'{year}'])\n",
    "    _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "    retdata.append([year,'RankNet-MLP',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "    \n",
    "    dfout, accret = eval_sync(ranknetdf[year]['oracle_mean'], errlist[year],force2int=True)\n",
    "    #fsamples, ftss = df2samples(dfout)\n",
    "    fsamples, ftss = runs2samples(ranknet_ret[f'oracle-TIMEDIFF-{year}-noinlap-nopitage'],errlist[f'{year}'])\n",
    "    _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "    retdata.append([year,'RankNet-Oracle',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "        \n",
    "        \n",
    "stint_result = pd.DataFrame(data=retdata, columns=cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stint_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stint_result.to_csv('stint_experiment_result_v4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis of car12, pitstop=94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxs = ranknet_ret[f'pitmodel-TIMEDIFF-2018-noinlap-nopitage']\n",
    "dfx = pd.concat(dfxs)\n",
    "df12 = dfx[dfx['carno']==12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdata = df12.pred_endlap\n",
    "plt.hist(hdata, bins=range(min(hdata),max(hdata)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pitstops:', set(df12.startlap.values))\n",
    "pit94 = df12[df12['startlap']==93]\n",
    "observed = pit94[['endlap','endrank']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bins_labels(bins, **kwargs):\n",
    "    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n",
    "    #plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "# https://stackoverflow.com/questions/27083051/matplotlib-xticks-not-lining-up-with-histogram\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 7))\n",
    "hdata = pit94.pred_endlap.astype(int)\n",
    "\n",
    "bins = np.arange(min(hdata),max(hdata))\n",
    "axs[0].hist(hdata, bins - 0.5, align='mid')\n",
    "axs[0].set_xticks(bins)\n",
    "axs[0].plot(observed[0][0],2,'*',color='r')\n",
    "axs[0].set_xlabel('Predicted Lap of Next PitStop')\n",
    "axs[0].set_ylabel('Probability(%)')\n",
    "\n",
    "\n",
    "hdata = pit94.pred_endrank\n",
    "axs[1].hist(hdata, bins=np.arange(0,10)-0.5,label='forecast')\n",
    "axs[1].set_xticks(np.arange(0,10))\n",
    "axs[1].plot(observed[0][1],10,'*',color='r',label='observed')\n",
    "axs[1].set_xlabel('Predicted Rank Posisiton of Next PitStop')\n",
    "axs[1].set_ylabel('Probability(%)')\n",
    "\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.tight_layout()\n",
    "plt.savefig('pit94_stint_forecast.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "# https://stackoverflow.com/questions/27083051/matplotlib-xticks-not-lining-up-with-histogram\n",
    "fig, axs = plt.subplots(1,2, figsize=(12, 3))\n",
    "hdata = pit94.pred_endlap.astype(int)\n",
    "\n",
    "bins = np.arange(min(hdata),max(hdata))\n",
    "axs[0].hist(hdata, bins - 0.5, align='mid')\n",
    "axs[0].set_xticks(bins)\n",
    "axs[0].plot(observed[0][0],2,'*',color='r')\n",
    "axs[0].set_xlabel('Predicted Lap of Next PitStop')\n",
    "axs[0].set_ylabel('Probability(%)')\n",
    "\n",
    "\n",
    "hdata = pit94.pred_endrank\n",
    "axs[1].hist(hdata, bins=np.arange(0,10)-0.5,label='forecast')\n",
    "axs[1].set_xticks(np.arange(0,10))\n",
    "axs[1].plot(observed[0][1],10,'*',color='r',label='observed')\n",
    "axs[1].set_xlabel('Predicted Rank Posisiton of Next PitStop')\n",
    "axs[1].set_ylabel('Probability(%)')\n",
    "\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.tight_layout()\n",
    "plt.savefig('pit94_stint_forecast_h.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdata = pit94.pred_endrank\n",
    "plt.hist(hdata, bins=range(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit94.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation shortterm forecasting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model,SignAcc,MAE,50-Risk,90-Risk\n",
    "# \n",
    "cols = ['Year','Model','SignAcc','MAE','50-Risk','90-Risk']\n",
    "models = {'currank':'CurRank','rf':'RandomForest','svr':'SVM','xgb':'XGBoost'}\n",
    "#plen=2\n",
    "#usemeanstr='median'\n",
    "usemeanstr='mean'\n",
    "stint_result = {}\n",
    "\n",
    "_context_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: Gateway-2018 clf: currank\n",
      "model: acc={0.92}, mae={0.33}, rmse={1.17},r2={0.96}, {236}\n",
      "            naive: acc={0.92}, mae={0.33}, rmse={1.17},r2={0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.03616936523185792, 0.03613187431169014, 0.036094383391522364])\n",
      "year: Gateway-2018 clf: rf\n",
      "model: acc={0.50}, mae={1.83}, rmse={2.96},r2={0.73}, {236}\n",
      "            naive: acc={0.92}, mae={0.33}, rmse={1.17},r2={0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.1971928673524381, 0.1971553764322703, 0.19711788551210258])\n",
      "year: Gateway-2018 clf: svr\n",
      "model: acc={0.92}, mae={0.33}, rmse={1.17},r2={0.96}, {236}\n",
      "            naive: acc={0.92}, mae={0.33}, rmse={1.17},r2={0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.03621622888206763, 0.036178737961899854, 0.036141247041732084])\n",
      "year: Gateway-2018 clf: xgb\n",
      "model: acc={0.14}, mae={3.66}, rmse={5.30},r2={0.13}, {236}\n",
      "            naive: acc={0.92}, mae={0.33}, rmse={1.17},r2={0.96}\n",
      "dict_values([0.3955666986901609, 0.39552920776999323, 0.3954917168498255])\n",
      "           Year         Model   SignAcc       MAE   50-Risk   90-Risk\n",
      "0  Gateway-2018       CurRank  0.915254  0.334490  0.036132  0.036094\n",
      "1  Gateway-2018  RandomForest  0.495763  1.827332  0.197155  0.197118\n",
      "2  Gateway-2018           SVM  0.915254  0.334924  0.036179  0.036141\n",
      "3  Gateway-2018       XGBoost  0.139831  3.662039  0.395529  0.395492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    }
   ],
   "source": [
    "for plen in [2]:\n",
    "    outfile=f'{dataroot}/shortterm-dfout-mlmodels-{version}-end{include_str}-rerank-t{plen}-c{_context_len}.pickle'\n",
    "    shortterm_df = load_dfout(outfile, basedir = './')\n",
    "\n",
    "    retdata = []\n",
    "    #for year in ['Indy500-2018','Indy500-2019','Phoenix-2018']:\n",
    "    for year in [_test_event]:\n",
    "        \n",
    "        testevent = year\n",
    "        gvar.maxlap = get_event_info(testevent)[0]\n",
    "        \n",
    "        for clf in ['currank','rf','svr','xgb']:\n",
    "            print('year:',year,'clf:',clf)\n",
    "            #dfout, accret = eval_sync(preddf[year][clf],errlist[year])\n",
    "            dfout = shortterm_df[year][clf]\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "            fsamples, ftss = df2samples(dfout)\n",
    "            _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "\n",
    "            retdata.append([year,models[clf],accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "            \n",
    "        if False:\n",
    "            #oracle\n",
    "            outfile=f'shortterm-dfout-ranknet-indy500-rank-inlap-nopitage-20182019-oracle-l10-alldata.pickle'\n",
    "            _all = load_dfout_all(outfile)[0]\n",
    "            ranknetdf, acc, ret, pret = _all[0],_all[1],_all[2],_all[3]\n",
    "            allsamples, alltss = get_allsamples(year=year, model='oracle')\n",
    "            #_, pret[mid]= prisk_direct_bysamples(ret[mid][0][1], ret[mid][0][2])\n",
    "            _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)\n",
    "\n",
    "            dfout = do_rerank(ranknetdf[year]['oracle_mean'])\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "            #fsamples, ftss = runs2samples_ex(ranknet_ret[f'oracle-RANK-{year}-inlap-nopitage'],[])\n",
    "            #_, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "            retdata.append([year,'RankNet-Oracle',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "            #pitmodel\n",
    "            outfile=f'shortterm-dfout-ranknet-indy500-rank-inlap-nopitage-20182019-alldata.pickle'\n",
    "            _all = load_dfout_all(outfile)[0]\n",
    "            ranknetdf, acc, ret = _all[0],_all[1],_all[2]\n",
    "\n",
    "            dfout = do_rerank(ranknetdf[year]['pitmodel_mean'])\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "            #fsamples, ftss = runs2samples_ex(ranknet_ret[f'pitmodel-RANK-{year}-inlap-nopitage'],[])\n",
    "            #_, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "            allsamples, alltss = get_allsamples(year=year, model='pitmodel')\n",
    "            #_, pret[mid]= prisk_direct_bysamples(ret[mid][0][1], ret[mid][0][2])\n",
    "            _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)        \n",
    "            retdata.append([year,'RankNet-MLP',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "            #print('oracle:', ranknet_pret[f'oracle-RANK-2018-inlap-nopitage'])\n",
    "            #print('pitmodel:', ranknet_pret[f'pitmodel-RANK-2018-inlap-nopitage'])\n",
    "\n",
    "    stint_result[plen] = pd.DataFrame(data=retdata, columns=cols)\n",
    "    \n",
    "    print(stint_result[plen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Model</th>\n",
       "      <th>SignAcc</th>\n",
       "      <th>MAE</th>\n",
       "      <th>50-Risk</th>\n",
       "      <th>90-Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>CurRank</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.334490</td>\n",
       "      <td>0.036132</td>\n",
       "      <td>0.036094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.495763</td>\n",
       "      <td>1.827332</td>\n",
       "      <td>0.197155</td>\n",
       "      <td>0.197118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.334924</td>\n",
       "      <td>0.036179</td>\n",
       "      <td>0.036141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>3.662039</td>\n",
       "      <td>0.395529</td>\n",
       "      <td>0.395492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Year         Model   SignAcc       MAE   50-Risk   90-Risk\n",
       "0  Gateway-2018       CurRank  0.915254  0.334490  0.036132  0.036094\n",
       "1  Gateway-2018  RandomForest  0.495763  1.827332  0.197155  0.197118\n",
       "2  Gateway-2018           SVM  0.915254  0.334924  0.036179  0.036141\n",
       "3  Gateway-2018       XGBoost  0.139831  3.662039  0.395529  0.395492"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stint_result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for plen in stint_result:\n",
    "    df = stint_result[plen]\n",
    "    df['plen'] = plen\n",
    "    \n",
    "    dfs.append(df)\n",
    "    \n",
    "alldf = pd.concat(dfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf.to_csv(f'evaluation_result_shortterm_plen2-8_all_v4-c{_context_len}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split by pit-covered-laps and normal laps\n",
    "\n",
    "define pit covered laps := two laps before and after a pit\n",
    "\n",
    "split shortterm-results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pit laps, pit-covered-laps\n",
    "# pitdata[year] = [pitlaps, pitcoveredlaps]\n",
    "covergap = 1\n",
    "with open(f'{dataroot}/pitcoveredlaps-alldata-g{covergap}.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    pitdata = pickle.load(f, encoding='latin1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_samples(full_samples, full_tss, clearidx):\n",
    "    \"\"\"\n",
    "    clear the laps in clearidx\n",
    "    \"\"\"\n",
    "    \n",
    "    carlist = full_tss.keys()\n",
    "    \n",
    "    for carid, carno in enumerate(carlist):\n",
    "        forecast = full_samples[carno]\n",
    "        target = full_tss[carno]\n",
    "        \n",
    "        forecast[:, clearidx] = np.nan\n",
    "        target[clearidx] = np.nan\n",
    "        \n",
    "        full_samples[carno] = forecast\n",
    "        full_tss[carno] = target\n",
    "        \n",
    "    return full_samples, full_tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model,SignAcc,MAE,50-Risk,90-Risk\n",
    "# \n",
    "cols = ['Year','Model','SignAcc','MAE','50-Risk','90-Risk']\n",
    "models = {'currank':'CurRank','rf':'RandomForest','svr':'SVM','xgb':'XGBoost'}\n",
    "#plen=2\n",
    "#usemeanstr='median'\n",
    "usemeanstr='mean'\n",
    "stint_split_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: Gateway-2018 clf: currank\n",
      "dict_values([0.004852312550544923, 0.004852312550544922, 0.004852312550544922])\n",
      "model: acc={1.00}, mae={0.04}, rmse={0.25},r2={1.00}, {121}\n",
      "            naive: acc={1.00}, mae={0.04}, rmse={0.25},r2={1.00}\n",
      "year: Gateway-2018 clf: rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.12454268879731968, 0.12454268879731968, 0.1245426887973197])\n",
      "model: acc={0.42}, mae={0.98}, rmse={1.64},r2={0.92}, {121}\n",
      "            naive: acc={1.00}, mae={0.04}, rmse={0.25},r2={1.00}\n",
      "year: Gateway-2018 clf: svr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.004852312550544923, 0.004852312550544922, 0.004852312550544922])\n",
      "model: acc={1.00}, mae={0.04}, rmse={0.25},r2={1.00}, {121}\n",
      "            naive: acc={1.00}, mae={0.04}, rmse={0.25},r2={1.00}\n",
      "year: Gateway-2018 clf: xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.2549389609889475, 0.2549389609889475, 0.2549389609889475])\n",
      "model: acc={0.12}, mae={2.18}, rmse={3.28},r2={0.67}, {121}\n",
      "            naive: acc={1.00}, mae={0.04}, rmse={0.25},r2={1.00}\n",
      "           Year         Model   SignAcc       MAE   50-Risk   90-Risk\n",
      "0  Gateway-2018       CurRank  1.000000  0.036590  0.004852  0.004852\n",
      "1  Gateway-2018  RandomForest  0.421488  0.975468  0.124543  0.124543\n",
      "2  Gateway-2018           SVM  1.000000  0.036590  0.004852  0.004852\n",
      "3  Gateway-2018       XGBoost  0.115702  2.180457  0.254939  0.254939\n",
      "year: Gateway-2018 clf: currank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.07219521872830076, 0.07211586152167444, 0.07203650431504811])\n",
      "model: acc={0.79}, mae={0.78}, rmse={1.83},r2={0.89}, {94}\n",
      "            naive: acc={0.79}, mae={0.78}, rmse={1.83},r2={0.89}\n",
      "year: Gateway-2018 clf: rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.30153754587838505, 0.3014581886717588, 0.30137883146513245])\n",
      "model: acc={0.54}, mae={2.86}, rmse={4.06},r2={0.48}, {94}\n",
      "            naive: acc={0.79}, mae={0.78}, rmse={1.83},r2={0.89}\n",
      "year: Gateway-2018 clf: svr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.07229441523658367, 0.07221505802995734, 0.07213570082333103])\n",
      "model: acc={0.79}, mae={0.78}, rmse={1.83},r2={0.89}, {94}\n",
      "            naive: acc={0.79}, mae={0.78}, rmse={1.83},r2={0.89}\n",
      "year: Gateway-2018 clf: xgb\n",
      "dict_values([0.5773038389048707, 0.5772244816982443, 0.5771451244916179])\n",
      "model: acc={0.16}, mae={5.66}, rmse={7.23},r2={-0.64}, {94}\n",
      "            naive: acc={0.79}, mae={0.78}, rmse={1.83},r2={0.89}\n",
      "           Year         Model   SignAcc       MAE   50-Risk   90-Risk\n",
      "0  Gateway-2018       CurRank  0.787234  0.780595  0.072116  0.072037\n",
      "1  Gateway-2018  RandomForest  0.542553  2.855568  0.301458  0.301379\n",
      "2  Gateway-2018           SVM  0.787234  0.781698  0.072215  0.072136\n",
      "3  Gateway-2018       XGBoost  0.159574  5.656009  0.577224  0.577145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/pengb/hpda/indycar/predictor/src/indycar/model/quicktest_modules.py:1699: RuntimeWarning: invalid value encountered in less_equal\n",
      "  * ((target <= quantile_forecast) - q)\n"
     ]
    }
   ],
   "source": [
    "for laptype in ['normal','pit']:\n",
    "    \n",
    "    plen = 2\n",
    "    \n",
    "    outfile=f'{dataroot}/shortterm-dfout-mlmodels-{version}-end{include_str}-rerank-t{plen}-c{_context_len}.pickle'\n",
    "    shortterm_df = load_dfout(outfile, basedir = './')\n",
    "\n",
    "    retdata = []\n",
    "    #for year in ['2018','2019']:\n",
    "    for year in [_test_event]:\n",
    "        \n",
    "        testevent = year\n",
    "        gvar.maxlap = get_event_info(testevent)[0]\n",
    "        \n",
    "        # select the set\n",
    "        pitcoveredlaps = pitdata[year][1]\n",
    "        normallaps = set([x for x in range(1,201)]) - pitcoveredlaps\n",
    "        \n",
    "        if laptype == 'normal':\n",
    "            sellaps = normallaps\n",
    "            clearlaps = pitcoveredlaps\n",
    "        else:\n",
    "            sellaps = pitcoveredlaps\n",
    "            clearlaps = normallaps\n",
    "        \n",
    "        \n",
    "        # pitcoveredlaps start idx = 1\n",
    "        startlaps = [x-plen-1 for x in sellaps]\n",
    "        #sellapidx = np.array([x-1 for x in sellaps])\n",
    "        clearidx = np.array([x-1 for x in clearlaps])\n",
    "        \n",
    "        for clf in ['currank','rf','svr','xgb']:\n",
    "            print('year:',year,'clf:',clf)\n",
    "            \n",
    "            dfout = shortterm_df[year][clf]\n",
    "            fsamples, ftss = df2samples(dfout)\n",
    "            \n",
    "            fsamples, ftss = clear_samples(fsamples, ftss,clearidx)\n",
    "            _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "            \n",
    "            # split dfout by startlap\n",
    "            # curlap = int(dfrec.startlap.values[0] + prediction_len)\n",
    "            dfout = dfout[dfout['startlap'].isin(startlaps)]\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "            retdata.append([year,models[clf],accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "            \n",
    "        if False:\n",
    "            #oracle\n",
    "            outfile=f'shortterm-dfout-ranknet-indy500-rank-inlap-nopitage-20182019-oracle-l10-alldata.pickle'\n",
    "            _all = load_dfout_all(outfile)[0]\n",
    "            ranknetdf, acc, ret, pret = _all[0],_all[1],_all[2],_all[3]\n",
    "\n",
    "            dfout = do_rerank(ranknetdf[year]['oracle_mean'])\n",
    "\n",
    "            allsamples, alltss = get_allsamples(year=year, model='oracle')\n",
    "\n",
    "            allsamples, alltss = clear_samples(allsamples, alltss,clearidx)\n",
    "            _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)\n",
    "\n",
    "            dfout = dfout[dfout['startlap'].isin(startlaps)]\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "            retdata.append([year,'RankNet-Oracle',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "            #pitmodel\n",
    "            outfile=f'shortterm-dfout-ranknet-indy500-rank-inlap-nopitage-20182019-alldata.pickle'\n",
    "            _all = load_dfout_all(outfile)[0]\n",
    "            ranknetdf, acc, ret = _all[0],_all[1],_all[2]\n",
    "\n",
    "            dfout = do_rerank(ranknetdf[year]['pitmodel_mean'])\n",
    "\n",
    "            allsamples, alltss = get_allsamples(year=year, model='pitmodel')\n",
    "            allsamples, alltss = clear_samples(allsamples, alltss,clearidx)\n",
    "            _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)        \n",
    "\n",
    "            dfout = dfout[dfout['startlap'].isin(startlaps)]\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "            retdata.append([year,'RankNet-MLP',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "\n",
    "            #print('oracle:', ranknet_pret[f'oracle-RANK-2018-inlap-nopitage'])\n",
    "            #print('pitmodel:', ranknet_pret[f'pitmodel-RANK-2018-inlap-nopitage'])\n",
    "\n",
    "    stint_split_result[laptype] = pd.DataFrame(data=retdata, columns=cols)\n",
    "    \n",
    "    print(stint_split_result[laptype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Model</th>\n",
       "      <th>SignAcc</th>\n",
       "      <th>MAE</th>\n",
       "      <th>50-Risk</th>\n",
       "      <th>90-Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>CurRank</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036590</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>0.004852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.421488</td>\n",
       "      <td>0.975468</td>\n",
       "      <td>0.124543</td>\n",
       "      <td>0.124543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036590</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>0.004852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.115702</td>\n",
       "      <td>2.180457</td>\n",
       "      <td>0.254939</td>\n",
       "      <td>0.254939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Year         Model   SignAcc       MAE   50-Risk   90-Risk\n",
       "0  Gateway-2018       CurRank  1.000000  0.036590  0.004852  0.004852\n",
       "1  Gateway-2018  RandomForest  0.421488  0.975468  0.124543  0.124543\n",
       "2  Gateway-2018           SVM  1.000000  0.036590  0.004852  0.004852\n",
       "3  Gateway-2018       XGBoost  0.115702  2.180457  0.254939  0.254939"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stint_split_result['normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Model</th>\n",
       "      <th>SignAcc</th>\n",
       "      <th>MAE</th>\n",
       "      <th>50-Risk</th>\n",
       "      <th>90-Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>CurRank</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.780595</td>\n",
       "      <td>0.072116</td>\n",
       "      <td>0.072037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.542553</td>\n",
       "      <td>2.855568</td>\n",
       "      <td>0.301458</td>\n",
       "      <td>0.301379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.781698</td>\n",
       "      <td>0.072215</td>\n",
       "      <td>0.072136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gateway-2018</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.159574</td>\n",
       "      <td>5.656009</td>\n",
       "      <td>0.577224</td>\n",
       "      <td>0.577145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Year         Model   SignAcc       MAE   50-Risk   90-Risk\n",
       "0  Gateway-2018       CurRank  0.787234  0.780595  0.072116  0.072037\n",
       "1  Gateway-2018  RandomForest  0.542553  2.855568  0.301458  0.301379\n",
       "2  Gateway-2018           SVM  0.787234  0.781698  0.072215  0.072136\n",
       "3  Gateway-2018       XGBoost  0.159574  5.656009  0.577224  0.577145"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stint_split_result['pit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for laptype in ['normal','pit']:\n",
    "    df = stint_split_result[laptype]\n",
    "    df['laptype'] = laptype\n",
    "    \n",
    "    dfs.append(df)\n",
    "    \n",
    "alldf = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf.to_csv(f'evaluation_result_shortterm_plen2-split-g{covergap}_v4-c{_context_len}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test new oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_split_result = {}\n",
    "for laptype in ['normal','pit']:\n",
    "    \n",
    "    plen = 2\n",
    "    \n",
    "    retdata = []\n",
    "    for year in ['2018']:\n",
    "        \n",
    "        # select the set\n",
    "        pitcoveredlaps = pitdata[year][1]\n",
    "        normallaps = set([x for x in range(1,201)]) - pitcoveredlaps\n",
    "        \n",
    "        if laptype == 'normal':\n",
    "            sellaps = normallaps\n",
    "            clearlaps = pitcoveredlaps\n",
    "        else:\n",
    "            sellaps = pitcoveredlaps\n",
    "            clearlaps = normallaps\n",
    "        \n",
    "        \n",
    "        # pitcoveredlaps start idx = 1\n",
    "        startlaps = [x-plen-1 for x in sellaps]\n",
    "        #sellapidx = np.array([x-1 for x in sellaps])\n",
    "        clearidx = np.array([x-1 for x in clearlaps])\n",
    "        print('year:%s, type:%s, lapscnt:%s'%(year, laptype, len(startlaps)))\n",
    "        \n",
    "        #oracle\n",
    "        outfile=f'shortterm-dfout-ranknet-indy500-rank-inlap-nopitage-20182019-oracle-l10-alldata-weighted.pickle'\n",
    "        _all = load_dfout_all(outfile)[0]\n",
    "        ranknetdf, acc, ret, pret = _all[0],_all[1],_all[2],_all[3]\n",
    "\n",
    "        dfout = do_rerank(ranknetdf[year]['oracle_mean'])\n",
    "\n",
    "        allsamples, alltss = get_allsamples(year=year, model='oracle')\n",
    "\n",
    "        allsamples, alltss = clear_samples(allsamples, alltss,clearidx)\n",
    "        _, prisk_vals = prisk_direct_bysamples(allsamples, alltss)\n",
    "\n",
    "        dfout = dfout[dfout['startlap'].isin(startlaps)]\n",
    "        accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "        print(year, laptype,'RankNet-Oracle',accret[0], accret[1], prisk_vals[1], prisk_vals[2])\n",
    "        retdata.append([year, 'RankNet-Oracle',accret[0], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "        \n",
    "    oracle_split_result[laptype] = pd.DataFrame(data=retdata, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_split_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for laptype in ['normal','pit']:\n",
    "    df = oracle_split_result[laptype]\n",
    "    df['laptype'] = laptype + '-weighted'\n",
    "    \n",
    "    dfs.append(df)\n",
    "    \n",
    "alldf = pd.concat(dfs)\n",
    "alldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = pd.read_csv(f'evaluation_result_shortterm_plen2-split-g{covergap}_v4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.append(xdf)\n",
    "df = pd.concat(dfs)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'evaluation_result_shortterm_plen2-split-g{covergap}_v4_weighted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
