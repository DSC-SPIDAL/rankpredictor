{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating the output forecastings\n",
    "\n",
    "based on: Evaluate-forecasts-paper-v5.ipynb\n",
    "\n",
    "output of forecastings format:\n",
    "\n",
    "    carno\tstartlap\tstartrank\tendrank\tdiff\tsign\tpred_endrank\tpred_diff\tpred_sign\tendlap\tpred_endlap\n",
    "      11\t12\t31\t3.0\t5.0\t2.0\t1\t1.0\t-2.0\t-1\t49\t58\n",
    "    \n",
    "refer to:\n",
    "    19.RankNet/stage_model_regressor.ipynb\n",
    "    19.RankNet/RankForecasting-stint-paper-1kpitmodel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n",
      "INFO:root:Using GPU\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import inspect\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pathlib import Path\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "#from indycar.model.stint_predictor_fastrun import *\n",
    "#import indycar.model.stint_simulator as stint\n",
    "#import indycar.model.stint_simulator_shortterm_pitmodel as stint\n",
    "#import indycar.model.stint_simulator_paper as stint\n",
    "\n",
    "import indycar.model.quicktest_simulator as stint\n",
    "\n",
    "# import all functions \n",
    "#from indycar.model.global_variables import _hi\n",
    "import indycar.model.global_variables as gvar\n",
    "from indycar.model.quicktest_modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfout(datafile, basedir = './data/'):\n",
    "    datafile = basedir + datafile  \n",
    "    #with open('laptime_rank_timediff_fulltest-oracle-%s.pickle'%year, 'rb') as f:\n",
    "    with open(datafile, 'rb') as f:\n",
    "        # The protocol version used is detected automatically, so we do not\n",
    "        # have to specify it.\n",
    "        dfout = pickle.load(f, encoding='latin1') \n",
    "        \n",
    "        return dfout[0]\n",
    "    \n",
    "def load_dfout_all(datafile):\n",
    "    #with open('laptime_rank_timediff_fulltest-oracle-%s.pickle'%year, 'rb') as f:\n",
    "    datafile = '../result/22.PaperFinal/' + datafile  \n",
    "    with open(datafile, 'rb') as f:\n",
    "        # The protocol version used is detected automatically, so we do not\n",
    "        # have to specify it.\n",
    "        dfout = pickle.load(f, encoding='latin1') \n",
    "        \n",
    "        return dfout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. evalute stint restuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load laptime and stage dataset: data//laptime_rank_timediff_pit-oracle-IndyCar_d31_v9_p0.pickle data//stagedata-IndyCar_d31_v9_p0.pickle\n"
     ]
    }
   ],
   "source": [
    "_inlap_status = 0\n",
    "_featureCnt = 9\n",
    "#\n",
    "# input data parameters\n",
    "#\n",
    "# event -> car#, maxlap\n",
    "_race_info = {}\n",
    "# the races have 7 years data \n",
    "races = ['Indy500', 'Texas','Iowa','Pocono']\n",
    "years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "\n",
    "events = []\n",
    "for race in races:\n",
    "    events.extend([f'{race}-{x}' for x in years])\n",
    "\n",
    "events.extend(['Phoenix-2018','Gateway-2018','Gateway-2019'])\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "\n",
    "# dataset shared\n",
    "dataOutputRoot = \"data/\"\n",
    "dataroot = dataOutputRoot\n",
    "covergap = 1\n",
    "dbid = f'IndyCar_d{len(events)}_v{_featureCnt}_p{_inlap_status}'\n",
    "LAPTIME_DATASET = f'{dataOutputRoot}/laptime_rank_timediff_pit-oracle-{dbid}.pickle' \n",
    "STAGE_DATASET = f'{dataOutputRoot}/stagedata-{dbid}.pickle' \n",
    "PITCOVERED_DATASET = f'{dataOutputRoot}/pitcoveredlaps-{dbid}-g{covergap}.pickle'\n",
    "PITSTOP_DATASET = f'{dataOutputRoot}/pitstop-{dbid}.csv' \n",
    "    \n",
    "print('Load laptime and stage dataset:',LAPTIME_DATASET, STAGE_DATASET)\n",
    "with open(LAPTIME_DATASET, 'rb') as f:\n",
    "    global_carids, laptime_data = pickle.load(f, encoding='latin1') \n",
    "with open(STAGE_DATASET, 'rb') as f:\n",
    "    #stagedata = pickle.load(f, encoding='latin1') \n",
    "    stagedata_o, _race_info, _events, _events_id = pickle.load(f, encoding='latin1') \n",
    "pitstop_df = pd.read_csv(PITSTOP_DATASET)\n",
    "\n",
    "with open(PITCOVERED_DATASET, 'rb') as f:\n",
    "    pitdata = pickle.load(f, encoding='latin1')\n",
    "\n",
    "#check it\n",
    "if not _events == events:\n",
    "    print('Error, events mismatch at:', STAGE_DATASET)\n",
    "    sys.exit(-1)\n",
    "\n",
    "#trainrace = 'Indy500','Iowa','Pocono','Texas'\n",
    "trainrace = 'Indy500'\n",
    "    \n",
    "#_train_events = [events_id[x] for x in [f'{trainrace}-{x}' for x in ['2013','2014','2015','2016','2017']]]\n",
    "#_test_events = [f'{trainrace}-{x}' for x in ['2018','2019']]\n",
    "if trainrace == 'Pocono':\n",
    "    # 2014, 2019 is bad\n",
    "    _train_years = ['2013','2015','2016','2017']\n",
    "    _test_events = [f'{trainrace}-2018']\n",
    "else:\n",
    "    _train_years = ['2013','2014','2015','2016','2017']\n",
    "    _test_events = [f'{trainrace}-2018',f'{trainrace}-2019']\n",
    "    \n",
    "_train_events = [events_id[x] for x in [f'{trainrace}-{x}' for x in _train_years]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvar.events = events\n",
    "gvar.events_id  = events_id\n",
    "#gvar.events_info = events_info\n",
    "gvar._race_info = _race_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "_trim = 0\n",
    "_include_final = True\n",
    "_include_stintlen = True\n",
    "#_include_stintlen = False\n",
    "include_str = '1' if _include_final else '0'\n",
    "stint_str = '1' if _include_stintlen else ''\n",
    "\n",
    "plen = 2\n",
    "_context_len = 60\n",
    "\n",
    "version = f'IndyCar-d{trainrace}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split by pit-covered-laps and normal laps\n",
    "\n",
    "define pit covered laps := two laps before and after a pit\n",
    "\n",
    "split shortterm-results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_samples(full_samples, full_tss, clearidx):\n",
    "    \"\"\"\n",
    "    clear the laps in clearidx\n",
    "    \"\"\"\n",
    "    \n",
    "    carlist = full_tss.keys()\n",
    "    \n",
    "    for carid, carno in enumerate(carlist):\n",
    "        forecast = full_samples[carno]\n",
    "        target = full_tss[carno]\n",
    "        \n",
    "        forecast[:, clearidx] = np.nan\n",
    "        target[clearidx] = np.nan\n",
    "        \n",
    "        full_samples[carno] = forecast\n",
    "        full_tss[carno] = target\n",
    "        \n",
    "    return full_samples, full_tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model,SignAcc,MAE,50-Risk,90-Risk\n",
    "# \n",
    "cols = ['Year','Model','ExpID', 'LapType', 'Top1Acc','SignAcc','MAE','50-Risk','90-Risk']\n",
    "models = {'currank':'CurRank','rf':'RandomForest','svr':'SVM','xgb':'XGBoost'}\n",
    "\n",
    "#plen=2\n",
    "#usemeanstr='median'\n",
    "usemeanstr='mean'\n",
    "#stint_split_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data//shortterm-dfout-mlmodels-IndyCar-dIndy500-end1-rerank-t2-c60.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-398629d84b8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mplen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{dataroot}/shortterm-dfout-mlmodels-{version}-end{include_str}-rerank-t{plen}-c{_context_len}.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mshortterm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dfout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasedir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#for year in ['Indy500-2018','Indy500-2019','Phoenix-2018']:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-640bd307b82e>\u001b[0m in \u001b[0;36mload_dfout\u001b[0;34m(datafile, basedir)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdatafile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasedir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatafile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#with open('laptime_rank_timediff_fulltest-oracle-%s.pickle'%year, 'rb') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# The protocol version used is detected automatically, so we do not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# have to specify it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data//shortterm-dfout-mlmodels-IndyCar-dIndy500-end1-rerank-t2-c60.pickle'"
     ]
    }
   ],
   "source": [
    "retdata = []\n",
    "   \n",
    "for plen in [2]:\n",
    "    outfile=f'{dataroot}/shortterm-dfout-mlmodels-{version}-end{include_str}-rerank-t{plen}-c{_context_len}.pickle'\n",
    "    shortterm_df = load_dfout(outfile, basedir = './')\n",
    "\n",
    "    #for year in ['Indy500-2018','Indy500-2019','Phoenix-2018']:\n",
    "    #for year in [_test_event]:\n",
    "    for year in _test_events:\n",
    "        \n",
    "        testevent = year\n",
    "        gvar.maxlap = get_event_info(testevent)[2]\n",
    "        \n",
    "        for clf in ['currank','rf','svr','xgb']:\n",
    "            print('year:',year,'clf:',clf)\n",
    "            #dfout, accret = eval_sync(preddf[year][clf],errlist[year])\n",
    "            dfout = shortterm_df[year][clf]\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "            fsamples, ftss = df2samples(dfout)\n",
    "            _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "\n",
    "            retdata.append([year,models[clf],'MLMODELS','all', accret[0], accret[4], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "            \n",
    "for laptype in ['normal','pit']:\n",
    "    \n",
    "    plen = 2\n",
    "    \n",
    "    outfile=f'{dataroot}/shortterm-dfout-mlmodels-{version}-end{include_str}-rerank-t{plen}-c{_context_len}.pickle'\n",
    "    shortterm_df = load_dfout(outfile, basedir = './')\n",
    "\n",
    "\n",
    "    #for year in ['2018','2019']:\n",
    "    #for year in [_test_event]:\n",
    "    for year in _test_events:\n",
    "        \n",
    "        \n",
    "        testevent = year\n",
    "        gvar.maxlap = get_event_info(testevent)[2]\n",
    "        \n",
    "        # select the set\n",
    "        pitcoveredlaps = pitdata[year][1]\n",
    "        normallaps = set([x for x in range(1,201)]) - pitcoveredlaps\n",
    "        \n",
    "        if laptype == 'normal':\n",
    "            sellaps = normallaps\n",
    "            clearlaps = pitcoveredlaps\n",
    "        else:\n",
    "            sellaps = pitcoveredlaps\n",
    "            clearlaps = normallaps\n",
    "        \n",
    "        \n",
    "        # pitcoveredlaps start idx = 1\n",
    "        startlaps = [x-plen-1 for x in sellaps]\n",
    "        #sellapidx = np.array([x-1 for x in sellaps])\n",
    "        clearidx = np.array([x-1 for x in clearlaps])\n",
    "        \n",
    "        for clf in ['currank','rf','svr','xgb']:\n",
    "            print('year:',year,'clf:',clf)\n",
    "            \n",
    "            dfout = shortterm_df[year][clf]\n",
    "            fsamples, ftss = df2samples(dfout)\n",
    "            \n",
    "            fsamples, ftss = clear_samples(fsamples, ftss,clearidx)\n",
    "            _, prisk_vals = prisk_direct_bysamples(fsamples, ftss)\n",
    "            \n",
    "            # split dfout by startlap\n",
    "            # curlap = int(dfrec.startlap.values[0] + prediction_len)\n",
    "            dfout = dfout[dfout['startlap'].isin(startlaps)]\n",
    "            accret = stint.get_evalret_shortterm(dfout)[0]\n",
    "\n",
    "            retdata.append([year,models[clf],'MLMODELS',laptype, accret[0], accret[4], accret[1], prisk_vals[1], prisk_vals[2]])\n",
    "            \n",
    "    #print(stint_split_result[laptype])\n",
    "retdf = pd.DataFrame(data=retdata, columns=cols) \n",
    "retdf.to_csv(f'{dataroot}/evaluation_result_shortterm_{trainrace}_plen2-split-g{covergap}_v6-c{_context_len}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
