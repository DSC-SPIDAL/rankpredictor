{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP-pitmodel\n",
    "\n",
    "test the new feature 'cur_cautionlaps'\n",
    "\n",
    "base: MLP-pitmodel-plen2\n",
    "\n",
    "build a pitstop dataset with <cautions_laps, pitage, gap2nextpit>\n",
    "gluonts interface of Dataset = Iterable[DataEntry], DataEntry = Dict(str, any)\n",
    "\n",
    "+ input pitstop dataset, remove pitstops with pit_oncaution = 1, refer to lapstatus_dataset-fastrun\n",
    "+ context_length = 1, prediction_length = 1\n",
    "+ target : gap to nextpit\n",
    "+ covariates are: cautions_laps, pitage, (carid, eid)\n",
    "+ modeling the distribution of nextpit-gap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using GPU\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import random\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import pickle\n",
    "import json\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import inspect\n",
    "from scipy import stats\n",
    "from pathlib import Path \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from pathlib import Path\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.distribution.neg_binomial import NegativeBinomialOutput\n",
    "from gluonts.distribution.student_t import StudentTOutput\n",
    "from gluonts.model.forecast import SampleForecast\n",
    "\n",
    "from indycar.model.mlp import MLPEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch_hdd/hpda/indycar/notebook/23.experiments'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def test_flag(a, bitflag):\n",
    "    return (a & bitflag) ==  bitflag\n",
    "\n",
    "#\n",
    "# remove NaN at the tail\n",
    "# there should be no nans in the middle of the ts\n",
    "COL_LAPTIME=0\n",
    "COL_RANK=1\n",
    "COL_TRACKSTATUS=2\n",
    "COL_LAPSTATUS=3\n",
    "COL_TIMEDIFF=4\n",
    "COL_CAUTION_LAPS_INSTINT=5\n",
    "COL_LAPS_INSTINT= 6\n",
    "COL_ELAPSED_TIME= 7\n",
    "COL_LAP2NEXTPIT = 8\n",
    "\n",
    "\n",
    "MODE_ORACLE = 0\n",
    "MODE_NOLAP = 1\n",
    "MODE_NOTRACK = 2\n",
    "MODE_TESTZERO = 4\n",
    "MODE_TESTCURTRACK = 8\n",
    "#MODE_STR={MODE_ORACLE:'oracle', MODE_NOLAP:'nolap',MODE_NOTRACK:'notrack',MODE_TEST:'test'}\n",
    "\n",
    "def split_ts(rec, carno, eid, include_end = False):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        ts\n",
    "    output:\n",
    "        nextpit records\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    pitstops = np.where(rec[COL_LAPSTATUS,:] == 1)[0]\n",
    "    \n",
    "    if len(pitstops)==0:\n",
    "        print('no pit ts')\n",
    "        return output\n",
    "    \n",
    "    #pit_oncaution = np.zeros_like((pitstops))\n",
    "    #for pit in pitstops:\n",
    "    #    if rec[COL_TRACKSTATUS,:] == 1:\n",
    "    #        pit_oncaution = 1\n",
    "    pit_oncaution = np.zeros_like((rec[COL_LAP2NEXTPIT,:]))\n",
    "    stint_len = np.zeros_like((rec[COL_LAP2NEXTPIT,:]))\n",
    "    pos = 0\n",
    "    for pit in pitstops:\n",
    "        if rec[COL_TRACKSTATUS,pit] == 1:\n",
    "            #next pit is oncaution\n",
    "            # set pos -> pit as oncaution\n",
    "            pit_oncaution[pos:pit] = 1\n",
    "        else:\n",
    "            pit_oncaution[pos:pit] = 0\n",
    "            \n",
    "        stint_len[pos:pit] = pit - pos\n",
    "        pos = pit\n",
    "        \n",
    "        \n",
    "    # calc cur_cautionlaps\n",
    "    # accumulate consecutive caution laps\n",
    "    trackstatus = rec[COL_TRACKSTATUS,:]\n",
    "    lapstatus = rec[COL_LAPSTATUS,:]\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    cur_cautionlaps = np.zeros_like(trackstatus)\n",
    "    cautionlaps_acc = 0\n",
    "    for idx in range(len(trackstatus)):\n",
    "        if (trackstatus[idx] == 0) or (lapstatus[idx] == 1):\n",
    "            #reset\n",
    "            cautionlaps_acc = 0\n",
    "        else:\n",
    "            cautionlaps_acc += 1\n",
    "            \n",
    "        #save state\n",
    "        cur_cautionlaps[idx] = cautionlaps_acc\n",
    "        \n",
    "            \n",
    "    #prepare output : lap2nextpit, CAUTION_LAPS_INSTINT,LAPS_INSTINT, pit_oncaution, carno, eid, lap, stintlen\n",
    "    if include_end:\n",
    "        #rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "        totallen = len(rec[COL_RANK, ~np.isnan(rec[COL_RANK,:])])\n",
    "    else:\n",
    "        totallen = pitstops[-1]\n",
    "        \n",
    "    # set the last stint target = 999, an invalid lap2nextpit\n",
    "    for idx in range(totallen):\n",
    "        output.append([ rec[COL_LAP2NEXTPIT ,idx] if idx < pitstops[-1] else 999\n",
    "                        ,rec[COL_CAUTION_LAPS_INSTINT ,idx]\n",
    "                        ,rec[COL_LAPS_INSTINT ,idx]\n",
    "                        ,pit_oncaution[idx]\n",
    "                        ,carno\n",
    "                        ,eid\n",
    "                        ,idx\n",
    "                        ,stint_len[idx]\n",
    "                        ,cur_cautionlaps[idx]\n",
    "                      ])\n",
    "        \n",
    "    return output\n",
    "\n",
    "def make_dataset_byevent(test_event = 'Indy500-2018', include_end = False):\n",
    "    \"\"\"\n",
    "    split the ts to train and test part by the ratio\n",
    "    \n",
    "    oracle_mode: false to simulate prediction in real by \n",
    "        set the covariates of track and lap status as nan in the testset\n",
    "            \n",
    "    \n",
    "    \"\"\"    \n",
    "    useeid = False\n",
    "    run_ts = COL_LAP2NEXTPIT\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    #_data: eventid, carids, datalist[carnumbers, features, lapnumber]->[laptime, rank, track, lap]]\n",
    "    for _data in laptime_data:\n",
    "        _train = []\n",
    "        _test = []\n",
    "        \n",
    "        if events[_data[0]] == test_event:\n",
    "            test_mode = True\n",
    "        elif _data[0] in _train_events:\n",
    "            test_mode = False\n",
    "        else:\n",
    "            print('skip event:', events[_data[0]])\n",
    "            continue\n",
    "            \n",
    "        # process for each ts\n",
    "        for rowid in range(_data[2].shape[0]):\n",
    "            # rec[features, lapnumber] -> [laptime, rank, track_status, lap_status,timediff]]\n",
    "            rec = _data[2][rowid].copy()\n",
    "            \n",
    "            #remove nan(only tails)\n",
    "            nans, x= nan_helper(rec[run_ts,:])\n",
    "            nan_count = np.sum(nans)             \n",
    "            rec = rec[:, ~np.isnan(rec[run_ts,:])]\n",
    "            \n",
    "            # remove short ts\n",
    "            totallen = rec.shape[1]\n",
    "            \n",
    "            carno = _data[1][rowid]\n",
    "            carid = global_carids[_data[1][rowid]]\n",
    "            \n",
    "            eid = _data[0]\n",
    "            \n",
    "            # all go to train set\n",
    "            output = split_ts(rec, carno, eid, include_end= include_end)\n",
    "            #if len(output) == 0:\n",
    "            #    continue\n",
    "            \n",
    "            test_rec_cnt = 0\n",
    "            if not test_mode:\n",
    "                _train.extend(output)\n",
    "                \n",
    "            else:\n",
    "                _test.extend(output)\n",
    "                test_rec_cnt += 1\n",
    "            \n",
    "            #add one ts\n",
    "            print(f'carno:{carno}, totallen:{totallen}, nancount:{nan_count}, test_reccnt:{test_rec_cnt}')\n",
    "\n",
    "        train_set.extend(_train)\n",
    "        test_set.extend(_test)\n",
    "\n",
    "    print(f'train len:{len(train_set)}, test len:{len(test_set)}')\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "def save_dataset(datafile,freq, prediction_length, cardinality, train_ds, test_ds):\n",
    "    with open(datafile, 'wb') as f:\n",
    "        #pack [global_carids, laptime_data]\n",
    "        savedata = [freq, prediction_length, cardinality, train_ds, test_ds]\n",
    "        #savedata = [freq, train_set, test_set]\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make gluonts\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "def makedb(data, scaler='standard', perm = True, feature_cnt=2):\n",
    "    db = []\n",
    "    start = pd.Timestamp(\"01-01-2019\", freq='1min')  # can be different for each time series\n",
    "    \n",
    "    scalers = {'minmax':MinMaxScaler(), 'standard':StandardScaler()}\n",
    "    \n",
    "    if isinstance(scaler, str):\n",
    "        if scaler in scalers:\n",
    "            scaler = scalers[scaler]\n",
    "            scaler.fit(data)\n",
    "            df = scaler.transform(data)\n",
    "        else:\n",
    "            # no scaler\n",
    "            df = data\n",
    "    else:\n",
    "        #use input scaler\n",
    "        #scaler.fit(data)\n",
    "        df = scaler.transform(data)\n",
    "        \n",
    "    \n",
    "    #permute\n",
    "    if perm:\n",
    "        perm = np.random.permutation(len(df))\n",
    "        df = df[perm]\n",
    "    \n",
    "    for x in df:\n",
    "        if feature_cnt == 3:\n",
    "            db.append({'target':np.array([x[0]]), 'feat':np.array([x[1],x[2],x[3]]),\n",
    "                  \"start\":start, 'forecast_start':start})\n",
    "        elif feature_cnt == 2:\n",
    "            db.append({'target':np.array([x[0]]), 'feat':np.array([x[1],x[2]]),\n",
    "                  \"start\":start, 'forecast_start':start})\n",
    "        else:\n",
    "            print('error _feature_cnt not support:', feature_cnt)\n",
    "            break\n",
    "        \n",
    "    return db, scaler, df\n",
    "\n",
    "\n",
    "def make_fulltestdb(scaler, maxgap=60, maxcautionlen=20, feature_cnt=2):\n",
    "    db = []\n",
    "    start = pd.Timestamp(\"01-01-2019\", freq='1min')  # can be different for each time series\n",
    "\n",
    "    data = []\n",
    "    for caution_lap in range(maxgap):\n",
    "        for pitage in range(caution_lap, maxgap):\n",
    "            if feature_cnt == 2:        \n",
    "                data.append([0.,caution_lap, pitage, 0])\n",
    "            elif feature_cnt == 3:        \n",
    "                for cur_cautionlap in range(0, caution_lap+1):\n",
    "                    data.append([0.,caution_lap, pitage, cur_cautionlap])\n",
    "    data = np.array(data)\n",
    "    \n",
    "    if not isinstance(scaler, str):\n",
    "        df = scaler.transform(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    #data\n",
    "    print(f'make full testdb: {len(df)} records')\n",
    "    for x in df:\n",
    "        if feature_cnt == 3:        \n",
    "            db.append({'target':np.array([x[0]]), 'feat':np.array([x[1],x[2],x[3]]),\n",
    "                  \"start\":start, 'forecast_start':start})\n",
    "        elif feature_cnt == 2:\n",
    "            db.append({'target':np.array([x[0]]), 'feat':np.array([x[1],x[2]]),\n",
    "                  \"start\":start, 'forecast_start':start})\n",
    "        else:\n",
    "            print('error _feature_cnt not support:', feature_cnt)\n",
    "            break\n",
    "\n",
    "    #reset data, used by PitModel.save_model()\n",
    "    if feature_cnt == 2:\n",
    "        data = data[:,:3]            \n",
    "            \n",
    "    return db, scaler, df, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, layers=[10,10,5], output = 'student', dropout = .5, id='all', feature_cnt = 2):\n",
    "    distr_outputs ={'student':StudentTOutput(),  \n",
    "                    'negbin':NegativeBinomialOutput() \n",
    "                   }\n",
    "    if not output in distr_outputs:\n",
    "        print(f'distr_output: {output} not found error.')\n",
    "        return\n",
    "    \n",
    "    distr_output = distr_outputs[output]\n",
    "    \n",
    "    modelid = f'mlp-{trainrace}-d%s-f%d-e%s-l%s-%s-d%s'%(id, feature_cnt,\n",
    "                                            epochs, '-'.join([str(x) for x in layers]), output, dropout)\n",
    "    \n",
    "    estimator = MLPEstimator(\n",
    "        num_hidden_dimensions=layers,\n",
    "        prediction_length=1,\n",
    "        context_length=1,\n",
    "        freq='1min',\n",
    "        dropout = dropout,\n",
    "        distr_output = distr_output,\n",
    "        trainer=Trainer(ctx=\"gpu(0)\", \n",
    "                        batch_size = 32,\n",
    "                        epochs= epochs,\n",
    "                        learning_rate=1e-3,\n",
    "                        hybridize=False,\n",
    "                        num_batches_per_epoch=100\n",
    "                       )\n",
    "    )    \n",
    "\n",
    "    predictor = estimator.train(train_ds)\n",
    "\n",
    "    return predictor, modelid\n",
    "\n",
    "def eval_model(predictor, test_ds):\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset    \n",
    "        predictor=predictor,  # predictor                                  \n",
    "        num_samples=1000,  # number of sample paths we want for evaluation \n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)     \n",
    "    evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9]) \n",
    "    agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_ds))\n",
    "    print(json.dumps(agg_metrics, indent=4)) \n",
    "    return tss, forecasts, agg_metrics\n",
    "\n",
    "\n",
    "def raw_eval(tss, forecasts):\n",
    "    \"\"\"\n",
    "    scaler\n",
    "    \"\"\"\n",
    "    #rec = np.zeros((_feature_cnt + 1))\n",
    "    rec = np.zeros((4))\n",
    "\n",
    "    truth, pred = [],[]\n",
    "    #go through the dataset\n",
    "    for idx in range(len(tss)):\n",
    "        rec[0] = list(tss[idx].values)[0]\n",
    "    \n",
    "        if isinstance(scaler, str):\n",
    "            truth.append(int(rec[0]))\n",
    "        else:\n",
    "            truth.append(int(scaler.inverse_transform(rec)[0]))\n",
    "    \n",
    "        rec[0] = np.mean(forecasts[idx].samples)\n",
    "        \n",
    "        if isinstance(scaler, str):\n",
    "            pred.append(int(rec[0]))    \n",
    "        else:\n",
    "            pred.append(int(scaler.inverse_transform(rec)[0]))        \n",
    "\n",
    "    #get mae\n",
    "    mae = mean_absolute_error(truth, pred)\n",
    "    print('mae = ', mae)\n",
    "    return mae\n",
    "\n",
    "def decode(tss, forecasts):\n",
    "    \"\"\"\n",
    "    scaler\n",
    "    \"\"\"\n",
    "    #rec = np.zeros((_feature_cnt + 1))\n",
    "    rec = np.zeros((4))\n",
    "\n",
    "    truth, pred = [],[]\n",
    "    sampleCnt = forecasts[0].samples.shape[0]\n",
    "    samples = np.zeros((sampleCnt, len(tss)))\n",
    "    \n",
    "    #go through the dataset\n",
    "    for idx in range(len(tss)):\n",
    "        rec[0] = list(tss[idx].values)[0]\n",
    "    \n",
    "        if isinstance(scaler, str):\n",
    "            truth.append(int(rec[0]))\n",
    "        else:\n",
    "            truth.append(int(scaler.inverse_transform(rec)[0]))\n",
    "    \n",
    "        #pred\n",
    "        rec[0] = np.mean(forecasts[idx].samples)\n",
    "        \n",
    "        if isinstance(scaler, str):\n",
    "            pred.append(int(rec[0]))    \n",
    "        else:\n",
    "            pred.append(int(scaler.inverse_transform(rec)[0]))        \n",
    "            \n",
    "        #samples\n",
    "        for sid in range(sampleCnt):\n",
    "            rec[0] = forecasts[idx].samples[sid]\n",
    "            if isinstance(scaler, str):\n",
    "                pred.append(int(rec[0]))    \n",
    "                samples[sid, idx] = int(rec[0])\n",
    "            else:\n",
    "                samples[sid, idx] = int(scaler.inverse_transform(rec)[0])        \n",
    "    \n",
    "    return truth, pred, samples\n",
    "\n",
    "def save_model(predictor, outdir):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    \n",
    "    predictor.serialize(Path(outdir)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pitmodel\n",
    "class PitModel():\n",
    "    \"\"\"\n",
    "     <caution_lap, pitage> -> [distribution]    \n",
    "     distribution := sorted cdf [val:probability, val2:p2, ...]\n",
    "         [0,:] -> val\n",
    "         [1,:] -> cdf p\n",
    "         \n",
    "     no scaler, raw feat and target\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, modelfile=''):\n",
    "        self.model = {}\n",
    "        self.name = ''\n",
    "        \n",
    "        if modelfile:\n",
    "            self.load_model(modelfile)\n",
    "                \n",
    "    def load_model(self, modelfile):\n",
    "        with open(modelfile, 'rb') as f:\n",
    "            self.name, self.model = pickle.load(f, encoding='latin1')\n",
    "            print(f'init model:{self.name}')\n",
    "        \n",
    "    def save_model(self, modelname, test_ds, forecasts, scaler):\n",
    "        \n",
    "        model = {}\n",
    "        \n",
    "        #get the sclaer for the first column(lap2nextpit)\n",
    "        sc, scf = '', ''\n",
    "        if isinstance(scaler, StandardScaler):\n",
    "            sc = StandardScaler()\n",
    "            sc.scale_ = scaler.scale_[0]\n",
    "            sc.mean_ = scaler.mean_[0]\n",
    "            sc.var_ = scaler.var_[0]\n",
    "\n",
    "            scf = StandardScaler()\n",
    "            scf.scale_ = scaler.scale_[1:]\n",
    "            scf.mean_ = scaler.mean_[1:]\n",
    "            scf.var_ = scaler.var_[1:]\n",
    "\n",
    "        \n",
    "        for idx, rec in enumerate(test_ds):\n",
    "            feat = rec[1:]\n",
    "                \n",
    "            key = '-'.join([str(int(x)) for x in feat])\n",
    "            \n",
    "            if not key in model:\n",
    "            \n",
    "                samples = forecasts[idx].samples.reshape(-1)\n",
    "                \n",
    "                if not isinstance(sc, str):\n",
    "                    samples = sc.inverse_transform(samples)\n",
    "                \n",
    "                #force to prediction to be valid lap2nextpit\n",
    "                samples = samples.astype(int)\n",
    "                samples = samples[samples > 0]\n",
    "\n",
    "                #\n",
    "                valset = set(list(samples))\n",
    "                plen = len(valset)\n",
    "                distr = np.zeros((2, plen))\n",
    "                distr[0, :] = sorted(valset)\n",
    "                smap = {val:id for id, val in enumerate(distr[0, :])}\n",
    "                for s in samples:\n",
    "                    distr[1,smap[s]] += 1\n",
    "                tsum = np.sum(distr[1,:])\n",
    "                distr[1, :] /= tsum\n",
    "                distr[1, :] = np.cumsum(distr[1, :])\n",
    "\n",
    "                model[key] = distr\n",
    "                \n",
    "        #save model\n",
    "        self.model = model\n",
    "        self.name = modelname\n",
    "        with open(modelname, 'wb') as f:\n",
    "            savedata = [self.name, self.model]\n",
    "            pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)        \n",
    "        print(f'save model {modelname} with {len(self.model)} keys.')\n",
    "                \n",
    "    def predict(self, *args):\n",
    "        key = '-'.join([str(int(x)) for x in args])\n",
    "        #if key in self.model:\n",
    "        try:\n",
    "            distr = self.model[key]\n",
    "            \n",
    "            #[0, 1.)\n",
    "            p = np.random.random()  \n",
    "            i = np.sum(distr[1,:] < p)\n",
    "            \n",
    "            return distr[0,i]\n",
    "        except:\n",
    "            #exception\n",
    "            #todo, backto special model\n",
    "            print(f'ERROR: key {key} not found in model')\n",
    "                       \n",
    "    def forecast_ds(self, test_ds, forecasts):\n",
    "        \"\"\"\n",
    "        test_ds as testset, the unsclaed input\n",
    "        forecasts ; the template\n",
    "        \"\"\"\n",
    "        \n",
    "        plen = len(test_ds)\n",
    "        sample_cnt = forecasts[0].samples.shape[0]\n",
    "        assert(plen == len(forecasts))\n",
    "        \n",
    "\n",
    "        #build a new forecasts object\n",
    "        nf = []\n",
    "        for fc in forecasts:\n",
    "            nfc = SampleForecast(samples = np.zeros_like(fc.samples), \n",
    "                                 freq=fc.freq, start_date=fc.start_date)\n",
    "            nf.append(nfc)\n",
    "        \n",
    "        for idx, rec in enumerate(test_ds):\n",
    "            feat = rec[1:]\n",
    "                    \n",
    "            onecast = np.zeros((sample_cnt))\n",
    "            for i in range(sample_cnt):\n",
    "                #onecast[i] = self.predict(feat[0], feat[1],feat[2])\n",
    "                onecast[i] = self.predict(*feat)\n",
    "        \n",
    "            nf[idx].samples = onecast\n",
    "\n",
    "        return nf\n",
    "    \n",
    "    def forecast_onecar(self, test_ds, plen = 2, sample_cnt=100):\n",
    "        \"\"\"\n",
    "        long-prediction for a single car\n",
    "        \n",
    "        input:\n",
    "            test_df ; 'lap2nextpit','caution_laps','pitage','lap'\n",
    "        \"\"\"\n",
    "        \n",
    "        #assert for one car\n",
    "        totallen = len(test_ds)\n",
    "        start = pd.Timestamp(\"01-01-2019\", freq='1min')  # can be different for each time series\n",
    "        \n",
    "        #samples\n",
    "        #onecast = np.zeros((sample_cnt, totallen+plen))\n",
    "        onecast = np.zeros((sample_cnt, totallen))\n",
    "        #onecast = np.zeros((sample_cnt, maxlap))\n",
    "        tss = np.zeros((totallen+plen))\n",
    "        \n",
    "        for idx, rec in enumerate(test_ds[:-plen]):\n",
    "        #for idx in range(maxlap-plen):\n",
    "            \n",
    "            #if idx<len(test_ds):\n",
    "            #    rec = test_ds[idx]\n",
    "            #else:\n",
    "            #    #use the last rec\n",
    "            #    # target, cautionlaps, pitage, cur_curcautionlaps\n",
    "            #    rec = [0, ] \n",
    "                \n",
    "            target = rec[0]\n",
    "            feat = rec[1:]\n",
    "            \n",
    "            if target == plen:\n",
    "                tss[idx + plen] = 1\n",
    "            \n",
    "            for i in range(sample_cnt):\n",
    "                nextpit = self.predict(*feat)\n",
    "                                                    \n",
    "                if nextpit == plen:\n",
    "                    onecast[i, idx + plen] = 1\n",
    "                \n",
    "        forecast = SampleForecast(samples = onecast, \n",
    "                             freq='1min', start_date=start)\n",
    "\n",
    "        return forecast, tss\n",
    "    \n",
    "    \n",
    "def save_full_pitmodel(mid, scaler, maxgap=60, feature_cnt = 2):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        p[mid]; predictor\n",
    "        runid ; 'all' or 'sel' of the trainning set\n",
    "    \"\"\"\n",
    "    \n",
    "    #get scaler\n",
    "    #scaler = _data[runid][-1]\n",
    "\n",
    "    # make full test set\n",
    "    test_ds, _, _, test_all = make_fulltestdb(scaler, maxgap = maxgap, feature_cnt = feature_cnt)\n",
    "\n",
    "    tss,forecasts, _ = eval_model(p[mid], test_ds)\n",
    "\n",
    "    pitmodel = PitModel()\n",
    "    \n",
    "    featurecnt_str = 'withcurcautionlaps' if (feature_cnt==3) else 'nocurcautionlaps'\n",
    "\n",
    "    #pitmodel.save_model(f'pitmodel-m{maxgap}-{mid}.pickle', test_all, forecasts, scaler)\n",
    "    pitmodel.save_model(f'{dataOutputRoot}/pitmodel-m{maxgap}-{mid}-1k-{featurecnt_str}.pickle', test_all, forecasts, scaler)\n",
    "\n",
    "    return pitmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(test_event, include_end, feature_cnt=2):\n",
    "    \n",
    "    _data = {}\n",
    "    \n",
    "    train, test =  make_dataset_byevent(test_event = test_event, include_end = include_end)\n",
    "    #prepare output : lap2nextpit, CAUTION_LAPS_INSTINT,LAPS_INSTINT, pit_oncaution, carno, eid, lap\n",
    "    df_train = pd.DataFrame(train,columns=['lap2nextpit', 'caution_laps','pitage', 'pit_oncaution', \n",
    "                     'carno','eid','lap','stint_len','cur_cautionlaps'])\n",
    "    df_test = pd.DataFrame(test,columns=['lap2nextpit', 'caution_laps','pitage', 'pit_oncaution', \n",
    "                     'carno','eid','lap','stint_len','cur_cautionlaps'])\n",
    "    \n",
    "    # select\n",
    "    train_sel = df_train[(df_train['pit_oncaution']==0) &(df_train['stint_len']>23)]\n",
    "    train_sel_2013_2017 = train_sel[train_sel['eid'].isin(_train_events)]\n",
    "    train_all_2013_2017 = df_train[df_train['eid'].isin(_train_events)]\n",
    "\n",
    "    print(len(train_all_2013_2017), len(train_sel_2013_2017))    \n",
    "\n",
    "    test_sel = df_test[(df_test['pit_oncaution']==0) &(df_test['stint_len']>23)]\n",
    "    test_all = df_test\n",
    "\n",
    "    train_sel_noshort = df_train[(df_train['stint_len']>15)]\n",
    "    train_sel_noshort_2013_2017 = train_sel_noshort[train_sel_noshort['eid'].isin(_train_events)]\n",
    "    test_sel_noshort = df_test[(df_test['stint_len']>15)]    \n",
    "    \n",
    "    \n",
    "    # selected db\n",
    "    trainset = train_sel_2013_2017[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    testset = test_sel[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    train_ds, scaler, _ = makedb(trainset, scaler='standard',feature_cnt=feature_cnt)\n",
    "    test_ds, _, _ = makedb(testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "    _data['sel'] = [trainset, testset, train_ds, test_ds, scaler]\n",
    "\n",
    "\n",
    "    # selected db\n",
    "    trainset = train_all_2013_2017[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    testset = test_all[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    train_ds, scaler, _ = makedb(trainset, scaler='standard',feature_cnt=feature_cnt)\n",
    "    test_ds, _, _ = makedb(testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "    _data['all'] = [trainset, testset, train_ds, test_ds, scaler]\n",
    "\n",
    "\n",
    "    # selected db\n",
    "    trainset = train_sel_noshort_2013_2017[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    testset = test_sel_noshort[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    train_ds, scaler, _ = makedb(trainset, scaler='standard',feature_cnt=feature_cnt)\n",
    "    test_ds, _, _ = makedb(testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "    _data['noshort'] = [trainset, testset, train_ds, test_ds, scaler]    \n",
    "\n",
    "    # add normal (normal pit only, with short pits)\n",
    "    train_sel_normal = df_train[(df_train['pit_oncaution']==0)]\n",
    "    train_sel_normal_2013_2017 = train_sel_normal[train_sel_normal['eid'].isin(_train_events)]\n",
    "    test_sel_normal = df_test[(df_test['pit_oncaution']==0)]\n",
    "\n",
    "    trainset = train_sel_normal_2013_2017[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    testset = test_sel_normal[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "\n",
    "    train_ds, scaler, _ = makedb(trainset, scaler='standard',feature_cnt=feature_cnt)\n",
    "    test_ds, _, _ = makedb(testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "    _data['normal'] = [trainset, testset, train_ds, test_ds, scaler]\n",
    "    \n",
    "    # add caution pits\n",
    "    train_sel_caution = df_train[(df_train['pit_oncaution']==1)]\n",
    "    train_sel_caution_2013_2017 = train_sel_caution[train_sel_caution['eid'].isin(_train_events)]\n",
    "    test_sel_caution = df_test[(df_test['pit_oncaution']==1)]\n",
    "\n",
    "    trainset = train_sel_caution_2013_2017[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    testset = test_sel_caution[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "\n",
    "    train_ds, scaler, _ = makedb(trainset, scaler='standard',feature_cnt=feature_cnt)\n",
    "    test_ds, _, _ = makedb(testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "    _data['caution'] = [trainset, testset, train_ds, test_ds, scaler]\n",
    "    \n",
    "    # add plen=2\n",
    "    train_sel_plen2 = df_train[(df_train['lap2nextpit']==2)]\n",
    "    train_sel_plen2_2013_2017 = train_sel_caution[train_sel_caution['eid'].isin(_train_events)]\n",
    "    test_sel_plen2 = df_test[(df_test['lap2nextpit']==2)]\n",
    "\n",
    "    trainset = train_sel_plen2_2013_2017[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "    testset = test_sel_plen2[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "\n",
    "    train_ds, scaler, _ = makedb(trainset, scaler='standard',feature_cnt=feature_cnt)\n",
    "    test_ds, _, _ = makedb(testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "    _data['plen2'] = [trainset, testset, train_ds, test_ds, scaler]\n",
    "        \n",
    "    \n",
    "    return df_train, df_test, _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#straight implementation of prisk\n",
    "def quantile_loss(target, quantile_forecast, q):\n",
    "    return 2.0 * np.nansum(\n",
    "        np.abs(\n",
    "            (quantile_forecast - target)\n",
    "            * ((target <= quantile_forecast) - q)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def abs_target_sum(target): \n",
    "    return np.nansum(np.abs(target)) \n",
    "\n",
    "def prisk_direct_bysamples(forecast, target, quantiles=[0.1,0.5,0.9], startid = 0, verbose=False):\n",
    "    \"\"\"\n",
    "    calculate prisk by <samples, tss> directly (equal to gluonts implementation)\n",
    "    \n",
    "    target: endrank\n",
    "    forecast: pred_endrank\n",
    "    item_id: <carno, startlap>\n",
    "    \"\"\"\n",
    "    \n",
    "    prisk = np.zeros((len(quantiles)))\n",
    "    target_sum = 0\n",
    "    aggrisk = np.zeros((len(quantiles)))\n",
    "    \n",
    "    #calc quantiles\n",
    "    # len(quantiles) x 1\n",
    "    quantile_forecasts = np.quantile(forecast, quantiles, axis=0)\n",
    "\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        q_forecast = quantile_forecasts[idx]\n",
    "        prisk[idx] = quantile_loss(target[startid:], q_forecast[startid:], q)\n",
    "        target_sum = abs_target_sum(target[startid:])\n",
    "\n",
    "    if verbose==True and carno==3:\n",
    "        print('target:', target[startid:])\n",
    "        print('forecast:', q_forecast[startid:])\n",
    "        print('target_sum:', target_sum)\n",
    "\n",
    "        print('quantile_forecasts:', quantile_forecasts[:,startid:])\n",
    "        \n",
    "    #agg\n",
    "    #aggrisk = np.mean(prisk, axis=0)\n",
    "    #prisk_sum = np.nansum(prisk, axis=0)\n",
    "    prisk_sum = prisk\n",
    "    \n",
    "    if verbose==True:\n",
    "        print('prisk:',prisk)\n",
    "        print('prisk_sum:',prisk_sum)\n",
    "        print('target_sum:',target_sum)\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        aggrisk[idx] = np.divide(prisk_sum[idx], target_sum)\n",
    "    \n",
    "    agg_metrics = {}\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        agg_metrics[f'wQuantileLoss[{q}]'] = aggrisk[idx]\n",
    "        \n",
    "    print(agg_metrics.values())\n",
    "    \n",
    "    return agg_metrics, aggrisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stage and laptime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load laptime and stage dataset: data//laptime_rank_timediff_pit-oracle-IndyCar_d31_v9_p0.pickle data//stagedata-IndyCar_d31_v9_p0.pickle\n"
     ]
    }
   ],
   "source": [
    "_inlap_status = 0\n",
    "_featureCnt = 9\n",
    "#\n",
    "# input data parameters\n",
    "#\n",
    "# event -> car#, maxlap\n",
    "_race_info = {}\n",
    "# the races have 7 years data \n",
    "races = ['Indy500', 'Texas','Iowa','Pocono']\n",
    "years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "\n",
    "events = []\n",
    "for race in races:\n",
    "    events.extend([f'{race}-{x}' for x in years])\n",
    "\n",
    "events.extend(['Phoenix-2018','Gateway-2018','Gateway-2019'])\n",
    "events_id={key:idx for idx, key in enumerate(events)}\n",
    "\n",
    "# dataset shared\n",
    "dataOutputRoot = \"data/\"\n",
    "covergap = 1\n",
    "dbid = f'IndyCar_d{len(events)}_v{_featureCnt}_p{_inlap_status}'\n",
    "LAPTIME_DATASET = f'{dataOutputRoot}/laptime_rank_timediff_pit-oracle-{dbid}.pickle' \n",
    "STAGE_DATASET = f'{dataOutputRoot}/stagedata-{dbid}.pickle' \n",
    "PITCOVERED_DATASET = f'{dataOutputRoot}/pitcoveredlaps-{dbid}-g{covergap}.pickle'\n",
    "    \n",
    "print('Load laptime and stage dataset:',LAPTIME_DATASET, STAGE_DATASET)\n",
    "with open(LAPTIME_DATASET, 'rb') as f:\n",
    "    global_carids, laptime_data = pickle.load(f, encoding='latin1') \n",
    "with open(STAGE_DATASET, 'rb') as f:\n",
    "    #stagedata = pickle.load(f, encoding='latin1') \n",
    "    stagedata, _race_info, _events, _events_id = pickle.load(f, encoding='latin1') \n",
    "\n",
    "#check it\n",
    "if not _events == events:\n",
    "    print('Error, events mismatch at:', STAGE_DATASET)\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_end = True\n",
    "includeend_str = '-includeend' if include_end else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model store\n",
    "dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip event: Indy500-2013\n",
      "skip event: Indy500-2014\n",
      "skip event: Indy500-2015\n",
      "skip event: Indy500-2016\n",
      "skip event: Indy500-2017\n",
      "skip event: Indy500-2018\n",
      "skip event: Indy500-2019\n",
      "carno:1, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:4, totallen:171, nancount:57, test_reccnt:0\n",
      "carno:5, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:6, totallen:187, nancount:41, test_reccnt:0\n",
      "carno:7, totallen:208, nancount:20, test_reccnt:0\n",
      "carno:9, totallen:60, nancount:168, test_reccnt:0\n",
      "carno:10, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:11, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:12, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:14, totallen:206, nancount:22, test_reccnt:0\n",
      "carno:15, totallen:184, nancount:44, test_reccnt:0\n",
      "carno:16, totallen:211, nancount:17, test_reccnt:0\n",
      "no pit ts\n",
      "carno:18, totallen:0, nancount:228, test_reccnt:0\n",
      "carno:19, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:20, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:25, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:27, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:55, totallen:173, nancount:55, test_reccnt:0\n",
      "carno:67, totallen:197, nancount:31, test_reccnt:0\n",
      "carno:77, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:78, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:83, totallen:198, nancount:30, test_reccnt:0\n",
      "carno:98, totallen:196, nancount:32, test_reccnt:0\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:243, nancount:5, test_reccnt:0\n",
      "carno:7, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:8, totallen:207, nancount:41, test_reccnt:0\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:90, nancount:158, test_reccnt:0\n",
      "carno:12, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:14, totallen:209, nancount:39, test_reccnt:0\n",
      "carno:15, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:17, totallen:239, nancount:9, test_reccnt:0\n",
      "carno:18, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:19, totallen:101, nancount:147, test_reccnt:0\n",
      "carno:20, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:25, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:27, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:28, totallen:135, nancount:113, test_reccnt:0\n",
      "carno:34, totallen:241, nancount:7, test_reccnt:0\n",
      "carno:67, totallen:210, nancount:38, test_reccnt:0\n",
      "carno:77, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:83, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:98, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:1, totallen:215, nancount:33, test_reccnt:0\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:4, totallen:215, nancount:33, test_reccnt:0\n",
      "carno:5, totallen:226, nancount:22, test_reccnt:0\n",
      "carno:7, totallen:193, nancount:55, test_reccnt:0\n",
      "carno:8, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:208, nancount:40, test_reccnt:0\n",
      "carno:14, totallen:217, nancount:31, test_reccnt:0\n",
      "carno:15, totallen:205, nancount:43, test_reccnt:0\n",
      "carno:18, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:19, totallen:155, nancount:93, test_reccnt:0\n",
      "carno:20, totallen:146, nancount:102, test_reccnt:0\n",
      "carno:22, totallen:192, nancount:56, test_reccnt:0\n",
      "carno:26, totallen:191, nancount:57, test_reccnt:0\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:28, totallen:214, nancount:34, test_reccnt:0\n",
      "carno:41, totallen:61, nancount:187, test_reccnt:0\n",
      "carno:67, totallen:148, nancount:100, test_reccnt:0\n",
      "carno:83, totallen:229, nancount:19, test_reccnt:0\n",
      "carno:98, totallen:204, nancount:44, test_reccnt:0\n",
      "carno:2, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:3, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:5, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:7, totallen:226, nancount:22, test_reccnt:0\n",
      "carno:8, totallen:232, nancount:16, test_reccnt:0\n",
      "carno:9, totallen:199, nancount:49, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:12, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:14, totallen:159, nancount:89, test_reccnt:0\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:18, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:19, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:20, totallen:203, nancount:45, test_reccnt:0\n",
      "no pit ts\n",
      "carno:21, totallen:39, nancount:209, test_reccnt:0\n",
      "carno:22, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:26, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:27, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:28, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:41, totallen:222, nancount:26, test_reccnt:0\n",
      "carno:83, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:98, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:1, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:2, totallen:191, nancount:57, test_reccnt:0\n",
      "carno:3, totallen:44, nancount:204, test_reccnt:0\n",
      "carno:4, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:5, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:7, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:8, totallen:244, nancount:4, test_reccnt:0\n",
      "carno:9, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:12, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:14, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:18, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:19, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:20, totallen:223, nancount:25, test_reccnt:0\n",
      "carno:21, totallen:192, nancount:56, test_reccnt:0\n",
      "carno:26, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:28, totallen:139, nancount:109, test_reccnt:0\n",
      "no pit ts\n",
      "carno:83, totallen:40, nancount:208, test_reccnt:0\n",
      "carno:88, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:98, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:1, totallen:230, nancount:18, test_reccnt:1\n",
      "no pit ts\n",
      "carno:4, totallen:0, nancount:248, test_reccnt:1\n",
      "carno:5, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:6, totallen:125, nancount:123, test_reccnt:1\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:12, totallen:181, nancount:67, test_reccnt:1\n",
      "carno:14, totallen:30, nancount:218, test_reccnt:1\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:18, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:19, totallen:204, nancount:44, test_reccnt:1\n",
      "carno:20, totallen:157, nancount:91, test_reccnt:1\n",
      "carno:21, totallen:211, nancount:37, test_reccnt:1\n",
      "carno:22, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:23, totallen:211, nancount:37, test_reccnt:1\n",
      "carno:26, totallen:202, nancount:46, test_reccnt:1\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:28, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:30, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:59, totallen:211, nancount:37, test_reccnt:1\n",
      "carno:88, totallen:205, nancount:43, test_reccnt:1\n",
      "carno:98, totallen:209, nancount:39, test_reccnt:1\n",
      "skip event: Texas-2019\n",
      "skip event: Iowa-2013\n",
      "skip event: Iowa-2014\n",
      "skip event: Iowa-2015\n",
      "skip event: Iowa-2016\n",
      "skip event: Iowa-2017\n",
      "skip event: Iowa-2018\n",
      "skip event: Iowa-2019\n",
      "skip event: Pocono-2013\n",
      "skip event: Pocono-2014\n",
      "skip event: Pocono-2015\n",
      "skip event: Pocono-2016\n",
      "skip event: Pocono-2017\n",
      "skip event: Pocono-2018\n",
      "skip event: Pocono-2019\n",
      "skip event: Phoenix-2018\n",
      "skip event: Gateway-2018\n",
      "skip event: Gateway-2019\n",
      "train len:22156, test len:4408\n",
      "22156 11489\n",
      "skip event: Indy500-2013\n",
      "skip event: Indy500-2014\n",
      "skip event: Indy500-2015\n",
      "skip event: Indy500-2016\n",
      "skip event: Indy500-2017\n",
      "skip event: Indy500-2018\n",
      "skip event: Indy500-2019\n",
      "carno:1, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:4, totallen:171, nancount:57, test_reccnt:0\n",
      "carno:5, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:6, totallen:187, nancount:41, test_reccnt:0\n",
      "carno:7, totallen:208, nancount:20, test_reccnt:0\n",
      "carno:9, totallen:60, nancount:168, test_reccnt:0\n",
      "carno:10, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:11, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:12, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:14, totallen:206, nancount:22, test_reccnt:0\n",
      "carno:15, totallen:184, nancount:44, test_reccnt:0\n",
      "carno:16, totallen:211, nancount:17, test_reccnt:0\n",
      "no pit ts\n",
      "carno:18, totallen:0, nancount:228, test_reccnt:0\n",
      "carno:19, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:20, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:25, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:27, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:55, totallen:173, nancount:55, test_reccnt:0\n",
      "carno:67, totallen:197, nancount:31, test_reccnt:0\n",
      "carno:77, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:78, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:83, totallen:198, nancount:30, test_reccnt:0\n",
      "carno:98, totallen:196, nancount:32, test_reccnt:0\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:243, nancount:5, test_reccnt:0\n",
      "carno:7, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:8, totallen:207, nancount:41, test_reccnt:0\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:90, nancount:158, test_reccnt:0\n",
      "carno:12, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:14, totallen:209, nancount:39, test_reccnt:0\n",
      "carno:15, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:17, totallen:239, nancount:9, test_reccnt:0\n",
      "carno:18, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:19, totallen:101, nancount:147, test_reccnt:0\n",
      "carno:20, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:25, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:27, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:28, totallen:135, nancount:113, test_reccnt:0\n",
      "carno:34, totallen:241, nancount:7, test_reccnt:0\n",
      "carno:67, totallen:210, nancount:38, test_reccnt:0\n",
      "carno:77, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:83, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:98, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:1, totallen:215, nancount:33, test_reccnt:0\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:4, totallen:215, nancount:33, test_reccnt:0\n",
      "carno:5, totallen:226, nancount:22, test_reccnt:0\n",
      "carno:7, totallen:193, nancount:55, test_reccnt:0\n",
      "carno:8, totallen:235, nancount:13, test_reccnt:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carno:9, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:208, nancount:40, test_reccnt:0\n",
      "carno:14, totallen:217, nancount:31, test_reccnt:0\n",
      "carno:15, totallen:205, nancount:43, test_reccnt:0\n",
      "carno:18, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:19, totallen:155, nancount:93, test_reccnt:0\n",
      "carno:20, totallen:146, nancount:102, test_reccnt:0\n",
      "carno:22, totallen:192, nancount:56, test_reccnt:0\n",
      "carno:26, totallen:191, nancount:57, test_reccnt:0\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:28, totallen:214, nancount:34, test_reccnt:0\n",
      "carno:41, totallen:61, nancount:187, test_reccnt:0\n",
      "carno:67, totallen:148, nancount:100, test_reccnt:0\n",
      "carno:83, totallen:229, nancount:19, test_reccnt:0\n",
      "carno:98, totallen:204, nancount:44, test_reccnt:0\n",
      "carno:2, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:3, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:5, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:7, totallen:226, nancount:22, test_reccnt:0\n",
      "carno:8, totallen:232, nancount:16, test_reccnt:0\n",
      "carno:9, totallen:199, nancount:49, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:12, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:14, totallen:159, nancount:89, test_reccnt:0\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:18, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:19, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:20, totallen:203, nancount:45, test_reccnt:0\n",
      "no pit ts\n",
      "carno:21, totallen:39, nancount:209, test_reccnt:0\n",
      "carno:22, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:26, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:27, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:28, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:41, totallen:222, nancount:26, test_reccnt:0\n",
      "carno:83, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:98, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:1, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:2, totallen:191, nancount:57, test_reccnt:0\n",
      "carno:3, totallen:44, nancount:204, test_reccnt:0\n",
      "carno:4, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:5, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:7, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:8, totallen:244, nancount:4, test_reccnt:0\n",
      "carno:9, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:12, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:14, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:18, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:19, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:20, totallen:223, nancount:25, test_reccnt:0\n",
      "carno:21, totallen:192, nancount:56, test_reccnt:0\n",
      "carno:26, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:28, totallen:139, nancount:109, test_reccnt:0\n",
      "no pit ts\n",
      "carno:83, totallen:40, nancount:208, test_reccnt:0\n",
      "carno:88, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:98, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:1, totallen:230, nancount:18, test_reccnt:1\n",
      "no pit ts\n",
      "carno:4, totallen:0, nancount:248, test_reccnt:1\n",
      "carno:5, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:6, totallen:125, nancount:123, test_reccnt:1\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:12, totallen:181, nancount:67, test_reccnt:1\n",
      "carno:14, totallen:30, nancount:218, test_reccnt:1\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:18, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:19, totallen:204, nancount:44, test_reccnt:1\n",
      "carno:20, totallen:157, nancount:91, test_reccnt:1\n",
      "carno:21, totallen:211, nancount:37, test_reccnt:1\n",
      "carno:22, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:23, totallen:211, nancount:37, test_reccnt:1\n",
      "carno:26, totallen:202, nancount:46, test_reccnt:1\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:28, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:30, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:59, totallen:211, nancount:37, test_reccnt:1\n",
      "carno:88, totallen:205, nancount:43, test_reccnt:1\n",
      "carno:98, totallen:209, nancount:39, test_reccnt:1\n",
      "skip event: Texas-2019\n",
      "skip event: Iowa-2013\n",
      "skip event: Iowa-2014\n",
      "skip event: Iowa-2015\n",
      "skip event: Iowa-2016\n",
      "skip event: Iowa-2017\n",
      "skip event: Iowa-2018\n",
      "skip event: Iowa-2019\n",
      "skip event: Pocono-2013\n",
      "skip event: Pocono-2014\n",
      "skip event: Pocono-2015\n",
      "skip event: Pocono-2016\n",
      "skip event: Pocono-2017\n",
      "skip event: Pocono-2018\n",
      "skip event: Pocono-2019\n",
      "skip event: Phoenix-2018\n",
      "skip event: Gateway-2018\n",
      "skip event: Gateway-2019\n",
      "train len:22156, test len:4408\n",
      "22156 11489\n",
      "skip event: Indy500-2013\n",
      "skip event: Indy500-2014\n",
      "skip event: Indy500-2015\n",
      "skip event: Indy500-2016\n",
      "skip event: Indy500-2017\n",
      "skip event: Indy500-2018\n",
      "skip event: Indy500-2019\n",
      "carno:1, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:4, totallen:171, nancount:57, test_reccnt:0\n",
      "carno:5, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:6, totallen:187, nancount:41, test_reccnt:0\n",
      "carno:7, totallen:208, nancount:20, test_reccnt:0\n",
      "carno:9, totallen:60, nancount:168, test_reccnt:0\n",
      "carno:10, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:11, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:12, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:14, totallen:206, nancount:22, test_reccnt:0\n",
      "carno:15, totallen:184, nancount:44, test_reccnt:0\n",
      "carno:16, totallen:211, nancount:17, test_reccnt:0\n",
      "no pit ts\n",
      "carno:18, totallen:0, nancount:228, test_reccnt:0\n",
      "carno:19, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:20, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:25, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:27, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:55, totallen:173, nancount:55, test_reccnt:0\n",
      "carno:67, totallen:197, nancount:31, test_reccnt:0\n",
      "carno:77, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:78, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:83, totallen:198, nancount:30, test_reccnt:0\n",
      "carno:98, totallen:196, nancount:32, test_reccnt:0\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:243, nancount:5, test_reccnt:0\n",
      "carno:7, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:8, totallen:207, nancount:41, test_reccnt:0\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:90, nancount:158, test_reccnt:0\n",
      "carno:12, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:14, totallen:209, nancount:39, test_reccnt:0\n",
      "carno:15, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:17, totallen:239, nancount:9, test_reccnt:0\n",
      "carno:18, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:19, totallen:101, nancount:147, test_reccnt:0\n",
      "carno:20, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:25, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:27, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:28, totallen:135, nancount:113, test_reccnt:0\n",
      "carno:34, totallen:241, nancount:7, test_reccnt:0\n",
      "carno:67, totallen:210, nancount:38, test_reccnt:0\n",
      "carno:77, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:83, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:98, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:1, totallen:215, nancount:33, test_reccnt:0\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:4, totallen:215, nancount:33, test_reccnt:0\n",
      "carno:5, totallen:226, nancount:22, test_reccnt:0\n",
      "carno:7, totallen:193, nancount:55, test_reccnt:0\n",
      "carno:8, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:208, nancount:40, test_reccnt:0\n",
      "carno:14, totallen:217, nancount:31, test_reccnt:0\n",
      "carno:15, totallen:205, nancount:43, test_reccnt:0\n",
      "carno:18, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:19, totallen:155, nancount:93, test_reccnt:0\n",
      "carno:20, totallen:146, nancount:102, test_reccnt:0\n",
      "carno:22, totallen:192, nancount:56, test_reccnt:0\n",
      "carno:26, totallen:191, nancount:57, test_reccnt:0\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:28, totallen:214, nancount:34, test_reccnt:0\n",
      "carno:41, totallen:61, nancount:187, test_reccnt:0\n",
      "carno:67, totallen:148, nancount:100, test_reccnt:0\n",
      "carno:83, totallen:229, nancount:19, test_reccnt:0\n",
      "carno:98, totallen:204, nancount:44, test_reccnt:0\n",
      "carno:2, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:3, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:5, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:7, totallen:226, nancount:22, test_reccnt:0\n",
      "carno:8, totallen:232, nancount:16, test_reccnt:0\n",
      "carno:9, totallen:199, nancount:49, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:12, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:14, totallen:159, nancount:89, test_reccnt:0\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:18, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:19, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:20, totallen:203, nancount:45, test_reccnt:0\n",
      "no pit ts\n",
      "carno:21, totallen:39, nancount:209, test_reccnt:0\n",
      "carno:22, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:26, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:27, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:28, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:41, totallen:222, nancount:26, test_reccnt:0\n",
      "carno:83, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:98, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:1, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:2, totallen:191, nancount:57, test_reccnt:0\n",
      "carno:3, totallen:44, nancount:204, test_reccnt:0\n",
      "carno:4, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:5, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:7, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:8, totallen:244, nancount:4, test_reccnt:0\n",
      "carno:9, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:12, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:14, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:18, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:19, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:20, totallen:223, nancount:25, test_reccnt:0\n",
      "carno:21, totallen:192, nancount:56, test_reccnt:0\n",
      "carno:26, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:28, totallen:139, nancount:109, test_reccnt:0\n",
      "no pit ts\n",
      "carno:83, totallen:40, nancount:208, test_reccnt:0\n",
      "carno:88, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:98, totallen:0, nancount:248, test_reccnt:0\n",
      "skip event: Texas-2018\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:4, totallen:72, nancount:176, test_reccnt:1\n",
      "carno:5, totallen:188, nancount:60, test_reccnt:1\n",
      "carno:7, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:9, totallen:189, nancount:59, test_reccnt:1\n",
      "carno:10, totallen:229, nancount:19, test_reccnt:1\n",
      "carno:12, totallen:221, nancount:27, test_reccnt:1\n",
      "carno:14, totallen:219, nancount:29, test_reccnt:1\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:18, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:19, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:20, totallen:220, nancount:28, test_reccnt:1\n",
      "carno:21, totallen:220, nancount:28, test_reccnt:1\n",
      "carno:22, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:23, totallen:85, nancount:163, test_reccnt:1\n",
      "carno:26, totallen:171, nancount:77, test_reccnt:1\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:28, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:30, totallen:201, nancount:47, test_reccnt:1\n",
      "carno:59, totallen:221, nancount:27, test_reccnt:1\n",
      "carno:88, totallen:187, nancount:61, test_reccnt:1\n",
      "carno:98, totallen:195, nancount:53, test_reccnt:1\n",
      "skip event: Iowa-2013\n",
      "skip event: Iowa-2014\n",
      "skip event: Iowa-2015\n",
      "skip event: Iowa-2016\n",
      "skip event: Iowa-2017\n",
      "skip event: Iowa-2018\n",
      "skip event: Iowa-2019\n",
      "skip event: Pocono-2013\n",
      "skip event: Pocono-2014\n",
      "skip event: Pocono-2015\n",
      "skip event: Pocono-2016\n",
      "skip event: Pocono-2017\n",
      "skip event: Pocono-2018\n",
      "skip event: Pocono-2019\n",
      "skip event: Phoenix-2018\n",
      "skip event: Gateway-2018\n",
      "skip event: Gateway-2019\n",
      "train len:22156, test len:4602\n",
      "22156 11489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip event: Indy500-2013\n",
      "skip event: Indy500-2014\n",
      "skip event: Indy500-2015\n",
      "skip event: Indy500-2016\n",
      "skip event: Indy500-2017\n",
      "skip event: Indy500-2018\n",
      "skip event: Indy500-2019\n",
      "carno:1, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:4, totallen:171, nancount:57, test_reccnt:0\n",
      "carno:5, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:6, totallen:187, nancount:41, test_reccnt:0\n",
      "carno:7, totallen:208, nancount:20, test_reccnt:0\n",
      "carno:9, totallen:60, nancount:168, test_reccnt:0\n",
      "carno:10, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:11, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:12, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:14, totallen:206, nancount:22, test_reccnt:0\n",
      "carno:15, totallen:184, nancount:44, test_reccnt:0\n",
      "carno:16, totallen:211, nancount:17, test_reccnt:0\n",
      "no pit ts\n",
      "carno:18, totallen:0, nancount:228, test_reccnt:0\n",
      "carno:19, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:20, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:25, totallen:228, nancount:0, test_reccnt:0\n",
      "carno:27, totallen:174, nancount:54, test_reccnt:0\n",
      "carno:55, totallen:173, nancount:55, test_reccnt:0\n",
      "carno:67, totallen:197, nancount:31, test_reccnt:0\n",
      "carno:77, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:78, totallen:172, nancount:56, test_reccnt:0\n",
      "carno:83, totallen:198, nancount:30, test_reccnt:0\n",
      "carno:98, totallen:196, nancount:32, test_reccnt:0\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:243, nancount:5, test_reccnt:0\n",
      "carno:7, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:8, totallen:207, nancount:41, test_reccnt:0\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:90, nancount:158, test_reccnt:0\n",
      "carno:12, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:14, totallen:209, nancount:39, test_reccnt:0\n",
      "carno:15, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:17, totallen:239, nancount:9, test_reccnt:0\n",
      "carno:18, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:19, totallen:101, nancount:147, test_reccnt:0\n",
      "carno:20, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:25, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:27, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:28, totallen:135, nancount:113, test_reccnt:0\n",
      "carno:34, totallen:241, nancount:7, test_reccnt:0\n",
      "carno:67, totallen:210, nancount:38, test_reccnt:0\n",
      "carno:77, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:83, totallen:242, nancount:6, test_reccnt:0\n",
      "carno:98, totallen:240, nancount:8, test_reccnt:0\n",
      "carno:1, totallen:215, nancount:33, test_reccnt:0\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:3, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:4, totallen:215, nancount:33, test_reccnt:0\n",
      "carno:5, totallen:226, nancount:22, test_reccnt:0\n",
      "carno:7, totallen:193, nancount:55, test_reccnt:0\n",
      "carno:8, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:9, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:208, nancount:40, test_reccnt:0\n",
      "carno:14, totallen:217, nancount:31, test_reccnt:0\n",
      "carno:15, totallen:205, nancount:43, test_reccnt:0\n",
      "carno:18, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:19, totallen:155, nancount:93, test_reccnt:0\n",
      "carno:20, totallen:146, nancount:102, test_reccnt:0\n",
      "carno:22, totallen:192, nancount:56, test_reccnt:0\n",
      "carno:26, totallen:191, nancount:57, test_reccnt:0\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:28, totallen:214, nancount:34, test_reccnt:0\n",
      "carno:41, totallen:61, nancount:187, test_reccnt:0\n",
      "carno:67, totallen:148, nancount:100, test_reccnt:0\n",
      "carno:83, totallen:229, nancount:19, test_reccnt:0\n",
      "carno:98, totallen:204, nancount:44, test_reccnt:0\n",
      "carno:2, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:3, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:5, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:7, totallen:226, nancount:22, test_reccnt:0\n",
      "carno:8, totallen:232, nancount:16, test_reccnt:0\n",
      "carno:9, totallen:199, nancount:49, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:11, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:12, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:14, totallen:159, nancount:89, test_reccnt:0\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:18, totallen:0, nancount:248, test_reccnt:0\n",
      "carno:19, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:20, totallen:203, nancount:45, test_reccnt:0\n",
      "no pit ts\n",
      "carno:21, totallen:39, nancount:209, test_reccnt:0\n",
      "carno:22, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:26, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:27, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:28, totallen:234, nancount:14, test_reccnt:0\n",
      "carno:41, totallen:222, nancount:26, test_reccnt:0\n",
      "carno:83, totallen:236, nancount:12, test_reccnt:0\n",
      "carno:98, totallen:235, nancount:13, test_reccnt:0\n",
      "carno:1, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:2, totallen:191, nancount:57, test_reccnt:0\n",
      "carno:3, totallen:44, nancount:204, test_reccnt:0\n",
      "carno:4, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:5, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:7, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:8, totallen:244, nancount:4, test_reccnt:0\n",
      "carno:9, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:10, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:12, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:14, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:18, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:19, totallen:139, nancount:109, test_reccnt:0\n",
      "carno:20, totallen:223, nancount:25, test_reccnt:0\n",
      "carno:21, totallen:192, nancount:56, test_reccnt:0\n",
      "carno:26, totallen:225, nancount:23, test_reccnt:0\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:0\n",
      "carno:28, totallen:139, nancount:109, test_reccnt:0\n",
      "no pit ts\n",
      "carno:83, totallen:40, nancount:208, test_reccnt:0\n",
      "carno:88, totallen:248, nancount:0, test_reccnt:0\n",
      "no pit ts\n",
      "carno:98, totallen:0, nancount:248, test_reccnt:0\n",
      "skip event: Texas-2018\n",
      "carno:2, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:4, totallen:72, nancount:176, test_reccnt:1\n",
      "carno:5, totallen:188, nancount:60, test_reccnt:1\n",
      "carno:7, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:9, totallen:189, nancount:59, test_reccnt:1\n",
      "carno:10, totallen:229, nancount:19, test_reccnt:1\n",
      "carno:12, totallen:221, nancount:27, test_reccnt:1\n",
      "carno:14, totallen:219, nancount:29, test_reccnt:1\n",
      "carno:15, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:18, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:19, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:20, totallen:220, nancount:28, test_reccnt:1\n",
      "carno:21, totallen:220, nancount:28, test_reccnt:1\n",
      "carno:22, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:23, totallen:85, nancount:163, test_reccnt:1\n",
      "carno:26, totallen:171, nancount:77, test_reccnt:1\n",
      "carno:27, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:28, totallen:248, nancount:0, test_reccnt:1\n",
      "carno:30, totallen:201, nancount:47, test_reccnt:1\n",
      "carno:59, totallen:221, nancount:27, test_reccnt:1\n",
      "carno:88, totallen:187, nancount:61, test_reccnt:1\n",
      "carno:98, totallen:195, nancount:53, test_reccnt:1\n",
      "skip event: Iowa-2013\n",
      "skip event: Iowa-2014\n",
      "skip event: Iowa-2015\n",
      "skip event: Iowa-2016\n",
      "skip event: Iowa-2017\n",
      "skip event: Iowa-2018\n",
      "skip event: Iowa-2019\n",
      "skip event: Pocono-2013\n",
      "skip event: Pocono-2014\n",
      "skip event: Pocono-2015\n",
      "skip event: Pocono-2016\n",
      "skip event: Pocono-2017\n",
      "skip event: Pocono-2018\n",
      "skip event: Pocono-2019\n",
      "skip event: Phoenix-2018\n",
      "skip event: Gateway-2018\n",
      "skip event: Gateway-2019\n",
      "train len:22156, test len:4602\n",
      "22156 11489\n"
     ]
    }
   ],
   "source": [
    "#trainrace = 'Indy500','Iowa','Pocono','Texas'\n",
    "trainrace = 'Texas'\n",
    "\n",
    "_train_years = ['2013','2014','2015','2016','2017']\n",
    "_train_events = [events_id[x] for x in [f'{trainrace}-{x}' for x in _train_years]]\n",
    "testevents = [f'{trainrace}-2018',f'{trainrace}-2019']\n",
    "dbid = f'{trainrace}_{_train_years[0]}_{_train_years[-1]}'\n",
    "\n",
    "for testevent in testevents:\n",
    "    for feature_cnt in [2,3]:\n",
    "        df_train, df_test, _data= build_datasets(testevent, include_end, feature_cnt=feature_cnt)\n",
    "\n",
    "        key = f'{testevent}-{feature_cnt}'\n",
    "        dataset[key] = [df_train, df_test, _data]\n",
    "        featurecnt_str = 'withcurcautionlaps' if (feature_cnt==3) else 'nocurcautionlaps'\n",
    "\n",
    "        #datafile = f'{dataOutputRoot}/pitstop_nextpit_dataset-{dbid}-t{testevent}-alldata-{featurecnt_str}{includeend_str}.pickle'\n",
    "        #with open(datafile, 'wb') as f:\n",
    "        #    print('save data:', datafile)\n",
    "        #    savedata = [df_train, df_test, events, testevent, _data]\n",
    "        #    pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save data: data//pitstop_nextpit_dataset-Texas_2013_2017-alldata--includeend.pickle\n"
     ]
    }
   ],
   "source": [
    "datafile = f'{dataOutputRoot}/pitstop_nextpit_dataset-{dbid}-alldata-{includeend_str}.pickle'\n",
    "with open(datafile, 'wb') as f:\n",
    "    print('save data:', datafile)\n",
    "    savedata = dataset\n",
    "    pickle.dump(savedata, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start model training\n",
      "INFO:root:Epoch[0] Learning rate is 0.001\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]INFO:root:Number of parameters in MLPTrainingNetwork: 213\n",
      "100%|| 100/100 [00:01<00:00, 71.37it/s, avg_epoch_loss=1.42]\n",
      "INFO:root:Epoch[0] Elapsed time 1.403 seconds\n",
      "INFO:root:Epoch[0] Evaluation metric 'epoch_loss'=1.417492\n",
      "INFO:root:Epoch[1] Learning rate is 0.001\n",
      "100%|| 100/100 [00:01<00:00, 81.61it/s, avg_epoch_loss=0.959]\n",
      "INFO:root:Epoch[1] Elapsed time 1.237 seconds\n",
      "INFO:root:Epoch[1] Evaluation metric 'epoch_loss'=0.958943\n",
      "INFO:root:Epoch[2] Learning rate is 0.001\n",
      "100%|| 100/100 [00:01<00:00, 65.68it/s, avg_epoch_loss=0.88]\n",
      "INFO:root:Epoch[2] Elapsed time 1.543 seconds\n",
      "INFO:root:Epoch[2] Evaluation metric 'epoch_loss'=0.879845\n",
      "INFO:root:Epoch[3] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 127.37it/s, avg_epoch_loss=0.818]\n",
      "INFO:root:Epoch[3] Elapsed time 0.787 seconds\n",
      "INFO:root:Epoch[3] Evaluation metric 'epoch_loss'=0.817695\n",
      "INFO:root:Epoch[4] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 143.83it/s, avg_epoch_loss=0.801]\n",
      "INFO:root:Epoch[4] Elapsed time 0.697 seconds\n",
      "INFO:root:Epoch[4] Evaluation metric 'epoch_loss'=0.800944\n",
      "INFO:root:Epoch[5] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 134.33it/s, avg_epoch_loss=0.779]\n",
      "INFO:root:Epoch[5] Elapsed time 0.746 seconds\n",
      "INFO:root:Epoch[5] Evaluation metric 'epoch_loss'=0.779073\n",
      "INFO:root:Epoch[6] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 120.62it/s, avg_epoch_loss=0.777]\n",
      "INFO:root:Epoch[6] Elapsed time 0.831 seconds\n",
      "INFO:root:Epoch[6] Evaluation metric 'epoch_loss'=0.776855\n",
      "INFO:root:Epoch[7] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 118.50it/s, avg_epoch_loss=0.756]\n",
      "INFO:root:Epoch[7] Elapsed time 0.846 seconds\n",
      "INFO:root:Epoch[7] Evaluation metric 'epoch_loss'=0.755812\n",
      "INFO:root:Epoch[8] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 101.84it/s, avg_epoch_loss=0.745]\n",
      "INFO:root:Epoch[8] Elapsed time 0.983 seconds\n",
      "INFO:root:Epoch[8] Evaluation metric 'epoch_loss'=0.744747\n",
      "INFO:root:Epoch[9] Learning rate is 0.001\n",
      "100%|| 100/100 [00:01<00:00, 99.33it/s, avg_epoch_loss=0.777]\n",
      "INFO:root:Epoch[9] Elapsed time 1.008 seconds\n",
      "INFO:root:Epoch[9] Evaluation metric 'epoch_loss'=0.777049\n",
      "INFO:root:Epoch[10] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 121.15it/s, avg_epoch_loss=0.754]\n",
      "INFO:root:Epoch[10] Elapsed time 0.832 seconds\n",
      "INFO:root:Epoch[10] Evaluation metric 'epoch_loss'=0.754288\n",
      "INFO:root:Epoch[11] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 134.35it/s, avg_epoch_loss=0.731]\n",
      "INFO:root:Epoch[11] Elapsed time 0.746 seconds\n",
      "INFO:root:Epoch[11] Evaluation metric 'epoch_loss'=0.731201\n",
      "INFO:root:Epoch[12] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 134.33it/s, avg_epoch_loss=0.729]\n",
      "INFO:root:Epoch[12] Elapsed time 0.747 seconds\n",
      "INFO:root:Epoch[12] Evaluation metric 'epoch_loss'=0.729362\n",
      "INFO:root:Epoch[13] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 135.73it/s, avg_epoch_loss=0.741]\n",
      "INFO:root:Epoch[13] Elapsed time 0.740 seconds\n",
      "INFO:root:Epoch[13] Evaluation metric 'epoch_loss'=0.741471\n",
      "INFO:root:Epoch[14] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 142.61it/s, avg_epoch_loss=0.707]\n",
      "INFO:root:Epoch[14] Elapsed time 0.703 seconds\n",
      "INFO:root:Epoch[14] Evaluation metric 'epoch_loss'=0.707182\n",
      "INFO:root:Epoch[15] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 150.95it/s, avg_epoch_loss=0.718]\n",
      "INFO:root:Epoch[15] Elapsed time 0.664 seconds\n",
      "INFO:root:Epoch[15] Evaluation metric 'epoch_loss'=0.717765\n",
      "INFO:root:Epoch[16] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 137.36it/s, avg_epoch_loss=0.726]\n",
      "INFO:root:Epoch[16] Elapsed time 0.731 seconds\n",
      "INFO:root:Epoch[16] Evaluation metric 'epoch_loss'=0.726398\n",
      "INFO:root:Epoch[17] Learning rate is 0.001\n",
      "100%|| 100/100 [00:01<00:00, 94.94it/s, avg_epoch_loss=0.723]\n",
      "INFO:root:Epoch[17] Elapsed time 1.055 seconds\n",
      "INFO:root:Epoch[17] Evaluation metric 'epoch_loss'=0.722915\n",
      "INFO:root:Epoch[18] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 138.05it/s, avg_epoch_loss=0.699]\n",
      "INFO:root:Epoch[18] Elapsed time 0.742 seconds\n",
      "INFO:root:Epoch[18] Evaluation metric 'epoch_loss'=0.698979\n",
      "INFO:root:Epoch[19] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 123.60it/s, avg_epoch_loss=0.696]\n",
      "INFO:root:Epoch[19] Elapsed time 0.811 seconds\n",
      "INFO:root:Epoch[19] Evaluation metric 'epoch_loss'=0.695701\n",
      "INFO:root:Epoch[20] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 144.72it/s, avg_epoch_loss=0.729]\n",
      "INFO:root:Epoch[20] Elapsed time 0.692 seconds\n",
      "INFO:root:Epoch[20] Evaluation metric 'epoch_loss'=0.729496\n",
      "INFO:root:Epoch[21] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 152.19it/s, avg_epoch_loss=0.69]\n",
      "INFO:root:Epoch[21] Elapsed time 0.658 seconds\n",
      "INFO:root:Epoch[21] Evaluation metric 'epoch_loss'=0.689892\n",
      "INFO:root:Epoch[22] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 123.35it/s, avg_epoch_loss=0.712]\n",
      "INFO:root:Epoch[22] Elapsed time 0.814 seconds\n",
      "INFO:root:Epoch[22] Evaluation metric 'epoch_loss'=0.711939\n",
      "INFO:root:Epoch[23] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 141.36it/s, avg_epoch_loss=0.708]\n",
      "INFO:root:Epoch[23] Elapsed time 0.709 seconds\n",
      "INFO:root:Epoch[23] Evaluation metric 'epoch_loss'=0.708363\n",
      "INFO:root:Epoch[24] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 147.64it/s, avg_epoch_loss=0.72]\n",
      "INFO:root:Epoch[24] Elapsed time 0.679 seconds\n",
      "INFO:root:Epoch[24] Evaluation metric 'epoch_loss'=0.720346\n",
      "INFO:root:Epoch[25] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 157.79it/s, avg_epoch_loss=0.691]\n",
      "INFO:root:Epoch[25] Elapsed time 0.636 seconds\n",
      "INFO:root:Epoch[25] Evaluation metric 'epoch_loss'=0.691020\n",
      "INFO:root:Epoch[26] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 144.53it/s, avg_epoch_loss=0.679]\n",
      "INFO:root:Epoch[26] Elapsed time 0.694 seconds\n",
      "INFO:root:Epoch[26] Evaluation metric 'epoch_loss'=0.679458\n",
      "INFO:root:Epoch[27] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 118.26it/s, avg_epoch_loss=0.713]\n",
      "INFO:root:Epoch[27] Elapsed time 0.847 seconds\n",
      "INFO:root:Epoch[27] Evaluation metric 'epoch_loss'=0.713298\n",
      "INFO:root:Epoch[28] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 124.64it/s, avg_epoch_loss=0.704]\n",
      "INFO:root:Epoch[28] Elapsed time 0.804 seconds\n",
      "INFO:root:Epoch[28] Evaluation metric 'epoch_loss'=0.703911\n",
      "INFO:root:Epoch[29] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 145.18it/s, avg_epoch_loss=0.696]\n",
      "INFO:root:Epoch[29] Elapsed time 0.691 seconds\n",
      "INFO:root:Epoch[29] Evaluation metric 'epoch_loss'=0.695990\n",
      "INFO:root:Epoch[30] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 130.73it/s, avg_epoch_loss=0.687]\n",
      "INFO:root:Epoch[30] Elapsed time 0.766 seconds\n",
      "INFO:root:Epoch[30] Evaluation metric 'epoch_loss'=0.686724\n",
      "INFO:root:Epoch[31] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 156.67it/s, avg_epoch_loss=0.709]\n",
      "INFO:root:Epoch[31] Elapsed time 0.640 seconds\n",
      "INFO:root:Epoch[31] Evaluation metric 'epoch_loss'=0.708952\n",
      "INFO:root:Epoch[32] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 154.13it/s, avg_epoch_loss=0.679]\n",
      "INFO:root:Epoch[32] Elapsed time 0.650 seconds\n",
      "INFO:root:Epoch[32] Evaluation metric 'epoch_loss'=0.678846\n",
      "INFO:root:Epoch[33] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 130.97it/s, avg_epoch_loss=0.673]\n",
      "INFO:root:Epoch[33] Elapsed time 0.766 seconds\n",
      "INFO:root:Epoch[33] Evaluation metric 'epoch_loss'=0.672553\n",
      "INFO:root:Epoch[34] Learning rate is 0.001\n",
      "100%|| 100/100 [00:01<00:00, 83.03it/s, avg_epoch_loss=0.689]\n",
      "INFO:root:Epoch[34] Elapsed time 1.225 seconds\n",
      "INFO:root:Epoch[34] Evaluation metric 'epoch_loss'=0.689215\n",
      "INFO:root:Epoch[35] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 103.27it/s, avg_epoch_loss=0.701]\n",
      "INFO:root:Epoch[35] Elapsed time 0.971 seconds\n",
      "INFO:root:Epoch[35] Evaluation metric 'epoch_loss'=0.700753\n",
      "INFO:root:Epoch[36] Learning rate is 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 141.78it/s, avg_epoch_loss=0.669]\n",
      "INFO:root:Epoch[36] Elapsed time 0.707 seconds\n",
      "INFO:root:Epoch[36] Evaluation metric 'epoch_loss'=0.669150\n",
      "INFO:root:Epoch[37] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 135.49it/s, avg_epoch_loss=0.673]\n",
      "INFO:root:Epoch[37] Elapsed time 0.739 seconds\n",
      "INFO:root:Epoch[37] Evaluation metric 'epoch_loss'=0.673108\n",
      "INFO:root:Epoch[38] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 150.99it/s, avg_epoch_loss=0.718]\n",
      "INFO:root:Epoch[38] Elapsed time 0.664 seconds\n",
      "INFO:root:Epoch[38] Evaluation metric 'epoch_loss'=0.717541\n",
      "INFO:root:Epoch[39] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 132.92it/s, avg_epoch_loss=0.673]\n",
      "INFO:root:Epoch[39] Elapsed time 0.754 seconds\n",
      "INFO:root:Epoch[39] Evaluation metric 'epoch_loss'=0.672770\n",
      "INFO:root:Epoch[40] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 138.80it/s, avg_epoch_loss=0.681]\n",
      "INFO:root:Epoch[40] Elapsed time 0.723 seconds\n",
      "INFO:root:Epoch[40] Evaluation metric 'epoch_loss'=0.681454\n",
      "INFO:root:Epoch[41] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 145.31it/s, avg_epoch_loss=0.677]\n",
      "INFO:root:Epoch[41] Elapsed time 0.690 seconds\n",
      "INFO:root:Epoch[41] Evaluation metric 'epoch_loss'=0.676955\n",
      "INFO:root:Epoch[42] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 118.00it/s, avg_epoch_loss=0.702]\n",
      "INFO:root:Epoch[42] Elapsed time 0.851 seconds\n",
      "INFO:root:Epoch[42] Evaluation metric 'epoch_loss'=0.702084\n",
      "INFO:root:Epoch[43] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 130.71it/s, avg_epoch_loss=0.665]\n",
      "INFO:root:Epoch[43] Elapsed time 0.773 seconds\n",
      "INFO:root:Epoch[43] Evaluation metric 'epoch_loss'=0.665238\n",
      "INFO:root:Epoch[44] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 120.93it/s, avg_epoch_loss=0.669]\n",
      "INFO:root:Epoch[44] Elapsed time 0.832 seconds\n",
      "INFO:root:Epoch[44] Evaluation metric 'epoch_loss'=0.669400\n",
      "INFO:root:Epoch[45] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 156.60it/s, avg_epoch_loss=0.693]\n",
      "INFO:root:Epoch[45] Elapsed time 0.643 seconds\n",
      "INFO:root:Epoch[45] Evaluation metric 'epoch_loss'=0.692604\n",
      "INFO:root:Epoch[46] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 157.29it/s, avg_epoch_loss=0.674]\n",
      "INFO:root:Epoch[46] Elapsed time 0.640 seconds\n",
      "INFO:root:Epoch[46] Evaluation metric 'epoch_loss'=0.674454\n",
      "INFO:root:Epoch[47] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 158.51it/s, avg_epoch_loss=0.69]\n",
      "INFO:root:Epoch[47] Elapsed time 0.636 seconds\n",
      "INFO:root:Epoch[47] Evaluation metric 'epoch_loss'=0.689614\n",
      "INFO:root:Epoch[48] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 158.16it/s, avg_epoch_loss=0.678]\n",
      "INFO:root:Epoch[48] Elapsed time 0.637 seconds\n",
      "INFO:root:Epoch[48] Evaluation metric 'epoch_loss'=0.677832\n",
      "INFO:root:Epoch[49] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 130.11it/s, avg_epoch_loss=0.7]\n",
      "INFO:root:Epoch[49] Elapsed time 0.771 seconds\n",
      "INFO:root:Epoch[49] Evaluation metric 'epoch_loss'=0.699951\n",
      "INFO:root:Epoch[50] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 148.42it/s, avg_epoch_loss=0.662]\n",
      "INFO:root:Epoch[50] Elapsed time 0.675 seconds\n",
      "INFO:root:Epoch[50] Evaluation metric 'epoch_loss'=0.661832\n",
      "INFO:root:Epoch[51] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 134.89it/s, avg_epoch_loss=0.675]\n",
      "INFO:root:Epoch[51] Elapsed time 0.743 seconds\n",
      "INFO:root:Epoch[51] Evaluation metric 'epoch_loss'=0.674686\n",
      "INFO:root:Epoch[52] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 157.95it/s, avg_epoch_loss=0.69]\n",
      "INFO:root:Epoch[52] Elapsed time 0.635 seconds\n",
      "INFO:root:Epoch[52] Evaluation metric 'epoch_loss'=0.689523\n",
      "INFO:root:Epoch[53] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 152.58it/s, avg_epoch_loss=0.689]\n",
      "INFO:root:Epoch[53] Elapsed time 0.657 seconds\n",
      "INFO:root:Epoch[53] Evaluation metric 'epoch_loss'=0.689029\n",
      "INFO:root:Epoch[54] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 150.88it/s, avg_epoch_loss=0.665]\n",
      "INFO:root:Epoch[54] Elapsed time 0.664 seconds\n",
      "INFO:root:Epoch[54] Evaluation metric 'epoch_loss'=0.665265\n",
      "INFO:root:Epoch[55] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 151.18it/s, avg_epoch_loss=0.663]\n",
      "INFO:root:Epoch[55] Elapsed time 0.663 seconds\n",
      "INFO:root:Epoch[55] Evaluation metric 'epoch_loss'=0.662639\n",
      "INFO:root:Epoch[56] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 126.02it/s, avg_epoch_loss=0.701]\n",
      "INFO:root:Epoch[56] Elapsed time 0.795 seconds\n",
      "INFO:root:Epoch[56] Evaluation metric 'epoch_loss'=0.700739\n",
      "INFO:root:Epoch[57] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 140.72it/s, avg_epoch_loss=0.671]\n",
      "INFO:root:Epoch[57] Elapsed time 0.713 seconds\n",
      "INFO:root:Epoch[57] Evaluation metric 'epoch_loss'=0.671207\n",
      "INFO:root:Epoch[58] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 105.04it/s, avg_epoch_loss=0.666]\n",
      "INFO:root:Epoch[58] Elapsed time 0.953 seconds\n",
      "INFO:root:Epoch[58] Evaluation metric 'epoch_loss'=0.665804\n",
      "INFO:root:Epoch[59] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 130.09it/s, avg_epoch_loss=0.674]\n",
      "INFO:root:Epoch[59] Elapsed time 0.770 seconds\n",
      "INFO:root:Epoch[59] Evaluation metric 'epoch_loss'=0.673531\n",
      "INFO:root:Epoch[60] Learning rate is 0.001\n",
      "100%|| 100/100 [00:00<00:00, 147.60it/s, avg_epoch_loss=0.693]\n",
      "INFO:root:Epoch[60] Elapsed time 0.679 seconds\n",
      "INFO:root:Epoch[60] Evaluation metric 'epoch_loss'=0.693332\n",
      "INFO:root:Loading parameters from best epoch (50)\n",
      "INFO:root:Epoch[61] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 131.34it/s, avg_epoch_loss=0.664]\n",
      "INFO:root:Epoch[61] Elapsed time 0.765 seconds\n",
      "INFO:root:Epoch[61] Evaluation metric 'epoch_loss'=0.664057\n",
      "INFO:root:Epoch[62] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 144.35it/s, avg_epoch_loss=0.658]\n",
      "INFO:root:Epoch[62] Elapsed time 0.697 seconds\n",
      "INFO:root:Epoch[62] Evaluation metric 'epoch_loss'=0.657668\n",
      "INFO:root:Epoch[63] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 116.86it/s, avg_epoch_loss=0.703]\n",
      "INFO:root:Epoch[63] Elapsed time 0.865 seconds\n",
      "INFO:root:Epoch[63] Evaluation metric 'epoch_loss'=0.703368\n",
      "INFO:root:Epoch[64] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 121.28it/s, avg_epoch_loss=0.658]\n",
      "INFO:root:Epoch[64] Elapsed time 0.826 seconds\n",
      "INFO:root:Epoch[64] Evaluation metric 'epoch_loss'=0.657596\n",
      "INFO:root:Epoch[65] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 101.57it/s, avg_epoch_loss=0.673]\n",
      "INFO:root:Epoch[65] Elapsed time 0.986 seconds\n",
      "INFO:root:Epoch[65] Evaluation metric 'epoch_loss'=0.672564\n",
      "INFO:root:Epoch[66] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 150.30it/s, avg_epoch_loss=0.667]\n",
      "INFO:root:Epoch[66] Elapsed time 0.667 seconds\n",
      "INFO:root:Epoch[66] Evaluation metric 'epoch_loss'=0.666520\n",
      "INFO:root:Epoch[67] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 142.23it/s, avg_epoch_loss=0.688]\n",
      "INFO:root:Epoch[67] Elapsed time 0.706 seconds\n",
      "INFO:root:Epoch[67] Evaluation metric 'epoch_loss'=0.688443\n",
      "INFO:root:Epoch[68] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 138.30it/s, avg_epoch_loss=0.661]\n",
      "INFO:root:Epoch[68] Elapsed time 0.725 seconds\n",
      "INFO:root:Epoch[68] Evaluation metric 'epoch_loss'=0.660810\n",
      "INFO:root:Epoch[69] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 151.54it/s, avg_epoch_loss=0.668]\n",
      "INFO:root:Epoch[69] Elapsed time 0.665 seconds\n",
      "INFO:root:Epoch[69] Evaluation metric 'epoch_loss'=0.667612\n",
      "INFO:root:Epoch[70] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 157.42it/s, avg_epoch_loss=0.681]\n",
      "INFO:root:Epoch[70] Elapsed time 0.640 seconds\n",
      "INFO:root:Epoch[70] Evaluation metric 'epoch_loss'=0.681283\n",
      "INFO:root:Epoch[71] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 151.83it/s, avg_epoch_loss=0.684]\n",
      "INFO:root:Epoch[71] Elapsed time 0.663 seconds\n",
      "INFO:root:Epoch[71] Evaluation metric 'epoch_loss'=0.683900\n",
      "INFO:root:Epoch[72] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 158.23it/s, avg_epoch_loss=0.662]\n",
      "INFO:root:Epoch[72] Elapsed time 0.635 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[72] Evaluation metric 'epoch_loss'=0.661721\n",
      "INFO:root:Epoch[73] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 153.24it/s, avg_epoch_loss=0.651]\n",
      "INFO:root:Epoch[73] Elapsed time 0.654 seconds\n",
      "INFO:root:Epoch[73] Evaluation metric 'epoch_loss'=0.650732\n",
      "INFO:root:Epoch[74] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 135.00it/s, avg_epoch_loss=0.694]\n",
      "INFO:root:Epoch[74] Elapsed time 0.743 seconds\n",
      "INFO:root:Epoch[74] Evaluation metric 'epoch_loss'=0.693686\n",
      "INFO:root:Epoch[75] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 158.60it/s, avg_epoch_loss=0.662]\n",
      "INFO:root:Epoch[75] Elapsed time 0.632 seconds\n",
      "INFO:root:Epoch[75] Evaluation metric 'epoch_loss'=0.661988\n",
      "INFO:root:Epoch[76] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 157.64it/s, avg_epoch_loss=0.66]\n",
      "INFO:root:Epoch[76] Elapsed time 0.636 seconds\n",
      "INFO:root:Epoch[76] Evaluation metric 'epoch_loss'=0.660338\n",
      "INFO:root:Epoch[77] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 158.52it/s, avg_epoch_loss=0.681]\n",
      "INFO:root:Epoch[77] Elapsed time 0.632 seconds\n",
      "INFO:root:Epoch[77] Evaluation metric 'epoch_loss'=0.680726\n",
      "INFO:root:Epoch[78] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 157.35it/s, avg_epoch_loss=0.698]\n",
      "INFO:root:Epoch[78] Elapsed time 0.637 seconds\n",
      "INFO:root:Epoch[78] Evaluation metric 'epoch_loss'=0.698342\n",
      "INFO:root:Epoch[79] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 157.56it/s, avg_epoch_loss=0.638]\n",
      "INFO:root:Epoch[79] Elapsed time 0.636 seconds\n",
      "INFO:root:Epoch[79] Evaluation metric 'epoch_loss'=0.638382\n",
      "INFO:root:Epoch[80] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 137.75it/s, avg_epoch_loss=0.663]\n",
      "INFO:root:Epoch[80] Elapsed time 0.728 seconds\n",
      "INFO:root:Epoch[80] Evaluation metric 'epoch_loss'=0.663145\n",
      "INFO:root:Epoch[81] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 149.92it/s, avg_epoch_loss=0.693]\n",
      "INFO:root:Epoch[81] Elapsed time 0.669 seconds\n",
      "INFO:root:Epoch[81] Evaluation metric 'epoch_loss'=0.693085\n",
      "INFO:root:Epoch[82] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 138.49it/s, avg_epoch_loss=0.66]\n",
      "INFO:root:Epoch[82] Elapsed time 0.724 seconds\n",
      "INFO:root:Epoch[82] Evaluation metric 'epoch_loss'=0.660498\n",
      "INFO:root:Epoch[83] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 159.56it/s, avg_epoch_loss=0.673]\n",
      "INFO:root:Epoch[83] Elapsed time 0.628 seconds\n",
      "INFO:root:Epoch[83] Evaluation metric 'epoch_loss'=0.673282\n",
      "INFO:root:Epoch[84] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 154.88it/s, avg_epoch_loss=0.677]\n",
      "INFO:root:Epoch[84] Elapsed time 0.647 seconds\n",
      "INFO:root:Epoch[84] Evaluation metric 'epoch_loss'=0.677163\n",
      "INFO:root:Epoch[85] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 150.60it/s, avg_epoch_loss=0.678]\n",
      "INFO:root:Epoch[85] Elapsed time 0.673 seconds\n",
      "INFO:root:Epoch[85] Evaluation metric 'epoch_loss'=0.677550\n",
      "INFO:root:Epoch[86] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 143.53it/s, avg_epoch_loss=0.644]\n",
      "INFO:root:Epoch[86] Elapsed time 0.708 seconds\n",
      "INFO:root:Epoch[86] Evaluation metric 'epoch_loss'=0.643864\n",
      "INFO:root:Epoch[87] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 151.33it/s, avg_epoch_loss=0.662]\n",
      "INFO:root:Epoch[87] Elapsed time 0.662 seconds\n",
      "INFO:root:Epoch[87] Evaluation metric 'epoch_loss'=0.661820\n",
      "INFO:root:Epoch[88] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 143.42it/s, avg_epoch_loss=0.688]\n",
      "INFO:root:Epoch[88] Elapsed time 0.702 seconds\n",
      "INFO:root:Epoch[88] Evaluation metric 'epoch_loss'=0.688151\n",
      "INFO:root:Epoch[89] Learning rate is 0.0005\n",
      "100%|| 100/100 [00:00<00:00, 154.27it/s, avg_epoch_loss=0.666]\n",
      "INFO:root:Epoch[89] Elapsed time 0.653 seconds\n",
      "INFO:root:Epoch[89] Evaluation metric 'epoch_loss'=0.666387\n",
      "INFO:root:Loading parameters from best epoch (79)\n",
      "INFO:root:Epoch[90] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 141.40it/s, avg_epoch_loss=0.667]\n",
      "INFO:root:Epoch[90] Elapsed time 0.709 seconds\n",
      "INFO:root:Epoch[90] Evaluation metric 'epoch_loss'=0.667468\n",
      "INFO:root:Epoch[91] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 149.92it/s, avg_epoch_loss=0.649]\n",
      "INFO:root:Epoch[91] Elapsed time 0.668 seconds\n",
      "INFO:root:Epoch[91] Evaluation metric 'epoch_loss'=0.649306\n",
      "INFO:root:Epoch[92] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 146.07it/s, avg_epoch_loss=0.694]\n",
      "INFO:root:Epoch[92] Elapsed time 0.686 seconds\n",
      "INFO:root:Epoch[92] Evaluation metric 'epoch_loss'=0.694217\n",
      "INFO:root:Epoch[93] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 144.26it/s, avg_epoch_loss=0.659]\n",
      "INFO:root:Epoch[93] Elapsed time 0.695 seconds\n",
      "INFO:root:Epoch[93] Evaluation metric 'epoch_loss'=0.658535\n",
      "INFO:root:Epoch[94] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 101.16it/s, avg_epoch_loss=0.642]\n",
      "INFO:root:Epoch[94] Elapsed time 0.993 seconds\n",
      "INFO:root:Epoch[94] Evaluation metric 'epoch_loss'=0.642351\n",
      "INFO:root:Epoch[95] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:01<00:00, 96.73it/s, avg_epoch_loss=0.672]\n",
      "INFO:root:Epoch[95] Elapsed time 1.038 seconds\n",
      "INFO:root:Epoch[95] Evaluation metric 'epoch_loss'=0.672277\n",
      "INFO:root:Epoch[96] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 126.71it/s, avg_epoch_loss=0.691]\n",
      "INFO:root:Epoch[96] Elapsed time 0.791 seconds\n",
      "INFO:root:Epoch[96] Evaluation metric 'epoch_loss'=0.690792\n",
      "INFO:root:Epoch[97] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 147.45it/s, avg_epoch_loss=0.656]\n",
      "INFO:root:Epoch[97] Elapsed time 0.680 seconds\n",
      "INFO:root:Epoch[97] Evaluation metric 'epoch_loss'=0.655819\n",
      "INFO:root:Epoch[98] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 142.71it/s, avg_epoch_loss=0.663]\n",
      "INFO:root:Epoch[98] Elapsed time 0.702 seconds\n",
      "INFO:root:Epoch[98] Evaluation metric 'epoch_loss'=0.663361\n",
      "INFO:root:Epoch[99] Learning rate is 0.00025\n",
      "100%|| 100/100 [00:00<00:00, 148.76it/s, avg_epoch_loss=0.698]\n",
      "INFO:root:Epoch[99] Elapsed time 0.674 seconds\n",
      "INFO:root:Epoch[99] Evaluation metric 'epoch_loss'=0.697810\n",
      "INFO:root:Loading parameters from best epoch (79)\n",
      "INFO:root:Epoch[100] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:00<00:00, 126.79it/s, avg_epoch_loss=0.653]\n",
      "INFO:root:Epoch[100] Elapsed time 0.792 seconds\n",
      "INFO:root:Epoch[100] Evaluation metric 'epoch_loss'=0.653446\n",
      "INFO:root:Epoch[101] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:00<00:00, 109.13it/s, avg_epoch_loss=0.657]\n",
      "INFO:root:Epoch[101] Elapsed time 0.918 seconds\n",
      "INFO:root:Epoch[101] Evaluation metric 'epoch_loss'=0.656865\n",
      "INFO:root:Epoch[102] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:00<00:00, 115.16it/s, avg_epoch_loss=0.658]\n",
      "INFO:root:Epoch[102] Elapsed time 0.870 seconds\n",
      "INFO:root:Epoch[102] Evaluation metric 'epoch_loss'=0.658120\n",
      "INFO:root:Epoch[103] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:00<00:00, 131.99it/s, avg_epoch_loss=0.696]\n",
      "INFO:root:Epoch[103] Elapsed time 0.759 seconds\n",
      "INFO:root:Epoch[103] Evaluation metric 'epoch_loss'=0.695706\n",
      "INFO:root:Epoch[104] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:01<00:00, 83.63it/s, avg_epoch_loss=0.658]\n",
      "INFO:root:Epoch[104] Elapsed time 1.215 seconds\n",
      "INFO:root:Epoch[104] Evaluation metric 'epoch_loss'=0.658418\n",
      "INFO:root:Epoch[105] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:01<00:00, 94.53it/s, avg_epoch_loss=0.657]\n",
      "INFO:root:Epoch[105] Elapsed time 1.060 seconds\n",
      "INFO:root:Epoch[105] Evaluation metric 'epoch_loss'=0.657191\n",
      "INFO:root:Epoch[106] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:00<00:00, 127.81it/s, avg_epoch_loss=0.686]\n",
      "INFO:root:Epoch[106] Elapsed time 0.784 seconds\n",
      "INFO:root:Epoch[106] Evaluation metric 'epoch_loss'=0.685634\n",
      "INFO:root:Epoch[107] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:00<00:00, 119.27it/s, avg_epoch_loss=0.673]\n",
      "INFO:root:Epoch[107] Elapsed time 0.840 seconds\n",
      "INFO:root:Epoch[107] Evaluation metric 'epoch_loss'=0.673451\n",
      "INFO:root:Epoch[108] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:00<00:00, 153.96it/s, avg_epoch_loss=0.669]\n",
      "INFO:root:Epoch[108] Elapsed time 0.651 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[108] Evaluation metric 'epoch_loss'=0.668534\n",
      "INFO:root:Epoch[109] Learning rate is 0.000125\n",
      "100%|| 100/100 [00:01<00:00, 95.27it/s, avg_epoch_loss=0.653]\n",
      "INFO:root:Epoch[109] Elapsed time 1.051 seconds\n",
      "INFO:root:Epoch[109] Evaluation metric 'epoch_loss'=0.652600\n",
      "INFO:root:Loading parameters from best epoch (79)\n",
      "INFO:root:Epoch[110] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 119.77it/s, avg_epoch_loss=0.689]\n",
      "INFO:root:Epoch[110] Elapsed time 0.837 seconds\n",
      "INFO:root:Epoch[110] Evaluation metric 'epoch_loss'=0.689449\n",
      "INFO:root:Epoch[111] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 131.70it/s, avg_epoch_loss=0.662]\n",
      "INFO:root:Epoch[111] Elapsed time 0.761 seconds\n",
      "INFO:root:Epoch[111] Evaluation metric 'epoch_loss'=0.662122\n",
      "INFO:root:Epoch[112] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 148.29it/s, avg_epoch_loss=0.657]\n",
      "INFO:root:Epoch[112] Elapsed time 0.676 seconds\n",
      "INFO:root:Epoch[112] Evaluation metric 'epoch_loss'=0.657425\n",
      "INFO:root:Epoch[113] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 144.03it/s, avg_epoch_loss=0.683]\n",
      "INFO:root:Epoch[113] Elapsed time 0.699 seconds\n",
      "INFO:root:Epoch[113] Evaluation metric 'epoch_loss'=0.682640\n",
      "INFO:root:Epoch[114] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 144.28it/s, avg_epoch_loss=0.695]\n",
      "INFO:root:Epoch[114] Elapsed time 0.695 seconds\n",
      "INFO:root:Epoch[114] Evaluation metric 'epoch_loss'=0.695256\n",
      "INFO:root:Epoch[115] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 128.54it/s, avg_epoch_loss=0.666]\n",
      "INFO:root:Epoch[115] Elapsed time 0.790 seconds\n",
      "INFO:root:Epoch[115] Evaluation metric 'epoch_loss'=0.665733\n",
      "INFO:root:Epoch[116] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:01<00:00, 99.75it/s, avg_epoch_loss=0.665]\n",
      "INFO:root:Epoch[116] Elapsed time 1.004 seconds\n",
      "INFO:root:Epoch[116] Evaluation metric 'epoch_loss'=0.665127\n",
      "INFO:root:Epoch[117] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 133.74it/s, avg_epoch_loss=0.696]\n",
      "INFO:root:Epoch[117] Elapsed time 0.753 seconds\n",
      "INFO:root:Epoch[117] Evaluation metric 'epoch_loss'=0.695906\n",
      "INFO:root:Epoch[118] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 145.56it/s, avg_epoch_loss=0.642]\n",
      "INFO:root:Epoch[118] Elapsed time 0.689 seconds\n",
      "INFO:root:Epoch[118] Evaluation metric 'epoch_loss'=0.642370\n",
      "INFO:root:Epoch[119] Learning rate is 6.25e-05\n",
      "100%|| 100/100 [00:00<00:00, 113.56it/s, avg_epoch_loss=0.657]\n",
      "INFO:root:Epoch[119] Elapsed time 0.882 seconds\n",
      "INFO:root:Epoch[119] Evaluation metric 'epoch_loss'=0.657027\n",
      "INFO:root:Loading parameters from best epoch (79)\n",
      "INFO:root:Epoch[120] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 140.50it/s, avg_epoch_loss=0.666]\n",
      "INFO:root:Epoch[120] Elapsed time 0.713 seconds\n",
      "INFO:root:Epoch[120] Evaluation metric 'epoch_loss'=0.665837\n",
      "INFO:root:Epoch[121] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 146.44it/s, avg_epoch_loss=0.694]\n",
      "INFO:root:Epoch[121] Elapsed time 0.692 seconds\n",
      "INFO:root:Epoch[121] Evaluation metric 'epoch_loss'=0.694032\n",
      "INFO:root:Epoch[122] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 151.09it/s, avg_epoch_loss=0.651]\n",
      "INFO:root:Epoch[122] Elapsed time 0.664 seconds\n",
      "INFO:root:Epoch[122] Evaluation metric 'epoch_loss'=0.650893\n",
      "INFO:root:Epoch[123] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 133.68it/s, avg_epoch_loss=0.65]\n",
      "INFO:root:Epoch[123] Elapsed time 0.771 seconds\n",
      "INFO:root:Epoch[123] Evaluation metric 'epoch_loss'=0.650478\n",
      "INFO:root:Epoch[124] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:01<00:00, 79.40it/s, avg_epoch_loss=0.689]\n",
      "INFO:root:Epoch[124] Elapsed time 1.261 seconds\n",
      "INFO:root:Epoch[124] Evaluation metric 'epoch_loss'=0.689161\n",
      "INFO:root:Epoch[125] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 118.99it/s, avg_epoch_loss=0.659]\n",
      "INFO:root:Epoch[125] Elapsed time 0.842 seconds\n",
      "INFO:root:Epoch[125] Evaluation metric 'epoch_loss'=0.659419\n",
      "INFO:root:Epoch[126] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:01<00:00, 89.51it/s, avg_epoch_loss=0.659]\n",
      "INFO:root:Epoch[126] Elapsed time 1.118 seconds\n",
      "INFO:root:Epoch[126] Evaluation metric 'epoch_loss'=0.658981\n",
      "INFO:root:Epoch[127] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 102.59it/s, avg_epoch_loss=0.654]\n",
      "INFO:root:Epoch[127] Elapsed time 0.980 seconds\n",
      "INFO:root:Epoch[127] Evaluation metric 'epoch_loss'=0.653632\n",
      "INFO:root:Epoch[128] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 136.83it/s, avg_epoch_loss=0.698]\n",
      "INFO:root:Epoch[128] Elapsed time 0.732 seconds\n",
      "INFO:root:Epoch[128] Evaluation metric 'epoch_loss'=0.697516\n",
      "INFO:root:Epoch[129] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 156.09it/s, avg_epoch_loss=0.641]\n",
      "INFO:root:Epoch[129] Elapsed time 0.642 seconds\n",
      "INFO:root:Epoch[129] Evaluation metric 'epoch_loss'=0.641049\n",
      "INFO:root:Epoch[130] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 143.31it/s, avg_epoch_loss=0.657]\n",
      "INFO:root:Epoch[130] Elapsed time 0.700 seconds\n",
      "INFO:root:Epoch[130] Evaluation metric 'epoch_loss'=0.657200\n",
      "INFO:root:Epoch[131] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 152.31it/s, avg_epoch_loss=0.673]\n",
      "INFO:root:Epoch[131] Elapsed time 0.662 seconds\n",
      "INFO:root:Epoch[131] Evaluation metric 'epoch_loss'=0.672961\n",
      "INFO:root:Epoch[132] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:01<00:00, 80.42it/s, avg_epoch_loss=0.676]\n",
      "INFO:root:Epoch[132] Elapsed time 1.246 seconds\n",
      "INFO:root:Epoch[132] Evaluation metric 'epoch_loss'=0.676202\n",
      "INFO:root:Epoch[133] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 105.40it/s, avg_epoch_loss=0.668]\n",
      "INFO:root:Epoch[133] Elapsed time 0.952 seconds\n",
      "INFO:root:Epoch[133] Evaluation metric 'epoch_loss'=0.668243\n",
      "INFO:root:Epoch[134] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 122.09it/s, avg_epoch_loss=0.663]\n",
      "INFO:root:Epoch[134] Elapsed time 0.823 seconds\n",
      "INFO:root:Epoch[134] Evaluation metric 'epoch_loss'=0.662927\n",
      "INFO:root:Epoch[135] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 121.82it/s, avg_epoch_loss=0.713]\n",
      "INFO:root:Epoch[135] Elapsed time 0.823 seconds\n",
      "INFO:root:Epoch[135] Evaluation metric 'epoch_loss'=0.712824\n",
      "INFO:root:Epoch[136] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 119.24it/s, avg_epoch_loss=0.653]\n",
      "INFO:root:Epoch[136] Elapsed time 0.843 seconds\n",
      "INFO:root:Epoch[136] Evaluation metric 'epoch_loss'=0.652917\n",
      "INFO:root:Epoch[137] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:01<00:00, 91.61it/s, avg_epoch_loss=0.663]\n",
      "INFO:root:Epoch[137] Elapsed time 1.095 seconds\n",
      "INFO:root:Epoch[137] Evaluation metric 'epoch_loss'=0.662938\n",
      "INFO:root:Epoch[138] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 124.17it/s, avg_epoch_loss=0.663]\n",
      "INFO:root:Epoch[138] Elapsed time 0.807 seconds\n",
      "INFO:root:Epoch[138] Evaluation metric 'epoch_loss'=0.662818\n",
      "INFO:root:Epoch[139] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 114.17it/s, avg_epoch_loss=0.684]\n",
      "INFO:root:Epoch[139] Elapsed time 0.877 seconds\n",
      "INFO:root:Epoch[139] Evaluation metric 'epoch_loss'=0.684497\n",
      "INFO:root:Epoch[140] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 131.95it/s, avg_epoch_loss=0.66]\n",
      "INFO:root:Epoch[140] Elapsed time 0.760 seconds\n",
      "INFO:root:Epoch[140] Evaluation metric 'epoch_loss'=0.659516\n",
      "INFO:root:Epoch[141] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:01<00:00, 89.55it/s, avg_epoch_loss=0.665]\n",
      "INFO:root:Epoch[141] Elapsed time 1.119 seconds\n",
      "INFO:root:Epoch[141] Evaluation metric 'epoch_loss'=0.665363\n",
      "INFO:root:Epoch[142] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:01<00:00, 97.29it/s, avg_epoch_loss=0.69]\n",
      "INFO:root:Epoch[142] Elapsed time 1.031 seconds\n",
      "INFO:root:Epoch[142] Evaluation metric 'epoch_loss'=0.690368\n",
      "INFO:root:Epoch[143] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 115.19it/s, avg_epoch_loss=0.659]\n",
      "INFO:root:Epoch[143] Elapsed time 0.870 seconds\n",
      "INFO:root:Epoch[143] Evaluation metric 'epoch_loss'=0.659316\n",
      "INFO:root:Epoch[144] Learning rate is 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:01<00:00, 93.29it/s, avg_epoch_loss=0.664]\n",
      "INFO:root:Epoch[144] Elapsed time 1.112 seconds\n",
      "INFO:root:Epoch[144] Evaluation metric 'epoch_loss'=0.664334\n",
      "INFO:root:Epoch[145] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 134.31it/s, avg_epoch_loss=0.662]\n",
      "INFO:root:Epoch[145] Elapsed time 0.747 seconds\n",
      "INFO:root:Epoch[145] Evaluation metric 'epoch_loss'=0.661726\n",
      "INFO:root:Epoch[146] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 149.20it/s, avg_epoch_loss=0.682]\n",
      "INFO:root:Epoch[146] Elapsed time 0.673 seconds\n",
      "INFO:root:Epoch[146] Evaluation metric 'epoch_loss'=0.682343\n",
      "INFO:root:Epoch[147] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 156.96it/s, avg_epoch_loss=0.66]\n",
      "INFO:root:Epoch[147] Elapsed time 0.639 seconds\n",
      "INFO:root:Epoch[147] Evaluation metric 'epoch_loss'=0.659565\n",
      "INFO:root:Epoch[148] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 127.05it/s, avg_epoch_loss=0.646]\n",
      "INFO:root:Epoch[148] Elapsed time 0.789 seconds\n",
      "INFO:root:Epoch[148] Evaluation metric 'epoch_loss'=0.645913\n",
      "INFO:root:Epoch[149] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 109.59it/s, avg_epoch_loss=0.678]\n",
      "INFO:root:Epoch[149] Elapsed time 0.914 seconds\n",
      "INFO:root:Epoch[149] Evaluation metric 'epoch_loss'=0.678016\n",
      "INFO:root:Epoch[150] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 111.30it/s, avg_epoch_loss=0.674]\n",
      "INFO:root:Epoch[150] Elapsed time 0.900 seconds\n",
      "INFO:root:Epoch[150] Evaluation metric 'epoch_loss'=0.674495\n",
      "INFO:root:Epoch[151] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 144.46it/s, avg_epoch_loss=0.672]\n",
      "INFO:root:Epoch[151] Elapsed time 0.704 seconds\n",
      "INFO:root:Epoch[151] Evaluation metric 'epoch_loss'=0.671672\n",
      "INFO:root:Epoch[152] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 106.34it/s, avg_epoch_loss=0.653]\n",
      "INFO:root:Epoch[152] Elapsed time 0.945 seconds\n",
      "INFO:root:Epoch[152] Evaluation metric 'epoch_loss'=0.652628\n",
      "INFO:root:Epoch[153] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 127.99it/s, avg_epoch_loss=0.703]\n",
      "INFO:root:Epoch[153] Elapsed time 0.787 seconds\n",
      "INFO:root:Epoch[153] Evaluation metric 'epoch_loss'=0.703241\n",
      "INFO:root:Epoch[154] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 114.56it/s, avg_epoch_loss=0.648]\n",
      "INFO:root:Epoch[154] Elapsed time 0.874 seconds\n",
      "INFO:root:Epoch[154] Evaluation metric 'epoch_loss'=0.648483\n",
      "INFO:root:Epoch[155] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 106.08it/s, avg_epoch_loss=0.665]\n",
      "INFO:root:Epoch[155] Elapsed time 0.944 seconds\n",
      "INFO:root:Epoch[155] Evaluation metric 'epoch_loss'=0.664700\n",
      "INFO:root:Epoch[156] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 132.48it/s, avg_epoch_loss=0.674]\n",
      "INFO:root:Epoch[156] Elapsed time 0.757 seconds\n",
      "INFO:root:Epoch[156] Evaluation metric 'epoch_loss'=0.674430\n",
      "INFO:root:Epoch[157] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 154.67it/s, avg_epoch_loss=0.691]\n",
      "INFO:root:Epoch[157] Elapsed time 0.649 seconds\n",
      "INFO:root:Epoch[157] Evaluation metric 'epoch_loss'=0.690679\n",
      "INFO:root:Epoch[158] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:01<00:00, 95.90it/s, avg_epoch_loss=0.654]\n",
      "INFO:root:Epoch[158] Elapsed time 1.044 seconds\n",
      "INFO:root:Epoch[158] Evaluation metric 'epoch_loss'=0.653843\n",
      "INFO:root:Epoch[159] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 127.66it/s, avg_epoch_loss=0.664]\n",
      "INFO:root:Epoch[159] Elapsed time 0.785 seconds\n",
      "INFO:root:Epoch[159] Evaluation metric 'epoch_loss'=0.663663\n",
      "INFO:root:Epoch[160] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 151.86it/s, avg_epoch_loss=0.688]\n",
      "INFO:root:Epoch[160] Elapsed time 0.660 seconds\n",
      "INFO:root:Epoch[160] Evaluation metric 'epoch_loss'=0.688273\n",
      "INFO:root:Epoch[161] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 146.20it/s, avg_epoch_loss=0.65]\n",
      "INFO:root:Epoch[161] Elapsed time 0.686 seconds\n",
      "INFO:root:Epoch[161] Evaluation metric 'epoch_loss'=0.650047\n",
      "INFO:root:Epoch[162] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 128.91it/s, avg_epoch_loss=0.66]\n",
      "INFO:root:Epoch[162] Elapsed time 0.778 seconds\n",
      "INFO:root:Epoch[162] Evaluation metric 'epoch_loss'=0.660252\n",
      "INFO:root:Epoch[163] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 117.24it/s, avg_epoch_loss=0.666]\n",
      "INFO:root:Epoch[163] Elapsed time 0.854 seconds\n",
      "INFO:root:Epoch[163] Evaluation metric 'epoch_loss'=0.665977\n",
      "INFO:root:Epoch[164] Learning rate is 5e-05\n",
      "100%|| 100/100 [00:00<00:00, 113.65it/s, avg_epoch_loss=0.685]\n",
      "INFO:root:Epoch[164] Elapsed time 0.881 seconds\n",
      "INFO:root:Epoch[164] Evaluation metric 'epoch_loss'=0.684861\n",
      "INFO:root:Epoch[165] Learning rate is 5e-05\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "p, t, s, e = {}, {} ,{}, {}\n",
    "\n",
    "testevent=f'{trainrace}-2018'\n",
    "feature_cnt=2\n",
    "\n",
    "key = f'{testevent}-{feature_cnt}'\n",
    "df_train, df_test, _data = dataset[key]\n",
    "trainset, testset, train_ds, test_ds, scaler = _data['sel']\n",
    "pm, mid = train_model(500,dropout = 0.1,id='sel', feature_cnt=feature_cnt)\n",
    "t[mid],s[mid], e[mid] = eval_model(pm, test_ds)\n",
    "p[mid] = pm\n",
    "\n",
    "mae = raw_eval(t[mid],s[mid])\n",
    "\n",
    "pitmodel = save_full_pitmodel(mid, scaler, maxgap=100,feature_cnt=feature_cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, t, s, e = {}, {} ,{}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sync_test_ds = _data['all'][3]\n",
    "cols = ['Year','Model','MAE','50-Risk','90-Risk']\n",
    "\n",
    "### run train on all data\n",
    "allruns = ['sel','all','noshort','normal','caution','plen2']\n",
    "testevent='Indy500-2018'\n",
    "\n",
    "retdata = []\n",
    "for feature_cnt in [2,3]:\n",
    "    for tid in allruns:\n",
    "        #train\n",
    "        key = f'{testevent}-{feature_cnt}'\n",
    "        df_train, df_test, _data = dataset[key]\n",
    "        \n",
    "        trainset, testset, train_ds, test_ds, scaler = _data[tid]\n",
    "        pm, mid = train_model(500,dropout = 0.1, id=tid, feature_cnt=feature_cnt)\n",
    "\n",
    "        #test\n",
    "        sync_testset = _data['normal'][1]\n",
    "        sync_test_ds, _, _ = makedb(sync_testset, scaler, perm=False, feature_cnt=feature_cnt)\n",
    "        t[mid],s[mid], e[mid] = eval_model(pm, sync_test_ds)\n",
    "        p[mid] = pm\n",
    "\n",
    "        mae = raw_eval(t[mid],s[mid])\n",
    "        retdata.append([testevent,tid, mae, e[mid][\"wQuantileLoss[0.5]\"],e[mid][\"wQuantileLoss[0.9]\"]])\n",
    "\n",
    "        #run_test(t[mid], s[mid], [31,816,846,856])\n",
    "        pitmodel = save_full_pitmodel(mid, scaler, maxgap=65,feature_cnt=feature_cnt)\n",
    "    \n",
    "pitmodel_result = pd.DataFrame(data=retdata, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savedata('mlp-train-savedata.pickle',[p,t,s,e])\n",
    "p.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitmodel_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testid = 'sel'\n",
    "retdata = []\n",
    "truth, pred, samples = {}, {}, {}     \n",
    "#cols = ['Year','Model','MAE','50-Risk','90-Risk','50-Risk','90-Risk']\n",
    "cols = ['Year','Model','MAE','50-Risk','90-Risk']\n",
    "\n",
    "trainruns = ['sel','all','normal']\n",
    "testruns = ['sel','all','normal','caution','plen2']\n",
    "\n",
    "for feature_cnt in [2,3]:\n",
    "    for tid in trainruns:\n",
    "        mid = f\"mlp-d{tid}-f{feature_cnt}-e500-l10-10-5-student-d0.1\"\n",
    "\n",
    "        for testevent in ['Indy500-2018','Indy500-2019']:\n",
    "            key = f'{testevent}-{feature_cnt}'\n",
    "            df_train, df_test, _data = dataset[key]\n",
    "            scaler = _data[tid][-1]\n",
    "\n",
    "            for testid in testruns:\n",
    "                #trainset, testset, train_ds, test_ds, scaler\n",
    "                sync_testset = _data[testid][1]\n",
    "                test_ds, _, _ = makedb(sync_testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "                tx,sx, ex = eval_model(p[mid], test_ds)\n",
    "                mae = raw_eval(tx,sx)\n",
    "                print(mid, 'mae=', mae,ex[\"wQuantileLoss[0.5]\"],ex[\"wQuantileLoss[0.9]\"] )\n",
    "\n",
    "                # prisk direct\n",
    "                truth[mid], pred[mid], samples[mid] = decode(tx,sx)\n",
    "                idx = np.where(samples[mid] < 0)\n",
    "                samples[mid][idx] = 0\n",
    "\n",
    "                _, prisk_vals = prisk_direct_bysamples(samples[mid],truth[mid])\n",
    "                print(prisk_vals[1], prisk_vals[2])        \n",
    "\n",
    "                retdata.append([testevent,tid + '-' + testid, mae, \n",
    "                               #ex[\"wQuantileLoss[0.5]\"],ex[\"wQuantileLoss[0.9]\"],\n",
    "                               prisk_vals[1], prisk_vals[2]])\n",
    "        \n",
    "test_result = pd.DataFrame(data=retdata, columns=cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('pitmodel_mlp_evaluation_result_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testid = 'sel'\n",
    "retdata = []\n",
    "truth, pred, samples = {}, {}, {}     \n",
    "#cols = ['Year','Model','MAE','50-Risk','90-Risk','50-Risk','90-Risk']\n",
    "cols = ['Year','Model','FeatureCnt','MAE','50-Risk','90-Risk']\n",
    "\n",
    "trainruns = ['sel','all']\n",
    "for feature_cnt in [2,3]:\n",
    "    for tid in trainruns:\n",
    "        mid = f\"mlp-d{tid}-f{feature_cnt}-e500-l10-10-5-student-d0.1\"\n",
    "\n",
    "        for testevent in ['Indy500-2018','Indy500-2019']:\n",
    "            key = f'{testevent}-{feature_cnt}'\n",
    "            df_train, df_test, _data = dataset[key]\n",
    "            scaler = _data[tid][-1]\n",
    "\n",
    "            for testid in ['all','sel', 'caution','shortnormal']:\n",
    "                #trainset, testset, train_ds, test_ds, scaler\n",
    "                if testid == 'shortnormal':\n",
    "                    test_sel = df_test[(df_test['pit_oncaution']==0) & (df_test['stint_len']<=23)]\n",
    "                    sync_testset = test_sel[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "                else:\n",
    "                    sync_testset = _data[testid][1]\n",
    "                           \n",
    "                test_ds, _, _ = makedb(sync_testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "                tx,sx, ex = eval_model(p[mid], test_ds)\n",
    "                mae = raw_eval(tx,sx)\n",
    "                print(mid, 'mae=', mae,ex[\"wQuantileLoss[0.5]\"],ex[\"wQuantileLoss[0.9]\"] )\n",
    "\n",
    "                # prisk direct\n",
    "                truth[mid], pred[mid], samples[mid] = decode(tx,sx)\n",
    "                idx = np.where(samples[mid] < 0)\n",
    "                samples[mid][idx] = 0\n",
    "\n",
    "                _, prisk_vals = prisk_direct_bysamples(samples[mid],truth[mid])\n",
    "                print(prisk_vals[1], prisk_vals[2])        \n",
    "\n",
    "                retdata.append([testevent,tid + '-' + testid , feature_cnt, mae, \n",
    "                               #ex[\"wQuantileLoss[0.5]\"],ex[\"wQuantileLoss[0.9]\"],\n",
    "                               prisk_vals[1], prisk_vals[2]])\n",
    "        \n",
    "test_result = pd.DataFrame(data=retdata, columns=cols) \n",
    "test_result.to_csv('pitmodel_mlp_evaluation_result_paper.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#straight implementation of prisk\n",
    "def quantile_loss(target, quantile_forecast, q):\n",
    "    return 2.0 * np.nansum(\n",
    "        np.abs(\n",
    "            (quantile_forecast - target)\n",
    "            * ((target <= quantile_forecast) - q)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def abs_target_sum(target): \n",
    "    return np.nansum(np.abs(target)) \n",
    "\n",
    "def prisk_direct_bysamples(forecast, target, quantiles=[0.1,0.5,0.9], startid = 0, verbose=False):\n",
    "    \"\"\"\n",
    "    calculate prisk by <samples, tss> directly (equal to gluonts implementation)\n",
    "    \n",
    "    target: endrank\n",
    "    forecast: pred_endrank\n",
    "    item_id: <carno, startlap>\n",
    "    \"\"\"\n",
    "    \n",
    "    prisk = np.zeros((len(quantiles)))\n",
    "    target_sum = 0\n",
    "    aggrisk = np.zeros((len(quantiles)))\n",
    "    \n",
    "    #calc quantiles\n",
    "    # len(quantiles) x 1\n",
    "    quantile_forecasts = np.quantile(forecast, quantiles, axis=0)\n",
    "\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        q_forecast = quantile_forecasts[idx]\n",
    "        prisk[idx] = quantile_loss(target[startid:], q_forecast[startid:], q)\n",
    "        target_sum = abs_target_sum(target[startid:])\n",
    "\n",
    "    if verbose==True and carno==3:\n",
    "        print('target:', target[startid:])\n",
    "        print('forecast:', q_forecast[startid:])\n",
    "        print('target_sum:', target_sum)\n",
    "\n",
    "        print('quantile_forecasts:', quantile_forecasts[:,startid:])\n",
    "        \n",
    "    #agg\n",
    "    #aggrisk = np.mean(prisk, axis=0)\n",
    "    #prisk_sum = np.nansum(prisk, axis=0)\n",
    "    prisk_sum = prisk\n",
    "    \n",
    "    if verbose==True:\n",
    "        print('prisk:',prisk)\n",
    "        print('prisk_sum:',prisk_sum)\n",
    "        print('target_sum:',target_sum)\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        aggrisk[idx] = np.divide(prisk_sum[idx], target_sum)\n",
    "    \n",
    "    agg_metrics = {}\n",
    "    for idx, q in enumerate(quantiles):\n",
    "        agg_metrics[f'wQuantileLoss[{q}]'] = aggrisk[idx]\n",
    "        \n",
    "    print(agg_metrics.values())\n",
    "    \n",
    "    return agg_metrics, aggrisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset, testset, train_ds, test_ds, scaler = _data['all']\n",
    "trainset, testset, train_ds, test_ds, scaler = _data['sel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm, mid = train_model(500,dropout = 0.1,id='sel')\n",
    "t[mid],s[mid], e[mid] = eval_model(pm, test_ds)\n",
    "p[mid] = pm\n",
    "\n",
    "mae = raw_eval(t[mid],s[mid])\n",
    "\n",
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitmodel = save_full_pitmodel(mid, 'sel', maxgap=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run train on all data\n",
    "trainset, testset, train_ds, test_ds, scaler = _data['all']\n",
    "\n",
    "pm, mid = train_model(500,dropout = 0.1,id='all')\n",
    "t[mid],s[mid], e[mid] = eval_model(pm, test_ds)\n",
    "p[mid] = pm\n",
    "\n",
    "mae = raw_eval(t[mid],s[mid])\n",
    "\n",
    "run_test(t[mid], s[mid], [31,816,846,856])\n",
    "pitmodel = save_full_pitmodel(mid, 'all', maxgap=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run train on all data\n",
    "tid = 'noshort'\n",
    "trainset, testset, train_ds, test_ds, scaler = _data[tid]\n",
    "\n",
    "pm, mid = train_model(500,dropout = 0.1,id=tid)\n",
    "t[mid],s[mid], e[mid] = eval_model(pm, test_ds)\n",
    "p[mid] = pm\n",
    "\n",
    "mae = raw_eval(t[mid],s[mid])\n",
    "\n",
    "run_test(t[mid], s[mid], [31,816,846,856])\n",
    "pitmodel = save_full_pitmodel(mid, tid, maxgap=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm, mid = train_model(500,dropout = 0.1)\n",
    "t[mid],s[mid], e[mid] = eval_model(pm)\n",
    "p[mid] = pm\n",
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm, mid = train_model(2000,dropout = 0.1)\n",
    "t[mid],s[mid], e[mid] = eval_model(pm)\n",
    "p[mid] = pm\n",
    "save_model(p[mid], mid)\n",
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm, mid = train_model(200,dropout = 0.1, layers=[8,4])\n",
    "t[mid],s[mid], e[mid] = eval_model(pm)\n",
    "p[mid] = pm\n",
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[mid],s[mid], e[mid] = eval_model(pm)\n",
    "p[mid] = pm\n",
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm, mid = train_model(500, layers=[10,5])\n",
    "t[mid],s[mid] = eval_model(pm)\n",
    "p[mid] = pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm, mid = train_model(500, layers=[32,16,8,4])\n",
    "t[mid],s[mid] = eval_model(pm)\n",
    "p[mid] = pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm, mid = train_model(2000, layers=[32,16,8,4])\n",
    "t[mid],s[mid] = eval_model(pm)\n",
    "p[mid] = pm\n",
    "run_test(t[mid], s[mid], [31,816,846,856])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec = next(iter(train_ds))\n",
    "trec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(tss, forecasts, [31,816,846,856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sel.iloc[816]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sel.iloc[836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[mid],s[mid] = eval_model(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred(t[mid],s[mid], 816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm, mid = train_model(2,dropout = 0.1)\n",
    "#t[mid],s[mid], e[mid] = eval_model(pm)\n",
    "p[mid] = pm\n",
    "#run_test(t[mid], s[mid], [31,816,846,856])\n",
    "\n",
    "#mid = 'mlp-e2000-l10-10-5-student-d0.1'\n",
    "save_model(p[mid], mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test pitmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_scaler = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'mlp-e2000-l10-10-5-student-d0.1'\n",
    "t[id],s[id], e[id] = eval_model(p[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = s[id]\n",
    "tss = t[id]\n",
    "\n",
    "scaler = _scaler\n",
    "testset = test_sel[['lap2nextpit','caution_laps','pitage']].values\n",
    "\n",
    "pitmodel = PitModel()\n",
    "\n",
    "pitmodel.save_model('pitmodel_test', testset, forecasts, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcasts = pitmodel.forecast_ds(testset, forecasts)\n",
    "scaler = _scaler\n",
    "run_test(tss, newcasts, [31,816,846,856],raw_forecast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pitmodel.model['0-0'][0,:], pitmodel.model['0-0'][1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pitmodel.model['10-13'][0,:], pitmodel.model['10-13'][1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitmodel.model['0-0'][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = _scaler\n",
    "test_all = df_test[['lap2nextpit','caution_laps','pitage']].values\n",
    "test_ds, _, test_set = makedb(test_all, scaler, perm=False)\n",
    "\n",
    "id = 'mlp-e2000-l10-10-5-student-d0.1'\n",
    "t[id],s[id], e[id] = eval_model(p[id])\n",
    "forecasts = s[id]\n",
    "tss = t[id]\n",
    "\n",
    "pitmodel = PitModel()\n",
    "\n",
    "pitmodel.save_model(f'mlp-{id}.pickle', test_all, forecasts, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testall_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = _scaler\n",
    "maxgap = 60\n",
    "test_ds, _, test_set, test_all = make_fulltestdb(scaler, maxgap = maxgap)\n",
    "\n",
    "id = 'mlp-e2000-l10-10-5-student-d0.1'\n",
    "\n",
    "t[id],s[id], e[id] = eval_model(p[id])\n",
    "forecasts = s[id]\n",
    "tss = t[id]\n",
    "\n",
    "pitmodel = PitModel()\n",
    "\n",
    "pitmodel.save_model(f'pitmodel-m{maxgap}-{id}.pickle', test_all, forecasts, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitmodel.model['0-0'][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pitmodel.model['0-0'][0,:], pitmodel.model['0-0'][1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['mlp-dsel-e500-l10-10-5-student-d0.1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shortterm predictions using pitmodel(plen=2)\n",
    "\n",
    "convert forecast samples of pitmodel to forecast samples of pit@plen laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_car12 = df_test[df_test['carno']==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds, _, test_set = makedb(test_car12[['lap2nextpit','caution_laps','pitage']].values, scaler, perm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_car12[test_car12['lap']==31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_car12[test_car12['lap']==30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = df_test[(df_test['eid']==5) & (df_test['carno']==12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12[(df12['lap']>=45) & (df12['lap']<55)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.cur_cautionlaps.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros((1000))\n",
    "for i in range(1000):\n",
    "    samples[i] = pitmodel.predict(1,16,1)\n",
    "    \n",
    "samples[samples<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(samples<5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on normal set\n",
    "tid = 'sel'\n",
    "testid = 'normal'\n",
    "feature_cnt = 2\n",
    "testevent = 'Indy500-2018'\n",
    "mid = f\"mlp-d{tid}-f{feature_cnt}-e500-l10-10-5-student-d0.1\"\n",
    "\n",
    "key = f'{testevent}-{feature_cnt}'\n",
    "df_train, df_test, _data = dataset[key]\n",
    "scaler = _data[tid][-1]\n",
    "\n",
    "#trainset, testset, train_ds, test_ds, scaler\n",
    "sync_testset = _data[testid][1]\n",
    "print('testset len=', len(sync_testset))\n",
    "test_ds, _, _ = makedb(sync_testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "tx,sx, ex = eval_model(p[mid], test_ds)\n",
    "mae = raw_eval(tx,sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid = 'sel'\n",
    "testid = 'sel'\n",
    "feature_cnt = 2\n",
    "testevent = 'Indy500-2018'\n",
    "mid = f\"mlp-d{tid}-f{feature_cnt}-e500-l10-10-5-student-d0.1\"\n",
    "\n",
    "key = f'{testevent}-{feature_cnt}'\n",
    "df_train, df_test, _data = dataset[key]\n",
    "scaler = _data[tid][-1]\n",
    "\n",
    "#trainset, testset, train_ds, test_ds, scaler\n",
    "sync_testset = _data[testid][1]\n",
    "print('testset len=', len(sync_testset))\n",
    "test_ds, _, _ = makedb(sync_testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "tx,sx, ex = eval_model(p[mid], test_ds)\n",
    "mae = raw_eval(tx,sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid = 'sel'\n",
    "testid = 'sel'\n",
    "feature_cnt = 2\n",
    "testevent = 'Indy500-2018'\n",
    "mid = f\"mlp-d{tid}-f{feature_cnt}-e500-l10-10-5-student-d0.1\"\n",
    "\n",
    "key = f'{testevent}-{feature_cnt}'\n",
    "df_train, df_test, _data = dataset[key]\n",
    "scaler = _data[tid][-1]\n",
    "\n",
    "#trainset, testset, train_ds, test_ds, scaler\n",
    "test_sel = df_test[(df_test['pit_oncaution']==0) & (df_test['stint_len']<23)]\n",
    "sync_testset = test_sel[['lap2nextpit','caution_laps','pitage','cur_cautionlaps']].values\n",
    "print('testset len=', len(sync_testset))\n",
    "test_ds, _, _ = makedb(sync_testset, scaler, perm=False,feature_cnt=feature_cnt)\n",
    "\n",
    "tx,sx, ex = eval_model(p[mid], test_ds)\n",
    "mae = raw_eval(tx,sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitmodel.model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitmodel.model['0-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
